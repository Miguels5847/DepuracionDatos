"""
DAG para orquestar el procesamiento mensual de la tabla potencia_dep para todo el a√±o 2019
Este DAG procesa los datos mes por mes, insert√°ndolos en potencia_dep_processed,
verificando primero que no existan datos con ceros.
final bueno del 2019 23-04-2025
"""

from datetime import datetime, date,timedelta
import os
import requests
import time
import calendar
import subprocess
import json
import psycopg2
import traceback
import calendar
import json
import pandas as pd
import numpy as np
import decimal
import statistics
import calendar
from kafka import KafkaProducer
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.models import Variable
from airflow.hooks.base import BaseHook
from airflow.utils.task_group import TaskGroup
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Connection
from airflow import settings
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.sensors.external_task import ExternalTaskSensor
# Configuraci√≥n de par√°metros
YEAR = 2019
MINI_PROXY_URL = "http://mini_nifi_proxy_centrosur:5001"  # URL del proxy
HTTP_TIMEOUT = 60  # Timeout para peticiones HTTP en segundos

# Grupo Productor y Consumidor
PRODUCER_GROUP_ID = "931bc6ff-9df8-3e05-ba74-b7e158593720"  # Grupo Productor
CONSUMER_GROUP_ID = "d29e61c5-d615-3dd7-97aa-b134e2e522ea"  # Grupo Consumidor

# IDs de procesadores - Grupo Productor
QUERY_DB_TABLE_PROCESSOR_ID = (
    "e14bc735-035b-302f-f039-62497dfa6206"  # QueryDatabaseTable
)
CONVERT_RECORD_ID = "f77f1a42-c782-31e0-4a8a-dbb948454782"
SPLIT_JSON_ID = "5932789d-1eb5-3ba2-6193-cafa9019bb00"
MERGE_CONTENT_ID = "cca494f4-07ad-35f5-fbff-75fbf07cabea"
PUBLISH_KAFKA_ID = "8d55bbd5-9228-31a0-1708-c81d28cfcece"
PUT_DATABASE_RECORD_ID = "a2cbc853-d842-3523-a89e-ce47daf170dd"  

# IDs de procesadores - Grupo Consumidor
CONSUME_KAFKA_PROCESSOR_ID = "9acbc84a-166f-3f89-0fed-abd745940144"
EVALUATE_JSON_PATH_ID = "2545d33b-6bf8-3686-8bcb-af71e5f31d48"
REPLACE_TEXT_ID = "207fd8f3-4553-3c01-66f1-34aa360aebb3"
PUT_DB_PROCESSOR_ID = "cc103488-b857-376c-27ad-270dfca2fe08"

# Configuraci√≥n del DAG
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2025, 3, 1),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=2),
    'email_on_success': False,
}

dag = DAG(
    "potencia_integrador_anual_corto",
    default_args=default_args,
    description="Procesamiento mensual de datos de potencia para el a√±o 2019",
    schedule_interval=None,  # Ejecuci√≥n manual
    catchup=False,
    tags=["centrosur", "potencia", "anual", "nifi", "kafka"],
)


# FUNCIONES DE UTILIDAD
def create_postgres_connections():
    """Create required Postgres connections if they don't exist"""
    connections = [
        {
            "conn_id": "postgres_centrosur",
            "conn_type": "postgres",
            "host": "postgres_centrosur",
            "login": "postgres",
            "password": "1150040812",
            "schema": "centrosur",
            "port": 5432,
        },
        {
            "conn_id": "postgres_base_prueba",
            "conn_type": "postgres",
            "host": "postgres_centrosur",  # Same host
            "login": "postgres",
            "password": "1150040812",
            "schema": "base_prueba",  # Different database
            "port": 5432,
        },
    ]
    session = settings.Session()
    for conn_info in connections:
        conn = Connection(**conn_info)
        existing_conn = (
            session.query(Connection).filter(Connection.conn_id == conn.conn_id).first()
        )
        if existing_conn:
            session.delete(existing_conn)
            session.commit()
        session.add(conn)
    session.commit()
    session.close()
    print("‚úÖ Postgres connections created/updated successfully")

def verificar_proxy_disponible():
    """Verifica que el proxy NiFi est√© disponible"""
    max_intentos = 5
    for intento in range(1, max_intentos + 1):
        try:
            print(
                f"Verificando disponibilidad del proxy NiFi (intento {intento}/{max_intentos})..."
            )
            response = requests.get(f"{MINI_PROXY_URL}/health", timeout=HTTP_TIMEOUT)

            # Registrar respuesta completa para diagn√≥stico
            print(
                f"Respuesta del proxy: Status={response.status_code}, Contenido={response.text[:200]}..."
            )

            if response.status_code == 200:
                data = response.json()
                # Verificar si el proxy reporta un estado saludable
                if data.get("status") == "healthy" or data.get("auth_working"):
                    print(f"Proxy NiFi est√° disponible y reporta estado saludable")
                    print(f"Detalles adicionales: {json.dumps(data)}")
                    return True
                else:
                    print(
                        f"Proxy disponible pero reporta estado: {data.get('status')}, mensaje: {data.get('message')}"
                    )
                    # Verificar informaci√≥n de autenticaci√≥n y conexi√≥n con NiFi
                    if "nifi_connected" in data:
                        print(f"Conexi√≥n con NiFi: {data.get('nifi_connected')}")
                    if "auth_working" in data:
                        print(f"Autenticaci√≥n funcionando: {data.get('auth_working')}")
                    if data.get("auth_working"):
                        return True
            else:
                print(
                    f"Proxy no disponible (intento {intento}/{max_intentos}): {response.status_code}"
                )
                print(f"Respuesta: {response.text[:200]}...")
        except requests.exceptions.ConnectionError as e:
            print(
                f"Error de conexi√≥n al proxy (intento {intento}/{max_intentos}): {str(e)}"
            )
            print(
                "Esto podr√≠a indicar que el servicio proxy no est√° en ejecuci√≥n o no es accesible en la red."
            )
        except requests.exceptions.Timeout as e:
            print(
                f"Timeout al conectar con el proxy (intento {intento}/{max_intentos}): {str(e)}"
            )
            print(
                "Esto podr√≠a indicar que el servicio proxy est√° sobrecargado o no responde."
            )
        except Exception as e:
            print(
                f"Error al verificar proxy (intento {intento}/{max_intentos}): {str(e)}"
            )

        if intento < max_intentos:
            print(f"Esperando 5 segundos antes de reintentar...")
            time.sleep(5)

    raise Exception("El proxy NiFi no est√° disponible despu√©s de m√∫ltiples intentos")

def decidir_ruta_mes(ti, mes, nombre_mes):
    """Funci√≥n helper para determinar la ruta correcta"""
    # Verificar el resultado expl√≠citamente
    resultado = ti.xcom_pull(task_ids=f"procesar_{nombre_mes}.verificar_si_{nombre_mes}_ya_procesado")
    print(f"Resultado de verificaci√≥n para mes {mes}: {resultado}")
    
    if resultado:
        print(f"‚úÖ DECISI√ìN: Mes {mes} ya procesado, SALTANDO...")
        return f"procesar_{nombre_mes}.skip_y_verificar.skip_{nombre_mes}"
    else:
        print(f"üîÑ DECISI√ìN: Mes {mes} necesita procesarse")
        return f"procesar_{nombre_mes}.camino_procesamiento.procesar_mes_{mes}"
def obtener_estado_procesador(id_procesador):
    """Obtiene el estado actual de un procesador a trav√©s del mini-proxy"""
    try:
        response = requests.get(
            f"{MINI_PROXY_URL}/api/processors/{id_procesador}", timeout=HTTP_TIMEOUT
        )

        if response.status_code == 200:
            datos_procesador = response.json()
            return datos_procesador
        else:
            return {
                "error": f"Error al obtener estado del procesador: {response.status_code}",
                "raw_response": response.text,
            }
    except Exception as e:
        return {"error": f"Error al consultar procesador: {str(e)}"}


def detener_procesador(id_procesador):
    """Detiene un procesador espec√≠fico en NiFi"""
    try:
        # Primero obtenemos el estado actual y la revisi√≥n
        processor_data = obtener_estado_procesador(id_procesador)

        # Verificar si el procesador existe
        if "error" in processor_data:
            print(
                f"WARNING: El procesador {id_procesador} no existe o hay un error: {processor_data['error']}"
            )
            return f"Warning: Procesador {id_procesador} no encontrado"

        # Extraemos la informaci√≥n necesaria
        current_state = processor_data.get("component", {}).get("state")
        revision = processor_data.get("revision", {})

        print(
            f"Estado actual del procesador {id_procesador} para detener: {current_state}"
        )

        # Si ya est√° detenido o deshabilitado, no hacer nada
        if current_state in ["STOPPED", "DISABLED"]:
            return f"Procesador {id_procesador} ya est√° detenido o deshabilitado"

        # Si no se pudo obtener el estado, devolver mensaje
        if not current_state:
            return f"No se pudo obtener el estado del procesador {id_procesador} para detener"

        # Preparamos los datos para actualizar el estado
        update_data = {
            "revision": revision,
            "component": {"id": id_procesador, "state": "STOPPED"},
        }

        # Enviamos la solicitud para cambiar el estado
        response = requests.put(
            f"{MINI_PROXY_URL}/api/processors/{id_procesador}",
            json=update_data,
            timeout=HTTP_TIMEOUT,
        )

        if response.status_code == 200:
            return f"Procesador {id_procesador} detenido correctamente"
        else:
            return f"Error al detener procesador: {response.status_code}"
    except Exception as e:
        return f"Excepci√≥n al detener procesador: {str(e)}"


def iniciar_procesador_con_reintento(processor_id, processor_name, max_retries=5):
    """Inicia un procesador con reintentos y verificaci√≥n mejorada"""
    for intento in range(1, max_retries + 1):
        try:
            print(f"   Intento {intento}/{max_retries} para iniciar {processor_name}")

            # Obtener estado actual y revisi√≥n
            datos_procesador = obtener_estado_procesador(processor_id)
            estado_actual = datos_procesador.get("component", {}).get("state")
            revision = datos_procesador.get("revision", {})

            print(f"   Estado actual: {estado_actual}")

            if estado_actual == "RUNNING":
                print(f"   ‚úÖ {processor_name} ya est√° en estado RUNNING")
                return f"‚úÖ {processor_name} ya est√° en estado RUNNING"

            # Preparar datos para actualizar estado
            update_data = {
                "revision": revision,
                "component": {"id": processor_id, "state": "RUNNING"},
            }

            # Enviar solicitud para cambiar estado
            response = requests.put(
                f"{MINI_PROXY_URL}/api/processors/{processor_id}",
                json=update_data,
                timeout=HTTP_TIMEOUT,
            )

            if response.status_code == 200:
                # Verificar que el cambio fue efectivo
                time.sleep(3)  # Esperar m√°s tiempo para confirmar el cambio
                new_status = obtener_estado_procesador(processor_id)
                new_state = new_status.get("component", {}).get("state")

                print(f"   Nuevo estado despu√©s de inicio: {new_state}")

                if new_state == "RUNNING":
                    print(f"   ‚úÖ {processor_name} iniciado correctamente")
                    return f"‚úÖ {processor_name} iniciado correctamente"
            else:
                print(
                    f"   ‚ùå Error c√≥digo {response.status_code}: {response.text[:100]}"
                )

            # Si llegamos aqu√≠, o el inicio fall√≥ o no se verific√≥ correctamente
            if intento < max_retries:
                print(f"   ‚è≥ Esperando 5 segundos antes de reintentar...")
                time.sleep(5)

        except Exception as e:
            print(f"   ‚ùå Error: {str(e)}")
            if intento < max_retries:
                time.sleep(5)

    # Si despu√©s de todos los intentos no logramos iniciar, devolver mensaje de error
    print(f"   ‚ö†Ô∏è No se pudo iniciar {processor_name} despu√©s de {max_retries} intentos")
    return f"‚ö†Ô∏è No se pudo iniciar {processor_name} despu√©s de {max_retries} intentos"

def stop_all_processors_individually():
    """Detiene todos los procesadores individuales en lugar de grupos completos"""
    try:
        producer_processors = [
            QUERY_DB_TABLE_PROCESSOR_ID,
            CONVERT_RECORD_ID,
            SPLIT_JSON_ID,
            MERGE_CONTENT_ID,
            PUBLISH_KAFKA_ID,
            PUT_DATABASE_RECORD_ID,
        ]
        # Lista de procesadores del consumidor
        consumer_processors = [
            CONSUME_KAFKA_PROCESSOR_ID,
            EVALUATE_JSON_PATH_ID,
            REPLACE_TEXT_ID,
            PUT_DB_PROCESSOR_ID,
        ]

        print("üìå Deteniendo procesadores del productor individualmente...")
        for processor_id in producer_processors:
            detener_procesador(processor_id)

        print("üìå Deteniendo procesadores del consumidor individualmente...")
        for processor_id in consumer_processors:
            detener_procesador(processor_id)

        # Esperar para que los procesadores se detengan
        time.sleep(15)

        return f"Todos los procesadores detenidos individualmente: {len(producer_processors) + len(consumer_processors)} procesadores"
    except Exception as e:
        print(f"‚ùå Error al detener procesadores: {str(e)}")
        return f"Error al detener procesadores: {str(e)}"

def iniciar_todos_procesadores_en_secuencia():
    """Inicia todos los procesadores en secuencia correcta con validaci√≥n"""
    results = []

    # Lista completa de todos los procesadores en orden correcto
    todos_procesadores = [
        # Grupo productor
        {
            "id": QUERY_DB_TABLE_PROCESSOR_ID,
            "name": "QueryDatabaseTable",
            "group": "producer",
        },
        {"id": CONVERT_RECORD_ID, "name": "ConvertRecord", "group": "producer"},

        {"id": SPLIT_JSON_ID, "name": "SplitJson", "group": "producer"},
 
        {"id": MERGE_CONTENT_ID, "name": "MergeContent", "group": "producer"},
        {"id": PUBLISH_KAFKA_ID, "name": "PublishKafka", "group": "producer"},
        {"id": PUT_DATABASE_RECORD_ID, "name": "PutDatabaseRecord", "group": "producer"},

        
        # Grupo consumidor
        {"id": CONSUME_KAFKA_PROCESSOR_ID, "name": "ConsumeKafka", "group": "consumer"},

        {"id": EVALUATE_JSON_PATH_ID, "name": "EvaluateJsonPath", "group": "consumer"},
        {"id": REPLACE_TEXT_ID, "name": "ReplaceText", "group": "consumer"},
        {"id": PUT_DB_PROCESSOR_ID, "name": "PutDatabaseRecord", "group": "consumer"},

    ]
    # Iniciar primero el grupo productor COMPLETO
    print("‚ñ∂Ô∏è Iniciando grupo PRODUCTOR completo...")
    for proc in [p for p in todos_procesadores if p["group"] == "producer"]:
        print(f"‚ñ∂Ô∏è Iniciando {proc['name']}...")
        result = iniciar_procesador_con_reintento(proc["id"], proc["name"])
        results.append(result)

        # Verificar estado despu√©s del inicio
        verify_state = obtener_estado_procesador(proc["id"])
        current_state = verify_state.get("component", {}).get("state", "UNKNOWN")
        print(f"   Estado actual de {proc['name']}: {current_state}")

        time.sleep(3)  # Pausa entre inicios

    # Esperar antes de iniciar consumidores
    print("‚è≥ Esperando 10 segundos para que los productores inicien completamente...")
    time.sleep(10)

    # Luego iniciar el grupo consumidor COMPLETO
    print("‚ñ∂Ô∏è Iniciando grupo CONSUMIDOR completo...")
    for proc in [p for p in todos_procesadores if p["group"] == "consumer"]:
        print(f"‚ñ∂Ô∏è Iniciando {proc['name']}...")
        result = iniciar_procesador_con_reintento(proc["id"], proc["name"])
        results.append(result)

        # Verificar estado despu√©s del inicio
        verify_state = obtener_estado_procesador(proc["id"])
        current_state = verify_state.get("component", {}).get("state", "UNKNOWN")
        print(f"   Estado actual de {proc['name']}: {current_state}")

        time.sleep(3)  # Pausa entre inicios

    return f"Iniciados {len(results)} procesadores en secuencia"

# 1. Primero, crear la tabla de datos_enviados en base_prueba
def crear_tabla_datos_enviados():
    """Crea la tabla de datos enviados si no existe"""
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        sql = """
        CREATE TABLE IF NOT EXISTS datos_enviados (
            id SERIAL PRIMARY KEY,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
            mes INTEGER,
            fecha DATE,
            hora INTEGER,
            alimentador VARCHAR(50),
            potencia_activa NUMERIC,
            potencia_reactiva NUMERIC,
            source VARCHAR(50),
            payload JSONB
        );
        """
        pg_hook.run(sql)
        print("‚úÖ Tabla datos_enviados creada/verificada correctamente")
        return True
    except Exception as e:
        print(f"‚ùå Error creando tabla datos_enviados: {str(e)}")
        return False

# 2. Modificar la funci√≥n de verificaci√≥n para usar la nueva tabla
def verificar_datos_enviados(mes, tiempo_espera_minutos=5, **kwargs):
    """Verifica la existencia de datos en la tabla datos_enviados"""
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        
        # Calcular tiempo de espera
        tiempo_total_segundos = tiempo_espera_minutos * 60
        tiempo_transcurrido = 0
        
        print(f"‚è≥ Verificando datos enviados para mes {mes}...")
        
        while tiempo_transcurrido < tiempo_total_segundos:
            # Consultar datos enviados para este mes
            query = f"""
            SELECT COUNT(*) FROM datos_enviados 
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = 2019
            """
            count = pg_hook.get_first(query)[0]
            
            print(f"üìä Verificaci√≥n a los {tiempo_transcurrido} segundos: {count} registros enviados para mes {mes}")
            
            if count > 0:
                print(f"‚úÖ Datos detectados en tabla datos_enviados para mes {mes}")
                return True
                
            # Esperar antes de la siguiente verificaci√≥n
            time.sleep(60)
            tiempo_transcurrido += 60
            
        print(f"‚ö†Ô∏è No se detectaron datos enviados para mes {mes} despu√©s de {tiempo_espera_minutos} minutos")
        return False
    except Exception as e:
        print(f"‚ùå Error verificando datos enviados: {str(e)}")
        return False
    
def iniciar_todos_procesadores_por_grupo():
    """Inicia todos los procesadores de ambos grupos uno por uno (reemplaza versi√≥n fallida)"""
    try:
        results = {}

        # Lista de procesadores del productor con IDs ACTUALIZADOS
        producer_processors = [
            {"id": QUERY_DB_TABLE_PROCESSOR_ID, "name": "QueryDatabaseTable"},
            {"id": CONVERT_RECORD_ID, "name": "ConvertRecord"},
            {"id": SPLIT_JSON_ID, "name": "SplitJson"},
            {"id": MERGE_CONTENT_ID, "name": "MergeContent"},
            {"id": PUBLISH_KAFKA_ID, "name": "PublishKafka"},
            {"id": PUT_DATABASE_RECORD_ID, "name": "PutDatabaseRecord"},
        ]
        # Lista de procesadores del consumidor con IDs ACTUALIZADOS
        consumer_processors = [
            {"id": CONSUME_KAFKA_PROCESSOR_ID, "name": "ConsumeKafka"},
            {"id": EVALUATE_JSON_PATH_ID, "name": "EvaluateJsonPath"},
            {"id": REPLACE_TEXT_ID, "name": "ReplaceText"},
            {"id": PUT_DB_PROCESSOR_ID, "name": "PutDatabaseRecord"},
        ]
        # Iniciar cada procesador individualmente - PRODUCTOR
        print("‚ñ∂Ô∏è Iniciando procesadores del productor individualmente...")
        for processor in producer_processors:
            result = iniciar_procesador_con_reintento(
                processor["id"], processor["name"], max_retries=3
            )
            results[f"producer_{processor['name']}"] = result
            time.sleep(3)  # Breve pausa entre cada inicio

        # Esperar antes de iniciar consumidores
        print(
            "‚è≥ Esperando 15 segundos para que los productores inicien completamente..."
        )
        time.sleep(15)
        
        # A√ëADIR ESTA PARTE QUE FALTA: Iniciar procesadores del consumidor
        print("‚ñ∂Ô∏è Iniciando procesadores del consumidor individualmente...")
        for processor in consumer_processors:
            result = iniciar_procesador_con_reintento(
                processor["id"], processor["name"], max_retries=3
            )
            results[f"consumer_{processor['name']}"] = result
            time.sleep(3)  # Breve pausa entre cada inicio
            
        return results
    except Exception as e:
        print(f"‚ùå Error iniciando procesadores: {str(e)}")
        return False

def configurar_topic_procesador(processor_id, property_name, topic_name):
    """Configura un procesador Kafka para usar un topic espec√≠fico"""
    try:
        # Obtener el estado actual del procesador
        processor_data = obtener_estado_procesador(processor_id)

        if "error" in processor_data:
            print(
                f"‚õî ERROR: No se pudo obtener el estado del procesador: {processor_data}"
            )
            return False

        revision = processor_data.get("revision", {})
        component = processor_data.get("component", {})
        properties = component.get("properties", {})

        # Guardar el topic actual para diagn√≥stico
        topic_actual = properties.get(property_name, "desconocido")
        print(f"üìã Topic actual en {processor_id}: {topic_actual}")

        # Actualizar la propiedad del topic
        properties[property_name] = topic_name

        # Enviar actualizaci√≥n al procesador
        update_data = {
            "revision": revision,
            "component": {"id": processor_id, "properties": properties},
        }
        # Primero asegurarse que el procesador est√© detenido
        detener_procesador(processor_id)
        time.sleep(5)

        update_response = requests.put(
            f"{MINI_PROXY_URL}/api/processors/{processor_id}",
            json=update_data,
            timeout=HTTP_TIMEOUT,
        )
        if update_response.status_code == 200:
            print(
                f"‚úÖ Topic actualizado correctamente a {topic_name} en {processor_id}"
            )
            return True
        else:
            print(f"‚ùå Error al actualizar topic: {update_response.status_code}")
            print(f"üìä Respuesta: {update_response.text[:200]}")
            return False

    except Exception as e:
        print(f"‚ùå Error al configurar topic: {str(e)}")
        return False

def publicar_datos_depurados_a_kafka(pg_hook, productor, mes, year=YEAR):
    """Publica todos los datos procesados del mes al topic potencia_datos_depurados con formato correcto"""
    try:
        print(f"üì§ Publicando datos depurados del mes {mes} a Kafka...")
        
        # Consultar datos procesados directamente de la tabla
        query = f"""
        SELECT fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva
        FROM potencia_dep_processed
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = {year}
        ORDER BY fecha, hora, alimentador
        """
        
        datos_procesados = pg_hook.get_records(query)
        if not datos_procesados:
            print(f"‚ö†Ô∏è No se encontraron datos procesados para el mes {mes}")
            return 0
            
        # Publicar cada registro al topic
        topic = "potencia_datos_depurados"
        registros_enviados = 0
        
        for dato in datos_procesados:
            # Calcular el d√≠a de semana correcto a partir de la fecha
            fecha_actual = dato[0]
            dia_semana_correcto = calcular_dia_semana(fecha_actual)
            
            # Asegurar que todos los campos existan y est√©n correctamente formateados
            mensaje_kafka = {
                'fecha': dato[0].strftime('%Y-%m-%d') if hasattr(dato[0], 'strftime') else str(dato[0]),
                'hora': str(dato[1]),
                'alimentador': str(dato[2]),
                'dia_semana': dia_semana_correcto,  # Usar el d√≠a semana calculado correctamente
                'potencia_activa': float(dato[4]) if dato[4] is not None else 0.0,
                'potencia_reactiva': float(dato[5]) if dato[5] is not None else 0.0
            }
            
            # Validar que el mensaje est√© correctamente formateado antes de enviarlo
            try:
                json_str = json.dumps(mensaje_kafka)  # Verificar que sea JSON v√°lido
                productor.send(topic, mensaje_kafka)
                registros_enviados += 1
                
                # Mostrar progreso cada 1000 registros
                if registros_enviados % 1000 == 0:
                    print(f"üìä Enviados {registros_enviados} registros a Kafka")
            except Exception as e:
                print(f"‚ùå Error formateando o enviando mensaje: {str(e)}")
                print(f"Mensaje que caus√≥ error: {mensaje_kafka}")
        
        # IMPORTANTE: Forzar env√≠o pendiente
        productor.flush()
        
        print(f"‚úÖ Total de {registros_enviados} registros publicados al topic {topic}")
        return registros_enviados
        
    except Exception as e:
        print(f"‚ùå Error publicando datos a Kafka: {str(e)}")
        traceback.print_exc()
        return 0

def limpiar_estado_procesador(processor_id):
    """Limpia el estado del procesador QueryDatabaseTable para forzar procesamiento desde cero"""
    host = "588a471cac3f"  # ID del contenedor confirmado

    try:
        print(f"üßπ Limpiando estado del procesador {processor_id}...")

        # 1. Obtener token
        cmd_token = f"curl -k -s -X POST -d 'username=admin&password=centrosur123' https://{host}:8443/nifi-api/access/token"
        token_process = subprocess.run(
            cmd_token, shell=True, capture_output=True, text=True
        )
        token = token_process.stdout.strip()

        if not token:
            print("‚ùå Error obteniendo token de autenticaci√≥n")
            return False

        # 2. Limpiar el estado del procesador
        cmd_clear = f"curl -k -s -X POST -H 'Authorization: Bearer {token}' -H 'Content-Type: application/json' https://{host}:8443/nifi-api/processors/{processor_id}/state/clear-requests"

        clear_process = subprocess.run(
            cmd_clear, shell=True, capture_output=True, text=True
        )
        if clear_process.returncode == 0:
            print("‚úÖ Estado del procesador limpiado correctamente")
            return True
        else:
            print(f"‚ùå Error limpiando estado: {clear_process.stderr}")
            return False

    except Exception as e:
        print(f"‚ùå Error general limpiando estado: {str(e)}")
        return False

def clear_processor_state_and_empty_all_queues():
    """Completely clears processor state and empties all connection queues"""
    try:
        print(
            "üßπ Performing complete cleanup of processor state and connection queues..."
        )

        # 1. Stop all processors first
        print("üìå Deteniendo procesadores del productor individualmente...")
        stop_all_processors_individually()
        time.sleep(5)  # Allow time for processors to stop

        # 2. Get token for direct NiFi API access
        host = "588a471cac3f"  # Container ID that works consistently
        cmd_token = f"curl -k -s -X POST -d 'username=admin&password=centrosur123' https://{host}:8443/nifi-api/access/token"
        token_process = subprocess.run(
            cmd_token, shell=True, capture_output=True, text=True
        )
        token = token_process.stdout.strip()

        if not token:
            print("‚ö†Ô∏è Couldn't get token, trying alternative method...")
            # Even without token, try to continue with other methods

        # 3. Clear state of the main processor - multiple attempts with different methods
        if token:
            # Method 1: Standard clear request
            cmd_clear = f"curl -k -s -X POST -H 'Authorization: Bearer {token}' https://{host}:8443/nifi-api/processors/{QUERY_DB_TABLE_PROCESSOR_ID}/state/clear-requests"
            subprocess.run(cmd_clear, shell=True)

            # Method 2: Get all connections and empty them
            cmd_get_connections = f"curl -k -s -X GET -H 'Authorization: Bearer {token}' -H 'Accept: application/json' https://{host}:8443/nifi-api/process-groups/{PRODUCER_GROUP_ID}/connections"
            conn_result = subprocess.run(
                cmd_get_connections, shell=True, capture_output=True, text=True
            )

            try:
                connections = json.loads(conn_result.stdout)
                if "connections" in connections:
                    for conn in connections["connections"]:
                        conn_id = conn.get("id")
                        cmd_empty = f"curl -k -s -X POST -H 'Authorization: Bearer {token}' -H 'Content-Type: application/json' -d '{{\"disconnectedNodeAcknowledged\":false}}' https://{host}:8443/nifi-api/flowfile-queues/{conn_id}/drop-requests"
                        subprocess.run(cmd_empty, shell=True)
            except:
                print("‚ö†Ô∏è Could not empty connection queues with token")

        # 4. Also use the mini-proxy for thorough cleanup
        try:
            response = requests.post(
                f"{MINI_PROXY_URL}/api/processors/{QUERY_DB_TABLE_PROCESSOR_ID}/state/clear",
                timeout=10,
            )

            # Also try to empty connection queues
            for group_id in [PRODUCER_GROUP_ID, CONSUMER_GROUP_ID]:
                response = requests.get(
                    f"{MINI_PROXY_URL}/api/process-groups/{group_id}/connections",
                    timeout=10,
                )

                if response.status_code == 200:
                    connections = response.json()
                    if "connections" in connections:
                        for conn in connections["connections"]:
                            conn_id = conn.get("id")
                            requests.post(
                                f"{MINI_PROXY_URL}/api/flowfile-queues/{conn_id}/drop-requests",
                                json={"disconnectedNodeAcknowledged": False},
                                timeout=10,
                            )
        except:
            # Continue even if this method fails
            pass

        time.sleep(10)  # Allow time for cleanup operations to complete
        print("‚úÖ Cleanup operations completed")
        return True

    except Exception as e:
        print(f"‚ö†Ô∏è Error in cleanup: {str(e)}")
        # Continue anyway to ensure the flow doesn't stop
        return True

def truncate_existing_month_data(mes_inicio, mes_fin, ano=2019):
    """Removes existing data for the month to allow clean reprocessing"""
    try:
        fecha_inicio = f"{ano}-{mes_inicio:02d}-01"
        fecha_fin = f"{ano}-{mes_fin:02d}-01"

        print(
            f"üóëÔ∏è Limpiando datos existentes en destino para {fecha_inicio} a {fecha_fin}"
        )
        pg_hook_destino = PostgresHook(postgres_conn_id="postgres_base_prueba")

        # Delete existing records for this month
        delete_query = f"""
        DELETE FROM potencia_dep_processed 
        WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
        """
        rows_deleted = pg_hook_destino.run(delete_query)
        print(f"‚úÖ Eliminando {rows_deleted} datos del mes {mes_inicio}")
        return True
    except Exception as e:
        print(f"‚ùå Error al eliminar datos del mes: {str(e)}")
        return False

def actualizar_consulta_sin_docker(processor_id, tabla_temporal):
    """Actualiza la consulta SQL sin depender de Docker"""
    try:
        print(f"üîÑ Actualizando consulta para usar tabla temporal {tabla_temporal}...")

        # Crear la consulta que usa EXCLUSIVAMENTE la tabla temporal
        nueva_consulta = f"""/* TABLA_TEMPORAL_EXCLUSIVA_{int(time.time())} */
SELECT 
    fecha, 
    hora, 
    alimentador, 
    dia_semana, 
    potencia_activa AS potencia_activa_real, 
    potencia_reactiva AS potencia_reactiva_real
FROM {tabla_temporal}
ORDER BY fecha, hora, alimentador
"""

        # Obtener estado del procesador
        processor_data = obtener_estado_procesador(processor_id)

        if "error" in processor_data:
            print(f"‚ö†Ô∏è Error obteniendo estado: {processor_data['error']}")
            return False
        revision = processor_data.get("revision", {})
        component = processor_data.get("component", {})
        properties = dict(component.get("properties", {}))

        # Mostrar consulta actual para diagn√≥stico
        consulta_actual = properties.get("Custom Query", "NO DISPONIBLE")
        print(f"üìã Consulta actual: {consulta_actual[:100]}...")

        # Actualizar consulta
        properties["Custom Query"] = nueva_consulta
        properties["db-fetch-sql-query"] = nueva_consulta  # Nombre alternativo

        # Remover propiedades problem√°ticas
        for key in [
            "start_date",
            "end_date",
            "Maximum-value Column",
            "Maximum-value Columns",
        ]:
            if key in properties:
                del properties[key]
        # Crear datos para actualizaci√≥n
        update_data = {
            "revision": revision,
            "component": {"id": processor_id, "properties": properties},
        }
        # Enviar actualizaci√≥n
        response = requests.put(
            f"{MINI_PROXY_URL}/api/processors/{processor_id}",
            json=update_data,
            timeout=HTTP_TIMEOUT,
        )
        if response.status_code == 200:
            print(f"‚úÖ Consulta actualizada correctamente para usar {tabla_temporal}")

            # Verificaci√≥n de actualizaci√≥n
            time.sleep(3)
            verify_data = obtener_estado_procesador(processor_id)
            updated_query = (
                verify_data.get("component", {})
                .get("properties", {})
                .get("Custom Query", "")
            )
            if tabla_temporal in updated_query:
                print("‚úÖ Verificaci√≥n confirma actualizaci√≥n correcta")
                return True
            else:
                print("‚ö†Ô∏è Verificaci√≥n no encontr√≥ la tabla temporal en la consulta")
                print(f"Consulta actual: {updated_query[:100]}...")
        else:
            print(f"‚ùå Error actualizando consulta: {response.status_code}")
            print(f"Respuesta: {response.text[:200]}")

        return False
    except Exception as e:
        print(f"‚ùå Error actualizando consulta: {str(e)}")
        return False

def decidir_procesar_mes(ti, mes, grupo):
    """Decide si procesar un mes basado en verificaci√≥n previa"""
    verificacion_key = f"procesar_{grupo}.verificar_si_{grupo}_ya_procesado"
    if not ti.xcom_pull(task_ids=verificacion_key):
        return f"procesar_{grupo}.procesar_mes_{mes}"
    else:
        return f"procesar_{grupo}.skip_{grupo}"

def get_token_with_retry(max_retries=3):
    """Obtiene token con reintentos en caso de fallo"""
    host = "588a471cac3f"  # Container ID
    
    for intento in range(1, max_retries + 1):
        try:
            print(f"üîë Intento {intento}/{max_retries} de obtener token...")
            cmd_token = f"curl -k -s -X POST -d 'username=admin&password=centrosur123' https://{host}:8443/nifi-api/access/token"
            token_process = subprocess.run(cmd_token, shell=True, capture_output=True, text=True, timeout=30)
            
            token = token_process.stdout.strip()
            if token and len(token) > 20:  # Token v√°lido suele ser largo
                print("‚úÖ Token obtenido correctamente")
                return token
            
            # Intento alternativo con el mini-proxy si est√° disponible
            print("‚ö†Ô∏è M√©todo directo fall√≥, intentando con mini-proxy...")
            response = requests.post(
                f"{MINI_PROXY_URL}/api/get-token",
                json={"username": "admin", "password": "centrosur123"},
                timeout=HTTP_TIMEOUT
            )
            
            if response.status_code == 200:
                token_data = response.json()
                if "token" in token_data:
                    print("‚úÖ Token obtenido v√≠a mini-proxy")
                    return token_data["token"]
                    
            time.sleep(2 * intento)  # Espera creciente entre intentos
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error en intento {intento}: {str(e)}")
            time.sleep(2 * intento)
    
    print("‚ùå No se pudo obtener token despu√©s de varios intentos")
    return None

def procesar_mes_sustituyendo_tabla(mes, **kwargs):
    """
    Aplica el m√©todo nuclear para todos los meses: trabaja con una copia de potencia_dep
    para garantizar aislamiento total de datos por mes.
    """
    try:
        # Inicializaci√≥n de variables
        pg_hook = PostgresHook(postgres_conn_id="postgres_centrosur")
        
        ano = 2019
        mes_siguiente = mes + 1 if mes < 12 else 1
        ano_siguiente = ano if mes < 12 else ano + 1
        fecha_inicio = f"{ano}-{mes:02d}-01"
        fecha_fin = f"{ano_siguiente}-{mes_siguiente:02d}-01"
        print(f"üö® M√âTODO NUCLEAR: Aislamiento total para mes {mes} ({fecha_inicio} a {fecha_fin})")
        
        # Detener todos los procesadores para asegurar un entorno limpio
        print("‚è∏Ô∏è Deteniendo todos los procesadores...")
        stop_all_processors_individually()
        time.sleep(20)  # Tiempo extra para asegurar detenci√≥n completa
        
        # 1. Verificar que existe potencia_dep_original (nuestra fuente de verdad)
        original_existe = pg_hook.get_first(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'potencia_dep_original')"
        )[0]
        
        if not original_existe:
            print("‚ùå ERROR: No existe la tabla potencia_dep_original que debe servir como fuente de verdad")
            return False
        
        # 2. Verificar registros existentes para este mes en la fuente original
        query_verificar = f"""
            SELECT COUNT(*) FROM potencia_dep_original 
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = {ano}
        """
        count = pg_hook.get_first(query_verificar)[0]
        
        if count == 0:
            print(f"‚ö†Ô∏è No hay datos para el mes {mes} en potencia_dep_original. Abortando procesamiento.")
            return False
        
        print(f"‚úÖ Encontrados {count} registros en potencia_dep_original para el mes {mes}")
        
        # 3. CREAR TABLA DE TRABAJO con filtros de fecha (solo para el mes espec√≠fico)
        tabla_trabajo = f"potencia_dep_work_{int(time.time())}"
        print(f"üìã Creando tabla de trabajo {tabla_trabajo}...")
        
        pg_hook.run(f"DROP TABLE IF EXISTS {tabla_trabajo}")
        
        create_query = f"""
            CREATE TABLE {tabla_trabajo} AS
            SELECT * FROM potencia_dep_original 
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = {ano}
            AND fecha >= '{fecha_inicio}'
            AND fecha < '{fecha_fin}'
        """
        pg_hook.run(create_query)
        
        # 4. Verificar creaci√≥n correcta
        count_trabajo = pg_hook.get_first(f"SELECT COUNT(*) FROM {tabla_trabajo}")[0]
        print(f"‚úÖ Tabla de trabajo {tabla_trabajo} creada con {count_trabajo} registros exclusivos del mes {mes}")
        
        # 5. Registrar la tabla trabajo para limpieza posterior
        kwargs['ti'].xcom_push(key=f'tabla_trabajo_mes_{mes}', value=tabla_trabajo)
        
        # 6. M√âTODO NUCLEAR: Crear backup de potencia_dep actual y reemplazar con solo datos del mes
        print(f"‚ò¢Ô∏è APLICANDO M√âTODO NUCLEAR PARA MES {mes}...")
        
        # 6.1. Crear backup de tabla potencia_dep actual
        backup_tabla = f"potencia_dep_backup_mes{mes}_{int(time.time())}"
        pg_hook.run(f"CREATE TABLE {backup_tabla} AS SELECT * FROM potencia_dep")
        print(f"‚úÖ Backup creado: {backup_tabla}")
        
        # 6.2. Guardar referencia al backup para restauraci√≥n posterior
        kwargs['ti'].xcom_push(key=f'mes_{mes}_backup_table', value=backup_tabla)
        
        # 6.3. Truncar y reemplazar potencia_dep con solo datos del mes correcto
        pg_hook.run("TRUNCATE TABLE potencia_dep")
        pg_hook.run(f"""
            INSERT INTO potencia_dep
            SELECT * FROM {tabla_trabajo}
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = {ano}
        """)
        temp_count = pg_hook.get_first("SELECT COUNT(*) FROM potencia_dep")[0]
        print(f"‚úÖ Tabla potencia_dep modificada para contener SOLO {temp_count} registros del mes {mes}")
        
        # 7. Crear consulta simple para NiFi (sin necesidad de filtros adicionales)
        nueva_consulta = f"""/* M√âTODO_NUCLEAR_MES_{mes}_{int(time.time())} */
SELECT 
    fecha, 
    hora, 
    alimentador, 
    dia_semana, 
    potencia_activa AS potencia_activa_real, 
    potencia_reactiva AS potencia_reactiva_real
FROM potencia_dep
ORDER BY fecha, hora, alimentador
"""
        # 8. Actualizar el procesador con la consulta simple
        processor_data = obtener_estado_procesador(QUERY_DB_TABLE_PROCESSOR_ID)
        if processor_data and "error" not in processor_data:
            revision = processor_data.get("revision", {})
            properties = dict(processor_data.get("component", {}).get("properties", {}))
            
            properties["db-fetch-sql-query"] = nueva_consulta
            properties["Custom Query"] = nueva_consulta
            properties["Table Name"] = "" # Forzar a usar Custom Query
            
            update_data = {
                "revision": revision,
                "component": {
                    "id": QUERY_DB_TABLE_PROCESSOR_ID,
                    "properties": properties
                }
            }
            
            response = requests.put(
                f"{MINI_PROXY_URL}/api/processors/{QUERY_DB_TABLE_PROCESSOR_ID}",
                json=update_data,
                timeout=HTTP_TIMEOUT
            )
            
            if response.status_code == 200:
                print(f"‚úÖ Consulta nuclear para mes {mes} actualizada correctamente")
            else:
                print(f"‚ö†Ô∏è Error al actualizar consulta nuclear: {response.status_code}")
                print(f"Respuesta: {response.text[:200]}...")
        
        # 9. Limpiar datos existentes en destino
        print("üóëÔ∏è Limpiando datos existentes en destino...")
        pg_hook_destino = PostgresHook(postgres_conn_id="postgres_base_prueba")
        pg_hook_destino.run(f"""
            DELETE FROM potencia_dep_processed 
            WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
        """)
        
        # 10. Configurar topic Kafka √∫nico para evitar confusiones
        topic_name = f"potencia-mes{mes}-seguro-{int(time.time())}"
        print(f"üîÑ Configurando topic Kafka √∫nico: {topic_name}")
        configurar_topic_procesador("bd27ce5a-5b0d-3ab6-b0bc-dfffb23b7671", "Topic Name", topic_name)
        configurar_topic_procesador("6476eeac-d24a-302e-33cd-9a96ac8c92af", "topic", topic_name)
        
        # 11. Limpiar estado y colas
        print("üßπ Limpiando estado del procesador y colas...")
        limpiar_estado_procesador(QUERY_DB_TABLE_PROCESSOR_ID)
        clear_processor_state_and_empty_all_queues()
        
        # 12. Iniciar procesadores
        print("‚ñ∂Ô∏è Iniciando grupos de procesadores completos...")
        iniciar_todos_procesadores_por_grupo()
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        
        # Intentar limpiar la tabla de trabajo en caso de error
        try:
            pg_hook = PostgresHook(postgres_conn_id="postgres_centrosur")
            
            # Limpiar tabla de trabajo si fue creada
            if 'tabla_trabajo' in locals():
                pg_hook.run(f"DROP TABLE IF EXISTS {tabla_trabajo}")
                print(f"üßπ Eliminada tabla de trabajo {tabla_trabajo} despu√©s de error")
            
            # Restaurar tabla original si se hizo backup
            if 'backup_tabla' in locals():
                print(f"üîÑ Restaurando tabla potencia_dep desde backup {backup_tabla}...")
                pg_hook.run("DROP TABLE IF EXISTS potencia_dep")
                pg_hook.run(f"ALTER TABLE {backup_tabla} RENAME TO potencia_dep")
                print("‚úÖ Tabla potencia_dep restaurada desde backup")
            
        except Exception as cleanup_error:
            print(f"‚ö†Ô∏è Error adicional durante limpieza: {str(cleanup_error)}")
            
        return False

def verificar_tablas_temporales():
    """Muestra todas las tablas temporales actualmente existentes"""
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_centrosur")
        query = """
        SELECT table_name FROM information_schema.tables 
        WHERE table_name LIKE 'potencia_dep_%'
        """
        tablas = pg_hook.get_records(query)
        print(f"‚ÑπÔ∏è Tablas temporales existentes ({len(tablas)}):")
        for tabla in tablas:
            print(f"  - {tabla[0]}")
        return tablas
    except Exception as e:
        print(f"‚ùå Error verificando tablas: {str(e)}")
        return []
    
def registrar_log(pg_hook, nivel, mensaje, alimentador=None, fecha=None, hora=None, detalles=None):
    """Registra eventos en la tabla de logs (versi√≥n corregida)"""
    try:
        # Crear un diccionario de detalles completo para JSONB
        detalles_completos = {
            'nivel': nivel,  # Mover nivel a detalles
            'fuente': 'potencia_integrador_anual_depuracion',
            'hora': hora
        }
        # Si hay detalles adicionales, a√±adirlos
        if detalles:
            detalles_completos.update(detalles)
            
        # Serializar detalles manejando decimales y objetos datetime correctamente
        if detalles_completos:
            detalles_json = json.dumps(detalles_completos, default=ensure_json_serializable)
        else:
            detalles_json = None
        
        # Consulta ajustada a la estructura real de la tabla
        query = """
        INSERT INTO log_procesamiento 
        (timestamp, mensaje, alimentador, fecha, detalles)
        VALUES (NOW(), %s, %s, %s, %s)
        """
        
        pg_hook.run(query, parameters=(
            mensaje,
            alimentador,
            fecha,
            detalles_json
        ))
    except Exception as e:
        print(f"Error al registrar log: {str(e)}")
        traceback.print_exc()  # Agregar esto para ver el trace completo

def obtener_completitud_mes(pg_hook, alimentador, mes, year=YEAR):
    """
    Calcula el porcentaje de completitud para un alimentador en un mes espec√≠fico.
    Retorna porcentaje de valores no cero, total de registros, y registros con valores.
    """
    try:
        # Determinar n√∫mero de d√≠as en el mes
        dias_en_mes = calendar.monthrange(year, mes)[1]
        total_registros_teoricos = dias_en_mes * 24  # 24 horas por d√≠a
        # Consultar registros existentes
        query = """
        SELECT 
            COUNT(*) as total_registros,
            SUM(CASE WHEN potencia_activa IS NOT NULL AND potencia_activa != 0 THEN 1 ELSE 0 END) as registros_activa,
            SUM(CASE WHEN potencia_reactiva IS NOT NULL AND potencia_reactiva != 0 THEN 1 ELSE 0 END) as registros_reactiva
        FROM potencia_dep_processed
        WHERE alimentador = %s
          AND EXTRACT(MONTH FROM fecha) = %s
          AND EXTRACT(YEAR FROM fecha) = %s
        """
        
        result = pg_hook.get_first(query, parameters=(alimentador, mes, year))
        
        if not result or not result[0]:
            return 0, 0, 0
            
        total_registros = result[0]
        registros_activa = result[1] or 0
        registros_reactiva = result[2] or 0
        
        # Calcular completitud (promedio de activa y reactiva)
        completitud_activa = (registros_activa / total_registros_teoricos) * 100 if total_registros_teoricos > 0 else 0
        completitud_reactiva = (registros_reactiva / total_registros_teoricos) * 100 if total_registros_teoricos > 0 else 0
        completitud_promedio = (completitud_activa + completitud_reactiva) / 2
        return completitud_promedio, total_registros, min(registros_activa, registros_reactiva)
        
    except Exception as e:
        print(f"Error calculando completitud para alimentador {alimentador}, mes {mes}: {str(e)}")
        return 0, 0, 0

def truncar_datos_enviados(mes, **kwargs):
    """
    Trunca los datos del mes especificado de la tabla datos_enviados
    despu√©s de verificar que est√©n correctamente almacenados en potencia_dep_processed.
    """
    try:
        print(f"üßπ Iniciando limpieza de datos intermedios para el mes {mes}...")
        
        # Conexi√≥n a la base de datos
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        
        # Primero verificar que los datos existan en la tabla final
        query_verificacion = f"""
        SELECT COUNT(*) FROM potencia_dep_processed 
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = 2019
        """
        count_final = pg_hook.get_first(query_verificacion)[0]
        
        if count_final == 0:
            print(f"‚ö†Ô∏è No se encontraron datos en la tabla final para mes {mes}. Omitiendo truncado.")
            return "No hay datos para limpiar"
        
        print(f"‚úÖ Verificados {count_final} registros en tabla final para mes {mes}")
        
        # Eliminar los datos de este mes de la tabla intermedia
        query_truncado = f"""
        DELETE FROM datos_enviados 
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = 2019
        """
        registros_eliminados = pg_hook.run(query_truncado)
        print(f"üßπ Se eliminaron {registros_eliminados} registros de la tabla intermedia")
        
        return f"Limpieza completada: {registros_eliminados} registros eliminados"
        
    except Exception as e:
        error_msg = f"‚ùå Error al truncar datos enviados: {str(e)}"
        print(error_msg)
        return error_msg

def calcular_consistencia_datos(pg_hook, alimentador, fecha_inicio, fecha_fin):
    """
    Calcula la consistencia de los datos para un alimentador en un per√≠odo.
    La consistencia eval√∫a si los valores mantienen relaciones l√≥gicas entre ellos.
    
    Returns:
        float: Porcentaje de consistencia (0-100%)
    """
    try:
        # Consulta para verificar relaciones l√≥gicas entre potencia activa y reactiva
        query = """
        SELECT 
            COUNT(*) as total,
            SUM(CASE 
                WHEN potencia_activa IS NOT NULL 
                     AND potencia_reactiva IS NOT NULL 
                     AND potencia_activa >= 0 
                     AND potencia_reactiva >= 0
                     AND potencia_activa >= potencia_reactiva * 0.3 -- Relaci√≥n l√≥gica m√≠nima
                     AND potencia_activa <= potencia_reactiva * 3.0 -- Relaci√≥n l√≥gica m√°xima
                THEN 1 ELSE 0 END) as consistentes
        FROM potencia_dep_processed
        WHERE alimentador = %s
          AND fecha::date >= %s::date 
          AND fecha::date < %s::date
          AND potencia_activa IS NOT NULL
          AND potencia_reactiva IS NOT NULL
          AND (potencia_activa != 0 OR potencia_reactiva != 0)
        """
        resultado = pg_hook.get_first(query, parameters=(alimentador, fecha_inicio, fecha_fin))
        if not resultado or resultado[0] == 0:
            return 0.0
        
        total = resultado[0]
        consistentes = resultado[1] or 0
        
        return (consistentes / total) * 100 if total > 0 else 0.0
        
    except Exception as e:
        print(f"Error calculando consistencia: {str(e)}")
        return 0.0

def calcular_continuidad_datos(pg_hook, alimentador, fecha_inicio, fecha_fin):
    """
    Calcula la continuidad de los datos (ausencia de brechas) para un alimentador en un per√≠odo.
    
    Returns:
        float: Porcentaje de continuidad (0-100%)
    """
    try:
        # Primero calculamos el total te√≥rico de registros que deber√≠an existir
        query_dias = """
        SELECT (%s::date - %s::date)::integer as dias
        """
        dias = pg_hook.get_first(query_dias, parameters=(fecha_fin, fecha_inicio))[0]
        total_teorico = dias * 24  # 24 horas por d√≠a
        
        # Luego contamos registros existentes
        query_existentes = """
        SELECT COUNT(DISTINCT (fecha::date, hora)) 
        FROM potencia_dep_processed
        WHERE alimentador = %s
          AND fecha::date >= %s::date 
          AND fecha::date < %s::date
          AND potencia_activa IS NOT NULL
          AND potencia_reactiva IS NOT NULL
          AND (potencia_activa != 0 OR potencia_reactiva != 0)
        """
        existentes = pg_hook.get_first(query_existentes, parameters=(alimentador, fecha_inicio, fecha_fin))[0]
        return (existentes / total_teorico) * 100 if total_teorico > 0 else 0.0
        
    except Exception as e:
        print(f"Error calculando continuidad: {str(e)}")
        return 0.0

def detectar_outliers(pg_hook, alimentador, fecha_inicio, fecha_fin):
    """
    Detecta valores at√≠picos (outliers) para un alimentador en un per√≠odo.
    Utiliza el m√©todo de desviaci√≥n est√°ndar (¬±3œÉ).
    
    Returns:
        list: Lista de registros identificados como outliers
    """
    try:
        # Primero obtenemos estad√≠sticas descriptivas
        query_estadisticas = """
        SELECT 
            AVG(potencia_activa) as media_activa,
            STDDEV(potencia_activa) as std_activa,
            AVG(potencia_reactiva) as media_reactiva,
            STDDEV(potencia_reactiva) as std_reactiva
        FROM potencia_dep_processed
        WHERE alimentador = %s
          AND fecha::date >= %s::date 
          AND fecha::date < %s::date
          AND potencia_activa IS NOT NULL
          AND potencia_reactiva IS NOT NULL
          AND potencia_activa != 0
          AND potencia_reactiva != 0
        """
        estadisticas = pg_hook.get_first(query_estadisticas, parameters=(alimentador, fecha_inicio, fecha_fin))
        if not estadisticas or estadisticas[0] is None:
            return []
            
        media_activa, std_activa, media_reactiva, std_reactiva = estadisticas
        
        # Definir umbrales
        umbral_inferior_activa = media_activa - 3 * std_activa if std_activa else 0
        umbral_superior_activa = media_activa + 3 * std_activa if std_activa else float('inf')
        umbral_inferior_reactiva = media_reactiva - 3 * std_reactiva if std_reactiva else 0
        umbral_superior_reactiva = media_reactiva + 3 * std_reactiva if std_reactiva else float('inf')
        
        # Buscar outliers
        query_outliers = """
        SELECT fecha::date, hora, potencia_activa, potencia_reactiva
        FROM potencia_dep_processed
        WHERE alimentador = %s
          AND fecha::date >= %s::date 
          AND fecha::date < %s::date
          AND (
            (potencia_activa < %s OR potencia_activa > %s)
            OR
            (potencia_reactiva < %s OR potencia_reactiva > %s)
          )
          AND potencia_activa IS NOT NULL
          AND potencia_reactiva IS NOT NULL
          AND potencia_activa != 0
          AND potencia_reactiva != 0
        """
        
        outliers = pg_hook.get_records(query_outliers, parameters=(
            alimentador, fecha_inicio, fecha_fin,
            umbral_inferior_activa, umbral_superior_activa,
            umbral_inferior_reactiva, umbral_superior_reactiva
        ))
        # Convertir a lista de diccionarios para mejor manipulaci√≥n
        resultado = []
        for fecha, hora, p_activa, p_reactiva in outliers:
            resultado.append({
                'fecha': fecha.strftime('%Y-%m-%d') if hasattr(fecha, 'strftime') else str(fecha),
                'hora': hora,
                'potencia_activa': float(p_activa),
                'potencia_reactiva': float(p_reactiva)
            })
            
        return resultado
    except Exception as e:
        print(f"Error detectando outliers: {str(e)}")
        return []
    
def obtener_calidad_mes(pg_hook, alimentador, mes, year=YEAR):
    """
    Calcula el porcentaje de calidad de integridad para un alimentador en un mes espec√≠fico.
    La calidad se define como el porcentaje de registros que no tienen valores cero en ambas potencias.
    
    Args:
        pg_hook: PostgreSQL hook para conexi√≥n a base de datos
        alimentador: ID del alimentador
        mes: N√∫mero del mes (1-12)
        year: A√±o a procesar (default: valor global YEAR)
        
    Returns:
        float: Porcentaje de calidad (0-100%)
    """
    try:
        query = """
        SELECT 
            COUNT(*) as total,
            SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) as ceros,
            ROUND(100 - (SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) * 100.0) / COUNT(*), 2) as calidad
        FROM potencia_dep_processed
        WHERE alimentador = %s
          AND EXTRACT(MONTH FROM fecha) = %s
          AND EXTRACT(YEAR FROM fecha) = %s
        """
        
        resultado = pg_hook.get_first(query, parameters=(alimentador, mes, year))
        
        if not resultado or resultado[0] == 0:
            return 0.0  # Si no hay datos, calidad es 0
            
        # El tercer valor es la calidad calculada en la consulta
        calidad = resultado[2] or 0.0
        
        return calidad
        
    except Exception as e:
        print(f"Error calculando calidad del mes para {alimentador}, mes {mes}: {str(e)}")
        return 0.0

def buscar_dato_referencia_mejorado(pg_hook, alimentador, fecha, hora, mes_actual, year=YEAR):
    """
    B√∫squeda inteligente de datos de referencia considerando casos especiales como d√≠as 31
    que no existen en todos los meses.
    """
    try:
        # Convertir fecha a objeto datetime para manipulaci√≥n
        if isinstance(fecha, str):
            fecha_dt = datetime.strptime(fecha, '%Y-%m-%d')
        else:
            fecha_dt = fecha
            
        dia = fecha_dt.day
        mes_fecha = fecha_dt.month
        
        print(f"üîç Buscando referencia para {alimentador}, {fecha}, hora {hora}")
        
        # Si es d√≠a 29, 30 o 31, necesitamos estrategia especial para febrero y otros meses
        dia_especial = dia >= 29
        
        # 1. Primero buscar en meses posteriores hasta mes 6 (prioridad alta)
        for mes_ref in range(mes_actual, min(7, 13)):  # Incluir el mes 6 pero no ir m√°s all√° de diciembre
            # Para d√≠as especiales (29-31), ajustar al √∫ltimo d√≠a del mes de referencia
            if dia_especial:
                ultimo_dia_mes_ref = calendar.monthrange(year, mes_ref)[1]
                dia_ref = min(dia, ultimo_dia_mes_ref)
                fecha_ref = f"{year}-{mes_ref:02d}-{dia_ref:02d}"
            else:
                fecha_ref = f"{year}-{mes_ref:02d}-{dia:02d}"
                
            # Buscar para la misma hora
            query = """
            SELECT potencia_activa, potencia_reactiva
            FROM potencia_dep_processed
            WHERE alimentador = %s
            AND fecha::date = %s::date
            AND hora = %s
            AND potencia_activa > 0
            AND potencia_reactiva != 0
            """
            resultado = pg_hook.get_first(query, parameters=(alimentador, fecha_ref, hora))
            if resultado and resultado[0] is not None and resultado[1] is not None:
                print(f"‚úÖ Encontrada referencia en mes {mes_ref}, d√≠a {fecha_ref}")
                return {
                    'potencia_activa': float(resultado[0]),
                    'potencia_reactiva': float(resultado[1]),
                    'fuente': f"mes-futuro-{mes_ref}-exacto"
                }
        
        # 2. Luego buscar en meses anteriores (prioridad media)
        for mes_ref in range(mes_fecha-1, 0, -1):
            if dia_especial:
                ultimo_dia_mes_ref = calendar.monthrange(year, mes_ref)[1]
                dia_ref = min(dia, ultimo_dia_mes_ref)
                fecha_ref = f"{year}-{mes_ref:02d}-{dia_ref:02d}"
            else:
                fecha_ref = f"{year}-{mes_ref:02d}-{dia:02d}"
                
            query = """
            SELECT potencia_activa, potencia_reactiva
            FROM potencia_dep_processed
            WHERE alimentador = %s
            AND fecha::date = %s::date
            AND hora = %s
            AND potencia_activa > 0
            AND potencia_reactiva != 0
            """
            
            resultado = pg_hook.get_first(query, parameters=(alimentador, fecha_ref, hora))
            
            if resultado and resultado[0] is not None and resultado[1] is not None:
                print(f"‚úÖ Encontrada referencia en mes anterior {mes_ref}, d√≠a {fecha_ref}")
                return {
                    'potencia_activa': float(resultado[0]),
                    'potencia_reactiva': float(resultado[1]),
                    'fuente': f"mes-anterior-{mes_ref}-exacto"
                }
        
        # 3. Buscar en el mismo d√≠a pero en otros horarios (√∫ltimo recurso)
        query_hora_cercana = """
        SELECT hora, potencia_activa, potencia_reactiva
        FROM potencia_dep_processed
        WHERE alimentador = %s
        AND fecha::date = %s::date
        AND potencia_activa > 0
        AND potencia_reactiva != 0
        ORDER BY ABS(EXTRACT(HOUR FROM hora) - EXTRACT(HOUR FROM %s::time))
        LIMIT 1
        """
        # Si estamos en mes 6 o posterior, buscar directamente en el mes actual
        if mes_actual >= 6:
            resultado = pg_hook.get_first(query_hora_cercana, parameters=(
                alimentador, fecha_dt.strftime('%Y-%m-%d'), hora
            ))
            
            if resultado and resultado[1] is not None and resultado[2] is not None:
                print(f"‚úÖ Encontrada referencia en mismo d√≠a, hora cercana {resultado[0]}")
                return {
                    'potencia_activa': float(resultado[1]),
                    'potencia_reactiva': float(resultado[2]),
                    'fuente': f"mismo-dia-hora-cercana"
                }
        # 4. Si llegamos a mes 6, usar promedio del mes para ese alimentador
        if mes_actual >= 6:
            query_promedio_mes6 = """
            SELECT AVG(potencia_activa), AVG(potencia_reactiva)
            FROM potencia_dep_processed
            WHERE alimentador = %s
            AND EXTRACT(MONTH FROM fecha) = 6
            AND EXTRACT(YEAR FROM fecha) = %s
            AND potencia_activa > 0
            AND potencia_reactiva != 0
            """
            
            resultado = pg_hook.get_first(query_promedio_mes6, parameters=(alimentador, year))
            
            if resultado and resultado[0] is not None and resultado[1] is not None:
                print(f"‚úÖ Usando promedio del mes 6 para {alimentador}")
                return {
                    'potencia_activa': float(resultado[0]),
                    'potencia_reactiva': float(resultado[1]),
                    'fuente': "promedio-mes6"
                }
        
        # 5. Si a√∫n no encontramos nada, marcar como standby si estamos antes de mes 6
        if mes_actual < 6:
            motivo = f"No se encontraron datos de referencia para fecha {fecha}, hora {hora}"
            # Marcaremos en standby posteriormente
            print(f"‚ö†Ô∏è Alimentador {alimentador} en fecha {fecha} hora {hora} se mantiene en STANDBY: {motivo}")
        else:
            print(f"‚ö†Ô∏è No se encontraron datos de referencia para {alimentador} en fecha {fecha}, hora {hora}")
        
        # 6. √öltimo recurso: usar valores t√≠picos por hora del d√≠a
        hora_num = int(str(hora).split(':')[0])
        if hora_num >= 6 and hora_num <= 18:  # Horas diurnas
            return {
                'potencia_activa': 300.0,
                'potencia_reactiva': 100.0,
                'fuente': 'valor-tipico-diurno'
            }
        else:  # Horas nocturnas
            return {
                'potencia_activa': 200.0, 
                'potencia_reactiva': 50.0,
                'fuente': 'valor-tipico-nocturno'
            }
    
    except Exception as e:
        print(f"‚ùå Error en b√∫squeda de referencia mejorada: {str(e)}")
        traceback.print_exc()
        # Valor por defecto en caso de error
        return {
            'potencia_activa': 250.0,
            'potencia_reactiva': 75.0,
            'fuente': 'error-fallback'
        }

def procesar_standby_pendientes(mes_actual, **kwargs):
    """
    Procesa todos los registros standby pendientes cuando llegamos a mes 6.
    Libera los registros que hayan superado el l√≠mite de espera.
    """
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")

        if mes_actual >= 6:
            print(f"üîç Procesando todos los registros standby pendientes (mes actual: {mes_actual})")
            
            # 1. Liberar standby anteriores al mes 6
            query_liberar = """
            UPDATE registros_standby 
            SET activo = FALSE, 
                fecha_solucion = NOW(), 
                motivo_solucion = 'L√≠mite de espera (mes 6) alcanzado'
            WHERE activo = TRUE 
            AND EXTRACT(MONTH FROM fecha) < 6
            AND EXTRACT(YEAR FROM fecha) = 2019
            RETURNING alimentador, fecha, hora
            """
            
            registros_liberados = pg_hook.get_records(query_liberar)
            print(f"üìä {len(registros_liberados)} registros liberados por alcanzar l√≠mite de espera")
            
            # 2. Procesar todos los registros pendientes con datos del mes 6
            for mes_origen in range(1, 6):
                query_pendientes = """
                SELECT alimentador, fecha, hora
                FROM registros_standby
                WHERE EXTRACT(MONTH FROM fecha) = %s
                AND EXTRACT(YEAR FROM fecha) = 2019
                AND activo = TRUE
                """
                pendientes = pg_hook.get_records(query_pendientes, parameters=(mes_origen,))
                if pendientes:
                    print(f"üìã Procesando {len(pendientes)} registros pendientes del mes {mes_origen}")
                    
                    for registro in pendientes:
                        alimentador, fecha, hora = registro
                        
                        # Buscar datos en mes 6 para corregir
                        dato_referencia = buscar_dato_referencia_mes(pg_hook, alimentador, fecha, hora, 6, 2019)
                        
                        if dato_referencia:
                            actualizar_registro_standby(
                                pg_hook,
                                alimentador,
                                fecha,
                                hora,
                                dato_referencia.get('potencia_activa', 0),
                                dato_referencia.get('potencia_reactiva', 0),
                                f"Corregido con datos del mes 6 (procesamiento masivo)"
                            )
            
            return True
        return False
    except Exception as e:
        print(f"‚ùå Error procesando standby pendientes: {str(e)}")
        traceback.print_exc()
        return False

def marcar_alimentador_standby(pg_hook, alimentador, mes, motivo):
    """Marca un alimentador como en standby en la tabla de seguimiento"""
    query = """
    INSERT INTO alimentadores_standby (alimentador, mes_origen, fecha_standby, motivo, activo)
    VALUES (%s, %s, NOW(), %s, TRUE)
    """
    pg_hook.run(query, parameters=(alimentador, mes, motivo))
    
    # Log espec√≠fico de entrada en standby
    registrar_log(
        pg_hook, 'STANDBY', 
        f'Alimentador {alimentador} puesto en STANDBY para mes {mes}: {motivo}',
        alimentador=alimentador,
        detalles={'mes_origen': mes, 'motivo': motivo}
    )

def analizar_calidad_integridad(pg_hook, mes, year=YEAR):
    """
    Analiza la calidad de integridad de los datos por alimentador para un mes espec√≠fico.
    Identifica alimentadores que necesitan depuraci√≥n y los que pueden servir como referencia.
    """
    try:
        mes_siguiente = mes + 1 if mes < 12 else 1
        ano_siguiente = year if mes < 12 else year + 1
        fecha_inicio = f"{year}-{mes:02d}-01"
        fecha_fin = f"{ano_siguiente}-{mes_siguiente:02d}-01"
        
        query = f"""
        SELECT 
            alimentador,
            COUNT(*) AS total_datos,
            SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) AS datos_ceros,
            ROUND(
                (SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) * 100.0) / COUNT(*),
                2
            ) AS porcentaje_ceros,
            ROUND(
                100 - (SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) * 100.0) / COUNT(*),
                2
            ) AS calidad_integridad
        FROM datos_enviados
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = {year}
        GROUP BY alimentador
        ORDER BY calidad_integridad DESC
        """
        
        resultados = pg_hook.get_records(query)
        
        alimentadores_perfectos = []
        alimentadores_a_depurar = []
        
        for row in resultados:
            alimentador, total, ceros, pct_ceros, calidad = row
            
            if calidad == 100:
                alimentadores_perfectos.append({
                    'alimentador': alimentador,
                    'total_datos': total,
                    'calidad': calidad
                })
            else:
                alimentadores_a_depurar.append({
                    'alimentador': alimentador,
                    'total_datos': total,
                    'datos_ceros': ceros,
                    'calidad': calidad
                })
        print(f"‚úÖ An√°lisis de calidad completado: {len(alimentadores_perfectos)} alimentadores perfectos, {len(alimentadores_a_depurar)} requieren depuraci√≥n")
        
        return {
            'alimentadores_perfectos': alimentadores_perfectos,
            'alimentadores_a_depurar': alimentadores_a_depurar
        }
    except Exception as e:
        print(f"‚ùå Error al analizar calidad: {str(e)}")
        return {'alimentadores_perfectos': [], 'alimentadores_a_depurar': []}

def determinar_nivel_calidad(completitud, consistencia, continuidad, outliers):
    """
    Determina el nivel de calidad general basado en varias m√©tricas.
    
    Args:
        completitud: Porcentaje de datos completos (0-100%)
        consistencia: Porcentaje de datos consistentes (0-100%)
        continuidad: Porcentaje de continuidad de datos (0-100%)
        outliers: Lista de outliers detectados
        
    Returns:
        str: Nivel de calidad ('EXCELENTE', 'BUENO', 'REGULAR', 'BAJO')
    """
    try:
        # Factor de penalizaci√≥n por outliers
        factor_outliers = min(1.0, 1.0 - (len(outliers) / 1000)) if isinstance(outliers, list) else 0.95
        
        # Calcular puntaje promedio ponderado
        puntaje = (
            completitud * 0.4 +  # 40% de peso para completitud
            consistencia * 0.3 +  # 30% de peso para consistencia
            continuidad * 0.3     # 30% de peso para continuidad
        ) * factor_outliers
        
        # Determinar nivel
        if puntaje >= 90:
            return 'EXCELENTE'
        elif puntaje >= 75:
            return 'BUENO'
        elif puntaje >= 50:
            return 'REGULAR'
        else:
            return 'BAJO'
    except Exception as e:
        print(f"Error determinando nivel de calidad: {str(e)}")
        return 'INDETERMINADO'
    
def crear_tabla_registros_standby():
    """Crea la tabla para registrar datos en standby"""
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        sql = """
        CREATE TABLE IF NOT EXISTS registros_standby (
            id SERIAL PRIMARY KEY,
            alimentador VARCHAR(50) NOT NULL,
            mes_origen INTEGER NOT NULL,
            fecha DATE NOT NULL,
            hora TIME NOT NULL,
            motivo TEXT,
            activo BOOLEAN DEFAULT TRUE,
            fecha_standby TIMESTAMP DEFAULT NOW(),
            fecha_solucion TIMESTAMP,
            motivo_solucion TEXT
        );
        """
        pg_hook.run(sql)
        print("‚úÖ Tabla registros_standby creada/verificada correctamente")
        return True
    except Exception as e:
        print(f"‚ùå Error creando tabla registros_standby: {str(e)}")
        return False
    
def corregir_standby_anteriores(pg_hook, productor, alimentador, mes_actual, year=YEAR):
    """
    Verifica si hay registros en standby de meses anteriores y los corrige
    usando datos del mes actual como referencia
    """
    try:
        # Obtener registros en standby para este alimentador
        query_standby = """
        SELECT alimentador, mes_origen, fecha, hora 
        FROM registros_standby
        WHERE alimentador = %s 
        AND mes_origen < %s 
        AND activo = TRUE
        ORDER BY mes_origen
        """
        
        registros_standby = pg_hook.get_records(query_standby, parameters=(alimentador, mes_actual))
        
        if not registros_standby:
            print(f"‚úÖ No hay registros en standby para alimentador {alimentador}")
            return
        
        print(f"üîÑ Encontrados {len(registros_standby)} registros en standby para corregir")
        
        # Patrones para buscar correcciones - misma hora en d√≠as equivalentes
        for standby in registros_standby:
            alim, mes_origen, fecha_standby, hora_standby = standby
            
            # Convertir fecha a d√≠a del mes
            fecha_dt = datetime.strptime(str(fecha_standby), '%Y-%m-%d')
            dia_standby = fecha_dt.day
            
            # Buscar datos v√°lidos en el mes actual para la misma hora
            query_referencia = """
            SELECT fecha, potencia_activa, potencia_reactiva
            FROM datos_enviados
            WHERE alimentador = %s
            AND EXTRACT(MONTH FROM fecha) = %s
            AND EXTRACT(YEAR FROM fecha) = %s
            AND hora = %s
            AND potencia_activa != 0
            AND potencia_reactiva != 0
            ORDER BY ABS(EXTRACT(DAY FROM fecha) - %s)
            LIMIT 1
            """
            # Buscar fecha cercana al mismo d√≠a del mes
            referencia = pg_hook.get_first(query_referencia, 
                parameters=(alimentador, mes_actual, year, hora_standby, dia_standby))
            
            if referencia and referencia[1] and referencia[2]:
                # Se encontr√≥ dato v√°lido para corregir el standby
                fecha_ref, p_activa, p_reactiva = referencia
                
                # Actualizar el registro en potencia_dep_processed
                query_update = """
                UPDATE potencia_dep_processed
                SET potencia_activa = %s, 
                    potencia_reactiva = %s
                WHERE alimentador = %s
                AND fecha = %s
                AND hora = %s
                """
                pg_hook.run(query_update, parameters=(
                    p_activa, p_reactiva, alimentador, fecha_standby, hora_standby
                ))
                # Marcar como resuelto en la tabla de standby
                query_resolved = """
                UPDATE registros_standby
                SET activo = FALSE,
                    fecha_solucion = NOW(),
                    motivo_solucion = %s
                WHERE alimentador = %s
                AND fecha = %s
                AND hora = %s
                """
                
                motivo = f"Corregido con datos del mes {mes_actual}, valor: {p_activa}/{p_reactiva}"
                pg_hook.run(query_resolved, parameters=(
                    motivo, alimentador, fecha_standby, hora_standby
                ))
                
                # Registrar correcci√≥n en log
                registrar_log(
                    pg_hook, 'INFO',
                    f'Corregido registro standby: alimentador {alimentador}, fecha {fecha_standby}, hora {hora_standby}',
                    alimentador=alimentador,
                    fecha=fecha_standby,
                    detalles={
                        'mes_origen': mes_origen,
                        'mes_solucion': mes_actual,
                        'potencia_activa': float(p_activa),
                        'potencia_reactiva': float(p_reactiva)
                    }
                )
                print(f"‚úÖ Corregido registro standby: {fecha_standby} {hora_standby} con datos de {fecha_ref}")
                # Publicar el registro corregido a Kafka
                topic_datos_depurados = "potencia_datos_depurados"
                mensaje_kafka = {
                    'fecha': str(fecha_standby),
                    'hora': str(hora_standby),
                    'alimentador': alimentador,
                    'potencia_activa': float(p_activa),
                    'potencia_reactiva': float(p_reactiva),
                    'correccion_standby': True,
                    'mes_origen': mes_origen,
                    'mes_correccion': mes_actual
                }
                productor.send(topic_datos_depurados, mensaje_kafka)
            else:
                print(f"‚ö†Ô∏è No se encontr√≥ referencia para corregir standby: {fecha_standby} {hora_standby}")
        
        return True
    except Exception as e:
        print(f"‚ùå Error corrigiendo standby: {str(e)}")
        return False

def marcar_registro_standby(pg_hook, alimentador, mes, fecha, hora, motivo):
    """Marca un registro espec√≠fico como en standby para correcci√≥n futura"""
    query = """
    INSERT INTO registros_standby 
    (alimentador, mes_origen, fecha, hora, motivo, activo, fecha_standby)
    VALUES (%s, %s, %s, %s, %s, TRUE, NOW())
    """
    try:
        pg_hook.run(query, parameters=(alimentador, mes, fecha, hora, motivo))
        print(f"‚è≥ Registro standby: alimentador {alimentador}, fecha {fecha}, hora {hora}")
        # Log de registro en standby
        registrar_log(
            pg_hook, 
            'STANDBY', 
            f'Registro puesto en STANDBY: alimentador {alimentador}, fecha {fecha}, hora {hora}',
            alimentador=alimentador,
            fecha=fecha,
            hora=hora,
            detalles={'mes_origen': mes, 'motivo': motivo}
        )
    except Exception as e:
        print(f"Error marcando registro standby: {str(e)}")

def verificar_completitud_registros():
    """
    Funci√≥n que verifica completitud de 96 registros diarios por alimentador
    e identifica timestamps espec√≠ficos faltantes.
    """
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        
        # 1. Identificar d√≠as con registros incompletos
        query_incompletos = """
        SELECT
            alimentador,
            fecha,
            COUNT(*) AS total_registros
        FROM
            potencia_dep_processed
        GROUP BY
            alimentador,
            fecha
        HAVING
            COUNT(*) <> 96
        ORDER BY
            alimentador,
            fecha;
        """
        
        dias_incompletos = pg_hook.get_records(query_incompletos)
        
        # 2. Para cada d√≠a incompleto, identificar timestamps espec√≠ficos faltantes
        resultados = []
        for alimentador, fecha, total in dias_incompletos:
            # Generamos todos los timestamps que deber√≠an existir (cada 15 minutos)
            query_faltantes = """
            WITH todas_horas AS (
                SELECT generate_series(0, 23) AS hora
            ),
            todos_minutos AS (
                SELECT generate_series(0, 45, 15) AS minuto
            ),
            todos_timestamps AS (
                SELECT 
                    %s::date AS fecha,
                    make_time(hora, minuto, 0) AS hora
                FROM todas_horas, todos_minutos
            ),
            existentes AS (
                SELECT fecha, hora
                FROM potencia_dep_processed
                WHERE alimentador = %s AND fecha = %s::date
            )
            SELECT 
                tt.fecha,
                tt.hora
            FROM todos_timestamps tt
            LEFT JOIN existentes e ON tt.fecha = e.fecha AND tt.hora = e.hora
            WHERE e.fecha IS NULL
            ORDER BY tt.hora;
            """
            
            timestamps_faltantes = pg_hook.get_records(query_faltantes, 
                                                    parameters=(fecha, alimentador, fecha))
            
            # Convertir objetos time a cadenas para JSON serialization
            faltantes_serializables = [str(ts[1]) for ts in timestamps_faltantes]
            
            # Guardar resultados
            resultados.append({
                'alimentador': alimentador,
                'fecha': str(fecha),  # Convertir fecha a string para serializaci√≥n
                'total_existentes': total,
                'total_faltantes': 96 - total,
                'timestamps_faltantes': faltantes_serializables
            })
            
        # 3. Registrar resultados en tabla de monitoreo
        for resultado in resultados:
            registrar_log(
                pg_hook,
                'ALERTA',
                f'Completitud incompleta: alimentador {resultado["alimentador"]}, fecha {resultado["fecha"]}',
                alimentador=resultado["alimentador"],
                fecha=resultado["fecha"],
                detalles={
                    'total_existentes': resultado["total_existentes"],
                    'total_faltantes': resultado["total_faltantes"],
                    'timestamps_faltantes': resultado["timestamps_faltantes"]
                }
            )
            
        return resultados if resultados else []
        
    except Exception as e:
        print(f"Error verificando completitud: {str(e)}")
        return []

def resolver_caso_anomalo(alimentador=None, fecha=None, hora=None):
    """
    Resuelve un caso espec√≠fico de anomal√≠a en un alimentador
    mostrando el proceso paso a paso. Si no se especifican par√°metros,
    detectar√° autom√°ticamente casos an√≥malos.
    
    Args:
        alimentador: ID del alimentador (o None para detecci√≥n autom√°tica)
        fecha: Fecha en formato 'YYYY-MM-DD' (o None para detecci√≥n autom√°tica)
        hora: Hora en formato 'HH:MM:SS' (o None para detecci√≥n autom√°tica)
    """
    try:
        # Establecer conexi√≥n
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        
        # Si no se proporcionan par√°metros, detectar autom√°ticamente un caso an√≥malo
        if not alimentador or not fecha or not hora:
            print("üîç Detectando autom√°ticamente un caso an√≥malo")
            
            # Usar la consulta de anomal√≠as para encontrar un caso
            query_anomalia = """
            WITH estadisticas_alimentador AS (
                SELECT
                    alimentador,
                    AVG(potencia_activa) AS media_activa,
                    STDDEV(potencia_activa) AS std_activa,
                    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY potencia_activa) AS p95_activa,
                    PERCENTILE_CONT(0.05) WITHIN GROUP (ORDER BY potencia_activa) AS p05_activa
                FROM potencia_dep_processed
                GROUP BY alimentador
            )
            SELECT
                p.alimentador,
                p.fecha,
                p.hora,
                p.potencia_activa,
                p.potencia_reactiva,
                ABS(p.potencia_activa - e.media_activa) / GREATEST(e.std_activa, 0.001) AS desviacion_normalizada
            FROM potencia_dep_processed p
            JOIN estadisticas_alimentador e ON p.alimentador = e.alimentador
            WHERE (p.potencia_activa > e.p95_activa + 2 * e.std_activa
                OR (p.potencia_activa < e.p05_activa - e.std_activa AND p.potencia_activa > 0))
            ORDER BY desviacion_normalizada DESC
            LIMIT 1
            """
            
            anomalia = pg_hook.get_first(query_anomalia)
            
            if anomalia:
                alimentador = anomalia[0]
                fecha = anomalia[1]
                hora = anomalia[2]
                valor_actual_activa = float(anomalia[3])
                valor_actual_reactiva = float(anomalia[4])
                desviacion = float(anomalia[5])
                
                print(f"üîç Caso an√≥malo detectado autom√°ticamente:")
                print(f"- Alimentador: {alimentador}")
                print(f"- Fecha: {fecha}")
                print(f"- Hora: {hora}")
                print(f"- Potencia activa: {valor_actual_activa}")
                print(f"- Desviaci√≥n: {desviacion:.2f} sigmas")
            else:
                # Si no hay par√°metros y tampoco se detecta autom√°ticamente, usar el caso 5015 original
                alimentador = '5015'
                fecha = '2019-01-03'
                hora = '13:00:00'
                print(f"‚ö†Ô∏è No se detectaron anomal√≠as. Usando caso predeterminado: {alimentador}, {fecha} {hora}")
        
        print(f"üîç Analizando caso espec√≠fico: Alimentador {alimentador}, {fecha} {hora}")
        
        # PASO 1: Verificar el valor actual
        query_valor_actual = """
        SELECT potencia_activa, potencia_reactiva, dia_semana
        FROM potencia_dep_processed
        WHERE alimentador = %s AND fecha = %s AND hora = %s
        """
        
        valor_actual = pg_hook.get_first(
            query_valor_actual, 
            parameters=(alimentador, fecha, hora)
        )
        
        if valor_actual:
            print(f"Valor actual: P_activa={valor_actual[0]}, P_reactiva={valor_actual[1]}")
        else:
            print("No existe registro para esta combinaci√≥n")
            
        # PASO 2: Obtener valores del entorno (mismo d√≠a)
        valores_entorno = obtener_valores_entorno(pg_hook, alimentador, fecha, hora)
        
        print("Valores de entorno:")
        print("- Valores anteriores:")
        for v in valores_entorno['antes']:
            print(f"  {v['hora']}: P_activa={v['potencia_activa']}, P_reactiva={v['potencia_reactiva']}")
            
        print("- Valores posteriores:")
        for v in valores_entorno['despues']:
            print(f"  {v['hora']}: P_activa={v['potencia_activa']}, P_reactiva={v['potencia_reactiva']}")
            
        # PASO 3: Obtener valores hom√≥logos (mismo d√≠a de la semana, misma hora)
        fecha_dt = datetime.strptime(str(fecha), '%Y-%m-%d') if isinstance(fecha, str) else fecha
        dia_semana = fecha_dt.weekday()
        
        query_homologos = """
        SELECT 
            fecha, 
            hora, 
            potencia_activa, 
            potencia_reactiva
        FROM potencia_dep_processed
        WHERE 
            alimentador = %s AND
            EXTRACT(DOW FROM fecha) = %s AND
            hora = %s::time AND
            fecha != %s::date AND
            fecha >= %s::date - INTERVAL '4 weeks' AND
            fecha <= %s::date + INTERVAL '4 weeks' AND
            potencia_activa > 0
        ORDER BY 
            ABS(EXTRACT(EPOCH FROM (%s::date - fecha)) / 86400)
        LIMIT 5
        """
        
        homologos = pg_hook.get_records(
            query_homologos, 
            parameters=(
                alimentador, 
                dia_semana, 
                hora, 
                fecha,
                fecha,
                fecha,
                fecha
            )
        )
        
        print(f"\nValores hom√≥logos (otros {calendar.day_name[dia_semana]} a las {hora}):")
        for h in homologos:
            print(f"  {h[0]}: P_activa={h[2]}, P_reactiva={h[3]}")
            
        # PASO 4: Calcular valor de imputaci√≥n
        valor_imputado = calcular_imputacion(homologos, valores_entorno)
        
        print("\nValor calculado para imputaci√≥n:")
        print(f"- M√©todo: {valor_imputado['metodo']}")
        print(f"- Confianza: {valor_imputado['confianza']}")
        print(f"- P_activa: {valor_imputado['potencia_activa']}")
        print(f"- P_reactiva: {valor_imputado['potencia_reactiva']}")
        
        # PASO 5: Validar y ajustar
        valor_final = validar_y_ajustar_imputacion(valor_imputado, valores_entorno, 0.15)
        
        print("\nValor final validado:")
        print(f"- Estado: {valor_final['estado']}")
        print(f"- P_activa: {valor_final['potencia_activa']}")
        print(f"- P_reactiva: {valor_final['potencia_reactiva']}")
        
        if 'ajuste_aplicado' in valor_final and valor_final['ajuste_aplicado']:
            print("Se aplic√≥ ajuste para evitar pico an√≥malo:")
            print(f"- Referencia activa: {valor_final['referencia_activa']}")
            print(f"- L√≠mites permitidos: [{valor_final['limite_inferior_activa']:.4f}, {valor_final['limite_superior_activa']:.4f}]")
            
        # PASO 6: Comparar con el valor actual
        if valor_actual:
            diferencia_activa = abs(valor_actual[0] - valor_final['potencia_activa'])
            diferencia_porcentaje = diferencia_activa / valor_final['potencia_activa'] * 100 if valor_final['potencia_activa'] > 0 else 0
            
            print(f"\nDiferencia con valor actual: {diferencia_activa:.4f} ({diferencia_porcentaje:.2f}%)")
            
            # Determinar si el valor actual es un pico an√≥malo
            if diferencia_porcentaje > 15:
                print("‚ö†Ô∏è CONFIRMADO: El valor actual es un pico an√≥malo que debe ser corregido")
                
                # Obtener percentiles del d√≠a completo para contextualizar
                query_percentiles = """
                WITH datos_dia AS (
                    SELECT potencia_activa
                    FROM potencia_dep_processed
                    WHERE alimentador = %s AND fecha = %s
                )
                SELECT 
                    percentile_cont(0.25) WITHIN GROUP (ORDER BY potencia_activa) AS p25,
                    percentile_cont(0.5) WITHIN GROUP (ORDER BY potencia_activa) AS p50,
                    percentile_cont(0.75) WITHIN GROUP (ORDER BY potencia_activa) AS p75,
                    percentile_cont(0.9) WITHIN GROUP (ORDER BY potencia_activa) AS p90
                FROM datos_dia
                """
                
                percentiles = pg_hook.get_first(
                    query_percentiles, 
                    parameters=(alimentador, fecha)
                )
                
                if percentiles:
                    print(f"Contexto estad√≠stico del d√≠a:")
                    print(f"- P25: {percentiles[0]:.4f}")
                    print(f"- P50 (mediana): {percentiles[1]:.4f}")
                    print(f"- P75: {percentiles[2]:.4f}")
                    print(f"- P90: {percentiles[3]:.4f}")
                    
                    if valor_actual[0] > percentiles[3]:
                        print("‚ö†Ô∏è El valor actual supera el percentil 90 del d√≠a")
                
                # Visualizar antes y despu√©s
                print("\nComparaci√≥n de valores:")
                print(f"  ANTES: {valor_actual[0]} (actual)")
                print(f"  DESPU√âS: {valor_final['potencia_activa']:.4f} (propuesto)")
                
                # Preguntar si se quiere aplicar la correcci√≥n
                print("\nAplicando correcci√≥n autom√°ticamente seg√∫n umbral")
                if diferencia_porcentaje > 20:  # Umbral m√°s alto para correcci√≥n autom√°tica
                    # Aplicar correcci√≥n
                    query_update = """
                    UPDATE potencia_dep_processed
                    SET 
                        potencia_activa = %s,
                        potencia_reactiva = %s
                    WHERE
                        alimentador = %s AND
                        fecha = %s AND
                        hora = %s
                    """
                    
                    pg_hook.run(
                        query_update,
                        parameters=(
                            valor_final['potencia_activa'],
                            valor_final['potencia_reactiva'],
                            alimentador,
                            fecha,
                            hora
                        )
                    )
                    
                    print("‚úÖ Correcci√≥n aplicada correctamente")
                else:
                    print("‚ùå Correcci√≥n no aplicada")
            else:
                print("‚úÖ El valor actual est√° dentro de rangos aceptables")
        
        return {
            'alimentador': alimentador,
            'fecha': str(fecha),
            'hora': str(hora),
            'valor_actual': valor_actual,
            'valor_propuesto': valor_final,
            'homologos': homologos,
            'entorno': valores_entorno,
            'aplicar_correccion': diferencia_porcentaje > 15 if valor_actual else False
        }
        
    except Exception as e:
        print(f"Error analizando caso espec√≠fico: {str(e)}")
        traceback.print_exc()
        return None

def ejecutar_tareas_calidad_datos(**kwargs):
    """
    Funci√≥n principal que ejecuta todas las tareas de calidad de datos
    despu√©s del procesamiento principal.
    """
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        
        print("üîç Iniciando tareas de verificaci√≥n y correcci√≥n de calidad de datos")
        
        # 1. Verificar completitud de registros
        print("\n--- VERIFICACI√ìN DE COMPLETITUD DE REGISTROS ---")
        resultados_completitud = verificar_completitud_registros()
        if resultados_completitud:
            print(f"Se identificaron {len(resultados_completitud)} d√≠as con registros incompletos")
        else:
            print("No se encontraron problemas de completitud")
        
        # 2. Imputar valores faltantes
        print("\n--- IMPUTACI√ìN DE VALORES FALTANTES ---")
        fecha_inicio = (datetime.now() - timedelta(days=90)).strftime('%Y-%m-%d')  # √öltimos 90 d√≠as
        fecha_fin = datetime.now().strftime('%Y-%m-%d')
        
        resultados_imputacion = imputar_valores_faltantes(
            pg_hook, 
            alimentador=None,  # Procesar todos los alimentadores
            fecha_inicio=fecha_inicio,
            fecha_fin=fecha_fin
        )
        
        # 3. Analizar y corregir anomal√≠as
        print("\n--- AN√ÅLISIS Y CORRECCI√ìN DE ANOMAL√çAS ---")
        resultados_anomalias = analizar_anomalias_y_corregir()
        
        # 4. Detectar y corregir un caso espec√≠fico para demostraci√≥n
        print("\n--- AN√ÅLISIS DE CASO ESPEC√çFICO ---")
        caso_especifico = resolver_caso_anomalo()
        
        # 5. Verificar d√≠as de semana inconsistentes
        print("\n--- CORRECCI√ìN DE D√çAS DE SEMANA INCONSISTENTES ---")
        corregidos = corregir_dia_semana_inconsistente()
        
        # Generar resumen de resultados
        resumen = {
            'completitud': {
                'dias_incompletos': len(resultados_completitud) if resultados_completitud else 0,
                'detalles': resultados_completitud
            },
            'imputacion': {
                'total_procesados': resultados_imputacion.get('total_procesados', 0),
                'exitosos': resultados_imputacion.get('imputados_exitosos', 0),
                'limitados': resultados_imputacion.get('imputaciones_limitadas', 0),
                'fallidos': resultados_imputacion.get('imputaciones_fallidas', 0)
            },
            'anomalias': {
                'detectadas': resultados_anomalias.get('anomalias_detectadas', 0),
                'corregidas': resultados_anomalias.get('anomalias_corregidas', 0)
            },
            'dias_semana': {
                'corregidos': corregidos
            },
            'caso_ejemplo': {
                'alimentador': caso_especifico.get('alimentador') if caso_especifico else None,
                'fecha': caso_especifico.get('fecha') if caso_especifico else None,
                'hora': caso_especifico.get('hora') if caso_especifico else None
            }
        }
        
        print("\n=== RESUMEN DE TAREAS DE CALIDAD DE DATOS ===")
        print(f"D√≠as con registros incompletos: {resumen['completitud']['dias_incompletos']}")
        print(f"Valores imputados: {resumen['imputacion']['exitosos']} exitosos, {resumen['imputacion']['limitados']} limitados")
        print(f"Anomal√≠as: {resumen['anomalias']['detectadas']} detectadas, {resumen['anomalias']['corregidas']} corregidas")
        print(f"D√≠as de semana corregidos: {resumen['dias_semana']['corregidos']}")
        
        # Registrar en log
        registrar_log(
            pg_hook,
            'INFO',
            'Procesamiento de calidad de datos completado',
            detalles={
                'resumen': resumen
            }
        )
        
        return resumen
    
    except Exception as e:
        print(f"‚ùå Error en tareas de calidad de datos: {str(e)}")
        traceback.print_exc()
        return {'error': str(e)}

def analizar_anomalias_y_corregir():
    """
    Funci√≥n principal para detectar anomal√≠as en toda la serie de datos
    y corregir valores at√≠picos.
    """
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        
        # 1. Detectar valores an√≥malos por alimentador
        print("üîç Buscando valores an√≥malos en todos los alimentadores...")
        
        # Usar estad√≠sticas espec√≠ficas por alimentador para determinar umbrales din√°micos
        query_anomalias = """
        WITH estadisticas_alimentador AS (
            SELECT
                alimentador,
                AVG(potencia_activa) AS media_activa,
                STDDEV(potencia_activa) AS std_activa,
                PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY potencia_activa) AS mediana_activa,
                PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY potencia_activa) AS p95_activa,
                PERCENTILE_CONT(0.05) WITHIN GROUP (ORDER BY potencia_activa) AS p05_activa
            FROM potencia_dep_processed
            GROUP BY alimentador
        ),
        anomalias AS (
            SELECT
                p.alimentador,
                p.fecha,
                p.hora,
                p.potencia_activa,
                p.potencia_reactiva,
                e.media_activa,
                e.std_activa,
                e.mediana_activa,
                CASE 
                    -- Detectar picos hacia arriba
                    WHEN p.potencia_activa > (e.p95_activa + 2 * e.std_activa) THEN 'PICO_ALTO'
                    -- Detectar picos hacia abajo (valores extra√±amente bajos)
                    WHEN p.potencia_activa < (e.p05_activa - e.std_activa) AND p.potencia_activa > 0 THEN 'PICO_BAJO'
                    -- Detectar valores incorrectos (relaci√≥n entre activa y reactiva)
                    WHEN p.potencia_activa > 0 AND p.potencia_reactiva > p.potencia_activa * 2 THEN 'REL_INCORRECTA'
                    -- Normal
                    ELSE 'NORMAL'
                END AS tipo_anomalia,
                ABS(p.potencia_activa - e.mediana_activa) / 
                    CASE WHEN e.std_activa > 0 THEN e.std_activa ELSE 1 END AS desviacion_normalizada
            FROM potencia_dep_processed p
            JOIN estadisticas_alimentador e ON p.alimentador = e.alimentador
        )
        SELECT
            alimentador,
            fecha,
            hora,
            potencia_activa,
            potencia_reactiva,
            media_activa,
            mediana_activa,
            std_activa,
            tipo_anomalia,
            desviacion_normalizada
        FROM anomalias
        WHERE tipo_anomalia != 'NORMAL'
        AND desviacion_normalizada > 2.5
        ORDER BY desviacion_normalizada DESC
        LIMIT 1000
        """
        
        anomalias = pg_hook.get_records(query_anomalias)
        
        print(f"üîç Detectadas {len(anomalias)} potenciales anomal√≠as")
        
        # Procesar cada anomal√≠a detectada
        corregidas = 0
        for anomalia in anomalias:
            try:  # A√±ado try/except para manejar errores por anomal√≠a
                alimentador = anomalia[0]
                fecha = anomalia[1]
                hora = anomalia[2]
                
                print(f"\nProcesando anomal√≠a: {alimentador}, {fecha}, {hora}")
                print(f"- Tipo: {anomalia[8]}")
                print(f"- Valor actual: Activa={anomalia[3]}, Reactiva={anomalia[4]}")
                print(f"- Estad√≠sticas: Media={anomalia[5]:.4f}, Mediana={anomalia[6]:.4f}, StdDev={anomalia[7]:.4f}")
                print(f"- Desviaci√≥n normalizada: {anomalia[9]:.2f}")
                
                # Obtener valores del entorno
                valores_entorno = obtener_valores_entorno(pg_hook, alimentador, fecha, hora)
                
                # Obtener hom√≥logos - CORRECCI√ìN AQU√ç: Cambiamos la consulta problem√°tica
                fecha_dt = fecha  # Ya es un objeto date
                dia_semana = fecha_dt.weekday() if hasattr(fecha_dt, 'weekday') else 0
                
                # CONSULTA CORREGIDA: Eliminamos el uso problem√°tico de EXTRACT(EPOCH FROM...)
                query_homologos = """
                SELECT 
                    fecha, 
                    hora, 
                    potencia_activa, 
                    potencia_reactiva
                FROM potencia_dep_processed
                WHERE 
                    alimentador = %s AND
                    EXTRACT(DOW FROM fecha) = %s AND
                    hora = %s::time AND
                    fecha != %s::date AND
                    fecha >= (%s::date - INTERVAL '28 days') AND
                    fecha <= (%s::date + INTERVAL '28 days') AND
                    potencia_activa > 0
                ORDER BY 
                    fecha  -- Ordenamos simplemente por fecha en lugar de usar la funci√≥n ABS
                LIMIT 5
                """
                
                homologos = pg_hook.get_records(
                    query_homologos, 
                    parameters=(
                        alimentador, 
                        dia_semana, 
                        hora, 
                        fecha,
                        fecha,
                        fecha
                    )
                )
                
                # Calcular valor de imputaci√≥n
                valor_imputado = calcular_imputacion(homologos, valores_entorno)
                
                # Validar y ajustar
                valor_final = validar_y_ajustar_imputacion(valor_imputado, valores_entorno, 0.15)
                
                # Solo corregir si la diferencia es significativa
                valor_actual = float(anomalia[3])
                diferencia_porcentual = abs(valor_actual - valor_final['potencia_activa']) / max(valor_actual, 0.001) * 100
                
                if diferencia_porcentual > 20:  # Solo corregir diferencias mayores al 20%
                    print(f"‚úÖ Corrigiendo anomal√≠a (diferencia: {diferencia_porcentual:.2f}%)")
                    print(f"  - Valor corregido: Activa={valor_final['potencia_activa']:.4f}, Reactiva={valor_final['potencia_reactiva']:.4f}")
                    
                    # Actualizar en la base de datos
                    query_update = """
                    UPDATE potencia_dep_processed
                    SET 
                        potencia_activa = %s,
                        potencia_reactiva = %s
                    WHERE
                        alimentador = %s AND
                        fecha = %s AND
                        hora = %s
                    """
                    
                    pg_hook.run(
                        query_update,
                        parameters=(
                            valor_final['potencia_activa'],
                            valor_final['potencia_reactiva'],
                            alimentador,
                            fecha,
                            hora
                        )
                    )
                    
                    corregidas += 1
                    
                    # Registrar en log
                    registrar_log(
                        pg_hook,
                        'CORRECCI√ìN',
                        f'Anomal√≠a corregida: {alimentador}, {fecha}, {hora}',
                        alimentador=alimentador,
                        fecha=fecha,
                        hora=hora,
                        detalles={
                            'tipo_anomalia': anomalia[8],
                            'valor_original': {
                                'potencia_activa': float(anomalia[3]),
                                'potencia_reactiva': float(anomalia[4])
                            },
                            'valor_corregido': {
                                'potencia_activa': valor_final['potencia_activa'],
                                'potencia_reactiva': valor_final['potencia_reactiva']
                            },
                            'desviacion_original': float(anomalia[9])
                        }
                    )
            except Exception as e_proc:
                print(f"‚ö†Ô∏è Error procesando anomal√≠a {alimentador}, {fecha}, {hora}: {str(e_proc)}")
                continue  # Seguir con la siguiente anomal√≠a
                
        print(f"\nüîÑ Proceso completado. Se corrigieron {corregidas} anomal√≠as de {len(anomalias)} detectadas.")
        
        return {
            'anomalias_detectadas': len(anomalias),
            'anomalias_corregidas': corregidas
        }
        
    except Exception as e:
        print(f"Error en an√°lisis de anomal√≠as: {str(e)}")
        traceback.print_exc()
        return {'error': str(e)}

def insertar_valor_imputado(pg_hook, alimentador, fecha, hora, potencia_activa, potencia_reactiva, dia_semana):
    """
    Inserta un valor imputado en la tabla potencia_dep_processed.
    
    Args:
        pg_hook: PostgreSQL hook para conexi√≥n a base de datos
        alimentador: ID del alimentador
        fecha: Fecha del registro
        hora: Hora del registro
        potencia_activa: Valor imputado de potencia activa
        potencia_reactiva: Valor imputado de potencia reactiva
        dia_semana: D√≠a de la semana (0-6)
    
    Returns:
        bool: True si la inserci√≥n fue exitosa, False en caso contrario
    """
    try:
        # Verificar si ya existe un registro para esa combinaci√≥n
        query_check = """
        SELECT COUNT(*) 
        FROM potencia_dep_processed
        WHERE alimentador = %s AND fecha = %s AND hora = %s
        """
        
        count = pg_hook.get_first(query_check, parameters=(alimentador, fecha, hora))[0]
        
        if count > 0:
            # Actualizar registro existente
            query = """
            UPDATE potencia_dep_processed
            SET 
                potencia_activa = %s,
                potencia_reactiva = %s,
                dia_semana = %s
            WHERE 
                alimentador = %s AND 
                fecha = %s AND 
                hora = %s
            """
            
            pg_hook.run(
                query, 
                parameters=(
                    potencia_activa, 
                    potencia_reactiva, 
                    dia_semana,
                    alimentador, 
                    fecha, 
                    hora
                )
            )
        else:
            # Insertar nuevo registro
            query = """
            INSERT INTO potencia_dep_processed
                (fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva)
            VALUES
                (%s, %s, %s, %s, %s, %s)
            """
            
            pg_hook.run(
                query, 
                parameters=(
                    fecha, 
                    hora, 
                    alimentador, 
                    dia_semana,
                    potencia_activa, 
                    potencia_reactiva
                )
            )
        
        # Registrar en log
        registrar_log(
            pg_hook,
            'INFO',
            f'Valor imputado: alimentador {alimentador}, fecha {fecha}, hora {hora}',
            alimentador=alimentador,
            fecha=fecha,
            hora=hora,
            detalles={
                'potencia_activa': potencia_activa,
                'potencia_reactiva': potencia_reactiva,
                'tipo': 'imputado'
            }
        )
        
        return True
        
    except Exception as e:
        print(f"Error insertando valor imputado: {str(e)}")
        return False

def validar_y_ajustar_imputacion(valor_imputado, valores_entorno, max_variacion=0.15):
    """
    Valida que el valor imputado no genere picos an√≥malos y lo ajusta si es necesario.
    
    Args:
        valor_imputado: Dict con el valor calculado de imputaci√≥n
        valores_entorno: Dict con valores de entorno (antes/despu√©s)
        max_variacion: M√°xima variaci√≥n permitida como fracci√≥n (0.15 = 15%)
    
    Returns:
        dict: Valores validados y potencialmente ajustados
    """
    try:
        # Si no hay valores de entorno para comparar, aceptar el valor imputado
        if not valores_entorno['antes'] and not valores_entorno['despues']:
            return {
                'potencia_activa': valor_imputado['potencia_activa'],
                'potencia_reactiva': valor_imputado['potencia_reactiva'],
                'estado': 'OK',
                'ajuste_aplicado': False
            }
        
        # Calcular valores de referencia del entorno
        potencia_activa_ref = []
        potencia_reactiva_ref = []
        
        # Priorizar los valores m√°s cercanos (1 antes y 1 despu√©s si est√°n disponibles)
        if valores_entorno['antes']:
            potencia_activa_ref.append(valores_entorno['antes'][0]['potencia_activa'])
            potencia_reactiva_ref.append(valores_entorno['antes'][0]['potencia_reactiva'])
            
        if valores_entorno['despues']:
            potencia_activa_ref.append(valores_entorno['despues'][0]['potencia_activa'])
            potencia_reactiva_ref.append(valores_entorno['despues'][0]['potencia_reactiva'])
        
        # Si no hay valores de referencia disponibles
        if not potencia_activa_ref:
            return {
                'potencia_activa': valor_imputado['potencia_activa'],
                'potencia_reactiva': valor_imputado['potencia_reactiva'],
                'estado': 'OK',
                'ajuste_aplicado': False
            }
            
        # Calcular valor de referencia (media de los valores cercanos)
        potencia_activa_media_ref = statistics.mean(potencia_activa_ref)
        potencia_reactiva_media_ref = statistics.mean(potencia_reactiva_ref)
        
        # Calcular l√≠mites superior e inferior para potencia activa
        limite_inferior_activa = potencia_activa_media_ref * (1 - max_variacion)
        limite_superior_activa = potencia_activa_media_ref * (1 + max_variacion)
        
        # Calcular l√≠mites superior e inferior para potencia reactiva
        limite_inferior_reactiva = potencia_reactiva_media_ref * (1 - max_variacion)
        limite_superior_reactiva = potencia_reactiva_media_ref * (1 + max_variacion)
        
        # Comprobar si el valor imputado est√° dentro de los l√≠mites
        potencia_activa_final = valor_imputado['potencia_activa']
        potencia_reactiva_final = valor_imputado['potencia_reactiva']
        ajuste_aplicado = False
        
        # Ajustar potencia activa si est√° fuera de los l√≠mites
        if potencia_activa_final < limite_inferior_activa:
            potencia_activa_final = limite_inferior_activa
            ajuste_aplicado = True
        elif potencia_activa_final > limite_superior_activa:
            potencia_activa_final = limite_superior_activa
            ajuste_aplicado = True
            
        # Ajustar potencia reactiva si est√° fuera de los l√≠mites
        if potencia_reactiva_final < limite_inferior_reactiva:
            potencia_reactiva_final = limite_inferior_reactiva
            ajuste_aplicado = True
        elif potencia_reactiva_final > limite_superior_reactiva:
            potencia_reactiva_final = limite_superior_reactiva
            ajuste_aplicado = True
            
        return {
            'potencia_activa': potencia_activa_final,
            'potencia_reactiva': potencia_reactiva_final,
            'estado': 'LIMITADO' if ajuste_aplicado else 'OK',
            'ajuste_aplicado': ajuste_aplicado,
            'limite_inferior_activa': limite_inferior_activa,
            'limite_superior_activa': limite_superior_activa,
            'limite_inferior_reactiva': limite_inferior_reactiva,
            'limite_superior_reactiva': limite_superior_reactiva,
            'referencia_activa': potencia_activa_media_ref,
            'referencia_reactiva': potencia_reactiva_media_ref
        }
        
    except Exception as e:
        print(f"Error en validaci√≥n de imputaci√≥n: {str(e)}")
        return {
            'potencia_activa': valor_imputado['potencia_activa'],
            'potencia_reactiva': valor_imputado['potencia_reactiva'],
            'estado': 'ERROR',
            'error': str(e)
        }

def calcular_imputacion(homologos, valores_entorno):
    """
    Calcula un valor de imputaci√≥n basado en datos hist√≥ricos hom√≥logos y entorno.
    
    Args:
        homologos: Lista de registros hist√≥ricos hom√≥logos
        valores_entorno: Dict con valores de entorno (antes/despu√©s)
    
    Returns:
        dict: Valores calculados con potencia activa, reactiva y metadatos
    """
    try:
        potencia_activa_homologos = []
        potencia_reactiva_homologos = []
        
        # Extraer valores de hom√≥logos
        for h in homologos:
            potencia_activa_homologos.append(float(h[2]))
            potencia_reactiva_homologos.append(float(h[3]))
        
        # Extraer valores del entorno
        potencia_activa_entorno = []
        potencia_reactiva_entorno = []
        
        for v in valores_entorno['antes']:
            potencia_activa_entorno.append(v['potencia_activa'])
            potencia_reactiva_entorno.append(v['potencia_reactiva'])
            
        for v in valores_entorno['despues']:
            potencia_activa_entorno.append(v['potencia_activa'])
            potencia_reactiva_entorno.append(v['potencia_reactiva'])
        
        # Calcular valores imputados basados en diferentes estrategias
        resultado = {
            'metodo': '',
            'confianza': 0.0,
            'potencia_activa': 0.0,
            'potencia_reactiva': 0.0,
            'fuentes_homologos': len(homologos),
            'fuentes_entorno': len(potencia_activa_entorno)
        }
        
        # Estrategia 1: Si hay suficientes hom√≥logos, usar mediana de hom√≥logos (m√°s robusta que la media)
        if len(potencia_activa_homologos) >= 3:
            resultado['potencia_activa'] = statistics.median(potencia_activa_homologos)
            resultado['potencia_reactiva'] = statistics.median(potencia_reactiva_homologos)
            resultado['metodo'] = 'mediana_homologos'
            resultado['confianza'] = 0.9
            
        # Estrategia 2: Si hay pocos hom√≥logos pero suficientes, usar media ponderada por cercan√≠a temporal
        elif len(potencia_activa_homologos) > 0:
            resultado['potencia_activa'] = statistics.mean(potencia_activa_homologos)
            resultado['potencia_reactiva'] = statistics.mean(potencia_reactiva_homologos)
            resultado['metodo'] = 'media_homologos'
            resultado['confianza'] = 0.7
            
        # Estrategia 3: Si hay suficientes valores de entorno, usar interpolaci√≥n
        elif len(potencia_activa_entorno) >= 3:
            # Usar la media acotada (elimina extremos) para m√°s robustez
            resultado['potencia_activa'] = statistics.mean(sorted(potencia_activa_entorno)[1:-1])
            resultado['potencia_reactiva'] = statistics.mean(sorted(potencia_reactiva_entorno)[1:-1])
            resultado['metodo'] = 'media_acotada_entorno'
            resultado['confianza'] = 0.6
            
        # Estrategia 4: Si hay al menos un valor de entorno, usarlo directamente
        elif len(potencia_activa_entorno) > 0:
            resultado['potencia_activa'] = potencia_activa_entorno[0]
            resultado['potencia_reactiva'] = potencia_reactiva_entorno[0]
            resultado['metodo'] = 'valor_entorno_cercano'
            resultado['confianza'] = 0.4
            
        # Estrategia 5: No hay datos, usar valor por defecto seguro
        else:
            # Valores conservadores que no generen picos
            resultado['potencia_activa'] = 0.5
            resultado['potencia_reactiva'] = 0.2
            resultado['metodo'] = 'valor_por_defecto'
            resultado['confianza'] = 0.1
        
        return resultado
        
    except Exception as e:
        print(f"Error calculando valor de imputaci√≥n: {str(e)}")
        return {
            'metodo': 'error',
            'confianza': 0.0,
            'potencia_activa': 0.5,  # Valor seguro por defecto
            'potencia_reactiva': 0.2,
            'error': str(e)
        }

def obtener_valores_entorno(pg_hook, alimentador, fecha, hora):
    """
    Obtiene valores del entorno (antes y despu√©s) para un timestamp espec√≠fico.
    
    Args:
        pg_hook: PostgreSQL hook para conexi√≥n a base de datos
        alimentador: Alimentador a consultar
        fecha: Fecha del registro
        hora: Hora del registro
    
    Returns:
        dict: Valores de entorno con claves 'antes' y 'despues'
    """
    try:
        # Convertir la hora a objeto time si es string
        if isinstance(hora, str):
            hora_obj = datetime.strptime(hora, '%H:%M:%S').time()
        else:
            hora_obj = hora
            
        # 1. Obtener valores anteriores (hasta 2 horas antes)
        query_antes = """
        SELECT 
            fecha, 
            hora, 
            potencia_activa, 
            potencia_reactiva
        FROM potencia_dep_processed
        WHERE 
            alimentador = %s AND
            fecha = %s::date AND
            hora < %s::time AND
            hora >= (%s::time - INTERVAL '2 hours') AND
            potencia_activa > 0
        ORDER BY hora DESC
        LIMIT 4
        """
        
        valores_antes = pg_hook.get_records(
            query_antes, 
            parameters=(alimentador, fecha, hora, hora)
        )
        
        # 2. Obtener valores posteriores (hasta 2 horas despu√©s)
        query_despues = """
        SELECT 
            fecha, 
            hora, 
            potencia_activa, 
            potencia_reactiva
        FROM potencia_dep_processed
        WHERE 
            alimentador = %s AND
            fecha = %s::date AND
            hora > %s::time AND
            hora <= (%s::time + INTERVAL '2 hours') AND
            potencia_activa > 0
        ORDER BY hora ASC
        LIMIT 4
        """
        
        valores_despues = pg_hook.get_records(
            query_despues, 
            parameters=(alimentador, fecha, hora, hora)
        )
        
        # Normalizar resultados para facilitar su uso
        antes = []
        for v in valores_antes:
            antes.append({
                'fecha': v[0],
                'hora': v[1],
                'potencia_activa': float(v[2]),
                'potencia_reactiva': float(v[3])
            })
            
        despues = []
        for v in valores_despues:
            despues.append({
                'fecha': v[0],
                'hora': v[1],
                'potencia_activa': float(v[2]),
                'potencia_reactiva': float(v[3])
            })
        
        return {
            'antes': antes,
            'despues': despues
        }
        
    except Exception as e:
        print(f"Error obteniendo valores de entorno: {str(e)}")
        return {'antes': [], 'despues': []}

def identificar_timestamps_faltantes(pg_hook, alimentador=None, fecha_inicio=None, fecha_fin=None):
    """
    Identifica timestamps faltantes en una serie temporal que deber√≠a tener datos cada 15 minutos.
    
    Args:
        pg_hook: PostgreSQL hook para conexi√≥n a base de datos
        alimentador: Alimentador espec√≠fico o None para procesar todos
        fecha_inicio: Fecha de inicio para el rango a procesar (default: 7 d√≠as atr√°s)
        fecha_fin: Fecha de fin para el rango a procesar (default: hoy)
    
    Returns:
        list: Lista de registros faltantes con alimentador, fecha y hora
    """
    try:
        # Establecer valores por defecto si no se proporcionan
        if not fecha_inicio:
            fecha_inicio = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        if not fecha_fin:
            fecha_fin = datetime.now().strftime('%Y-%m-%d')
        
        # Si no se especifica alimentador, obtener todos los alimentadores activos
        if not alimentador:
            query_alimentadores = """
            SELECT DISTINCT alimentador 
            FROM potencia_dep_processed
            WHERE fecha BETWEEN %s::date AND %s::date
            ORDER BY alimentador
            """
            alimentadores = [row[0] for row in pg_hook.get_records(
                query_alimentadores, 
                parameters=(fecha_inicio, fecha_fin)
            )]
        else:
            alimentadores = [alimentador]
        
        resultados = []
        
        # Para cada alimentador, identificar timestamps faltantes
        for alim in alimentadores:
            query_faltantes = """
            WITH fechas AS (
                SELECT generate_series(%s::date, %s::date, '1 day'::interval)::date AS fecha
            ),
            horas AS (
                SELECT generate_series(0, 23) AS hora
            ),
            minutos AS (
                SELECT generate_series(0, 45, 15) AS minuto
            ),
            todos_timestamps AS (
                SELECT 
                    f.fecha,
                    make_time(h.hora, m.minuto, 0) AS hora,
                    %s AS alimentador
                FROM fechas f
                CROSS JOIN horas h
                CROSS JOIN minutos m
            ),
            existentes AS (
                SELECT fecha, hora
                FROM potencia_dep_processed
                WHERE 
                    alimentador = %s AND
                    fecha BETWEEN %s::date AND %s::date
            )
            SELECT 
                tt.alimentador,
                tt.fecha,
                tt.hora
            FROM todos_timestamps tt
            LEFT JOIN existentes e ON tt.fecha = e.fecha AND tt.hora = e.hora
            WHERE e.fecha IS NULL
            ORDER BY tt.fecha, tt.hora;
            """
            
            faltantes = pg_hook.get_records(
                query_faltantes, 
                parameters=(
                    fecha_inicio, fecha_fin, 
                    alim, alim, 
                    fecha_inicio, fecha_fin
                )
            )
            
            for registro in faltantes:
                resultados.append({
                    'alimentador': registro[0],
                    'fecha': registro[1],
                    'hora': registro[2]
                })
        
        return resultados
        
    except Exception as e:
        print(f"Error identificando timestamps faltantes: {str(e)}")
        return []

def corregir_registros_faltantes():
    """
    Identifica y corrige registros faltantes en la serie temporal
    """
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        
        # 1. Identificar registros faltantes
        registros_faltantes = verificar_completitud_registros()
        
        if not registros_faltantes:
            print("‚úÖ No se detectaron registros faltantes que requieran correcci√≥n.")
            return {'registros_corregidos': 0}
            
        print(f"üîç Detectados {len(registros_faltantes)} d√≠as con registros incompletos.")
        
        # 2. Para cada registro faltante, generar una imputaci√≥n
        registros_corregidos = 0
        for registro in registros_faltantes:
            alimentador = registro['alimentador']
            fecha = registro['fecha']
            timestamps_faltantes = registro.get('timestamps_faltantes', [])
            
            print(f"üîÑ Corrigiendo {len(timestamps_faltantes)} timestamps faltantes para {alimentador}, {fecha}")
            
            # Para cada timestamp faltante, imputar un valor
            for hora_str in timestamps_faltantes:
                try:
                    # Convertir a formato adecuado
                    if isinstance(fecha, str):
                        fecha_dt = datetime.strptime(fecha, '%Y-%m-%d').date()
                    else:
                        fecha_dt = fecha
                        
                    # Calcular d√≠a de semana correcto
                    dia_semana = calcular_dia_semana(fecha_dt)
                    
                    # Buscar datos hom√≥logos
                    query_homologos = """
                    SELECT potencia_activa, potencia_reactiva
                    FROM potencia_dep_processed
                    WHERE alimentador = %s
                    AND EXTRACT(DOW FROM fecha) = %s
                    AND hora = %s::time
                    AND potencia_activa > 0
                    ORDER BY fecha DESC
                    LIMIT 5
                    """
                    
                    homologos = pg_hook.get_records(query_homologos, 
                                                  parameters=(alimentador, dia_semana, hora_str))
                    
                    # Si hay hom√≥logos, calcular promedio
                    if homologos:
                        potencia_activa = statistics.mean([h[0] for h in homologos])
                        potencia_reactiva = statistics.mean([h[1] for h in homologos])
                    else:
                        # Si no hay hom√≥logos, usar valores conservadores
                        potencia_activa = 1.0
                        potencia_reactiva = 0.5
                    
                    # Insertar valor imputado
                    query_insert = """
                    INSERT INTO potencia_dep_processed 
                        (fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva)
                    VALUES (%s, %s, %s, %s, %s, %s)
                    ON CONFLICT (fecha, hora, alimentador) DO UPDATE 
                    SET potencia_activa = EXCLUDED.potencia_activa,
                        potencia_reactiva = EXCLUDED.potencia_reactiva,
                        dia_semana = EXCLUDED.dia_semana
                    """
                    
                    pg_hook.run(query_insert, 
                              parameters=(fecha_dt, hora_str, alimentador, dia_semana, 
                                         potencia_activa, potencia_reactiva))
                    
                    # Registrar en log
                    registrar_log(
                        pg_hook,
                        'IMPUTACI√ìN',
                        f'Registro faltante corregido: {alimentador}, {fecha}, {hora_str}',
                        alimentador=alimentador,
                        fecha=fecha_dt,
                        hora=hora_str,
                        detalles={
                            'potencia_activa': float(potencia_activa),
                            'potencia_reactiva': float(potencia_reactiva),
                            'fuente': 'correcci√≥n_completitud'
                        }
                    )
                    
                    registros_corregidos += 1
                    
                except Exception as e_corr:
                    print(f"‚ö†Ô∏è Error corrigiendo registro {alimentador}, {fecha}, {hora_str}: {str(e_corr)}")
        
        print(f"‚úÖ Proceso completo: {registros_corregidos} registros corregidos")
        
        return {
            'registros_corregidos': registros_corregidos
        }
        
    except Exception as e:
        print(f"‚ùå Error corrigiendo registros faltantes: {str(e)}")
        traceback.print_exc()
        return {'error': str(e)}

def recuperar_datos_mes_faltantes(mes=None, **kwargs):
    """
    Recupera los datos faltantes de un mes espec√≠fico (o detecta autom√°ticamente qu√© mes necesita correcci√≥n)
    usando el flujo NiFi/Kafka. Esta funci√≥n identifica registros en potencia_dep que no existen en
    potencia_dep_processed, los depura y los env√≠a a trav√©s de NiFi.
    
    Args:
        mes: Mes espec√≠fico a recuperar (1-12) o None para detecci√≥n autom√°tica
        
    Returns:
        dict: Resumen del proceso de recuperaci√≥n
    """
    try:
        print("üîç Iniciando an√°lisis de datos faltantes entre tablas")
        pg_hook_origen = PostgresHook(postgres_conn_id="postgres_centrosur")
        pg_hook_destino = PostgresHook(postgres_conn_id="postgres_base_prueba")
        
        # Crear extensi√≥n dblink si no existe
        try:
            pg_hook_origen.run("CREATE EXTENSION IF NOT EXISTS dblink")
        except Exception as e:
            print(f"‚ö†Ô∏è Error creando extensi√≥n dblink: {str(e)}")
        
        # 1. Obtener credenciales para dblink desde la conexi√≥n Airflow
        conn_destino = BaseHook.get_connection("postgres_base_prueba")
        dblink_dest_conn = f"host={conn_destino.host} dbname={conn_destino.schema} user={conn_destino.login} password={conn_destino.password} port={conn_destino.port or 5432}"
        
        # 1. Si no se especific√≥ un mes, detectar autom√°ticamente qu√© mes tiene m√°s discrepancias
        if mes is None:
            print("üîé Detectando autom√°ticamente el mes con m√°s registros faltantes...")
            
            discrepancias_por_mes = []
            for m in range(1, 13):
                # Calcular fechas para el rango del mes
                mes_siguiente = m + 1 if m < 12 else 1
                ano_siguiente = 2019 if m < 12 else 2020
                fecha_inicio = f"2019-{m:02d}-01"
                fecha_fin = f"{ano_siguiente}-{mes_siguiente:02d}-01"
                
                # Contar registros en origen
                total_origen = pg_hook_origen.get_first(f"""
                    SELECT COUNT(*) FROM potencia_dep 
                    WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
                """)[0]
                
                # Contar registros en destino
                total_destino = pg_hook_destino.get_first(f"""
                    SELECT COUNT(*) FROM potencia_dep_processed 
                    WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
                """)[0]
                
                # Calcular diferencia
                diferencia = total_origen - total_destino
                discrepancias_por_mes.append((m, diferencia, total_origen, total_destino))
                
            # Ordenar por mayor diferencia
            discrepancias_por_mes.sort(key=lambda x: x[1], reverse=True)
            
            # Elegir el mes con mayor discrepancia
            if discrepancias_por_mes and discrepancias_por_mes[0][1] > 0:
                mes = discrepancias_por_mes[0][0]
                print(f"üìä Mayor discrepancia detectada en mes {mes}: {discrepancias_por_mes[0][1]} registros faltantes")
            else:
                print("‚úÖ No se detectaron discrepancias significativas entre tablas")
                return {"recuperados": 0, "mes": None, "mensaje": "No se detectaron discrepancias"}
        
        # 2. Calcular rangos de fecha para el mes seleccionado
        mes_siguiente = mes + 1 if mes < 12 else 1
        ano_siguiente = 2019 if mes < 12 else 2020
        fecha_inicio = f"2019-{mes:02d}-01"
        fecha_fin = f"{ano_siguiente}-{mes_siguiente:02d}-01"
        
        # 3. Contar registros en origen y destino para este mes
        total_origen = pg_hook_origen.get_first(f"""
            SELECT COUNT(*) FROM potencia_dep 
            WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
        """)[0]
        
        total_destino = pg_hook_destino.get_first(f"""
            SELECT COUNT(*) FROM potencia_dep_processed 
            WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
        """)[0]
        
        faltantes = total_origen - total_destino
        print(f"üìä Total registros en origen: {total_origen}")
        print(f"üìä Total registros en destino: {total_destino}")
        print(f"üìä Registros faltantes: {faltantes}")
        
        if faltantes <= 0:
            print(f"‚úÖ No hay registros faltantes para el mes {mes}")
            return {"recuperados": 0, "mes": mes, "total": 0}
        
        # 4. Configurar NiFi para el procesamiento
        print("üîÑ Configurando procesadores NiFi para recuperaci√≥n...")
        
        # Detener todos los procesadores
        stop_all_processors_individually()
        time.sleep(5)
        
        # Configurar topic de Kafka para recuperaci√≥n
        topic_recuperacion = f"potencia-recuperacion-mes{mes}-{int(time.time())}"
        print(f"üì• Configurando topic: {topic_recuperacion}")
        configurar_topic_procesador(CONSUME_KAFKA_PROCESSOR_ID, "Topic Name", topic_recuperacion)
        
        # 5. Crear productor Kafka
        productor = KafkaProducer(
            bootstrap_servers=['kafka_centrosur:9092'],
            value_serializer=lambda v: json.dumps(v, default=decimal_json_serializer).encode('utf-8')
        )
        
        # 6. Obtener los alimentadores con datos faltantes
        alimentadores = pg_hook_origen.get_records(f"""
            SELECT DISTINCT alimentador 
            FROM potencia_dep
            WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
            ORDER BY alimentador
        """)
        
        total_enviados = 0
        lote_size = 5000  # Procesar en lotes para evitar sobrecarga
        
        # 7. Procesar cada alimentador
        for alim_row in alimentadores:
            alimentador = alim_row[0]
            print(f"üîÑ Procesando alimentador: {alimentador}")
            
            # 7.1 Obtener registros faltantes para este alimentador
            query_faltantes = f"""
            WITH datos_destino AS (
                SELECT alimentador, fecha, hora
                FROM dblink(
                    '{dblink_dest_conn}',
                    'SELECT alimentador, fecha, hora FROM potencia_dep_processed 
                    WHERE fecha >= ''{fecha_inicio}'' AND fecha < ''{fecha_fin}'''
                ) AS d(alimentador text, fecha date, hora time)
            )
            SELECT p.alimentador, p.fecha, p.hora, 
                EXTRACT(DOW FROM p.fecha) as dia_semana, 
                p.potencia_activa, p.potencia_reactiva
            FROM potencia_dep p
            LEFT JOIN datos_destino d ON 
                p.alimentador = d.alimentador AND 
                p.fecha = d.fecha AND 
                p.hora = d.hora
            WHERE p.fecha >= '{fecha_inicio}' AND p.fecha < '{fecha_fin}'
            AND p.alimentador = %s
            AND d.alimentador IS NULL
            ORDER BY p.fecha, p.hora
            """
            
            registros_faltantes = pg_hook_origen.get_records(query_faltantes, parameters=(alimentador,))
            
            if not registros_faltantes:
                print(f"‚úÖ No hay registros faltantes para el alimentador {alimentador}")
                continue
                
            print(f"üîç Encontrados {len(registros_faltantes)} registros faltantes para {alimentador}")
            
            # 7.2 Enviar registros a Kafka en lotes
            lote_actual = 0
            for registro in registros_faltantes:
                alimentador, fecha, hora, dia_semana, p_activa, p_reactiva = registro
                
                # Aplicar depuraci√≥n b√°sica
                p_activa = float(p_activa) if p_activa is not None and p_activa > 0 else 1.0
                p_reactiva = float(p_reactiva) if p_reactiva is not None and p_reactiva > 0 else 0.3
                
                # Verificar valores an√≥malos
                if p_activa > 1000 or p_reactiva > 500 or (p_reactiva > p_activa * 2):
                    p_activa = min(p_activa, 500)
                    p_reactiva = min(p_reactiva, 200)
                
                # Calcular d√≠a de semana correctamente
                dia_semana_correcto = calcular_dia_semana(fecha)
                
                # Formatear mensaje para Kafka
                fecha_str = fecha.strftime('%Y-%m-%d') if hasattr(fecha, 'strftime') else str(fecha)
                hora_str = hora.strftime('%H:%M:%S') if hasattr(hora, 'strftime') else str(hora)
                
                mensaje = {
                    'alimentador': alimentador,
                    'fecha': fecha_str,
                    'hora': hora_str,
                    'dia_semana': dia_semana_correcto,  # Usar d√≠a semana calculado correcto
                    'potencia_activa': p_activa,
                    'potencia_reactiva': p_reactiva,
                    'depurado': True,
                    'origen': f'recuperacion_mes{mes}'
                }
                
                # Enviar a Kafka
                productor.send(topic_recuperacion, mensaje)
                total_enviados += 1
                lote_actual += 1
                
                # Flush peri√≥dico para no saturar la memoria
                if lote_actual % 1000 == 0:
                    productor.flush()
                    print(f"  ‚è≥ Enviados {lote_actual} registros para {alimentador}...")
            
            # Flush al finalizar cada alimentador
            productor.flush()
            print(f"‚úÖ Enviados {lote_actual} registros para alimentador {alimentador}")
            
            # L√≠mite opcional por lotes
            if total_enviados >= lote_size:
                print(f"üîÑ Procesando lote de {lote_size} registros...")
                # Iniciar procesadores NiFi
                iniciar_procesadores_consumidor()
                
                # Esperar procesamiento
                tiempo_espera = 300  # 5 minutos
                print(f"‚è±Ô∏è Esperando {tiempo_espera} segundos para procesamiento...")
                time.sleep(tiempo_espera)
                
                # Detener para siguiente lote
                stop_all_processors_individually()
                time.sleep(5)
                total_enviados = 0
        
        # 8. Iniciar procesadores NiFi para procesar datos restantes
        if total_enviados > 0:
            print("üöÄ Iniciando procesadores NiFi para procesar los datos...")
            iniciar_procesadores_consumidor()
            
            # 9. Esperar a que se procesen los datos
            tiempo_espera = 900  # 15 minutos
            print(f"‚è±Ô∏è Esperando {tiempo_espera/60} minutos para completar el procesamiento...")
            
            # Monitoreo peri√≥dico
            for tiempo_transcurrido in range(0, tiempo_espera, 60):
                tiempo_restante = (tiempo_espera - tiempo_transcurrido) // 60
                
                # Verificar progreso
                count_actual = pg_hook_destino.get_first(f"""
                    SELECT COUNT(*) FROM potencia_dep_processed 
                    WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
                """)[0]
                
                progreso = count_actual - total_destino
                print(f"‚è±Ô∏è Tiempo restante: {tiempo_restante} minutos... Progreso: {progreso} nuevos registros")
                
                if progreso >= faltantes * 0.95:
                    print("‚úÖ Procesamiento casi completo, finalizando espera")
                    break
                
                time.sleep(60)
        
        # 10. Verificar resultados finales
        count_final = pg_hook_destino.get_first(f"""
            SELECT COUNT(*) FROM potencia_dep_processed 
            WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
        """)[0]
        
        registros_recuperados = count_final - total_destino
        porcentaje_recuperado = (registros_recuperados / faltantes) * 100 if faltantes > 0 else 0
        
        mensaje_final = f"""
        ‚úÖ Recuperaci√≥n completada para mes {mes}:
        - Registros originales: {total_origen}
        - Registros antes de recuperaci√≥n: {total_destino}
        - Registros despu√©s de recuperaci√≥n: {count_final}
        - Recuperados: {registros_recuperados} ({porcentaje_recuperado:.2f}%)
        """
        print(mensaje_final)
        
        return {
            "mes": mes,
            "recuperados": registros_recuperados,
            "total": total_origen,
            "porcentaje": porcentaje_recuperado
        }
        
    except Exception as e:
        error_msg = f"‚ùå Error en recuperaci√≥n de datos faltantes: {str(e)}"
        print(error_msg)
        traceback.print_exc()
        return {"error": error_msg, "mes": mes if 'mes' in locals() else None}

def imputar_faltantes():
    """
    Versi√≥n simplificada y optimizada de imputar_faltantes que usa procesamiento por lotes
    y consultas m√°s eficientes.
    """
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        year = YEAR  # Usar 2019
        
        print(f"üìä Ejecutando imputaci√≥n simplificada para el a√±o {year}...")
        
        # 1. Identificar huecos en la serie temporal usando CTE - CORRECCI√ìN AQU√ç
        query_huecos = f"""
        WITH fechas_completas AS (
            SELECT 
                a.alimentador,
                d.fecha::date, 
                make_time(h.h, 0, 0) as hora
            FROM 
                (SELECT DISTINCT alimentador FROM potencia_dep_processed) a,
                (SELECT generate_series('{year}-01-01'::date, '{year}-12-31'::date, '1 day'::interval)::date AS fecha) d,
                (SELECT generate_series(0, 23, 1) AS h) h
        ),
        datos_existentes AS (
            SELECT DISTINCT alimentador, fecha::date, hora::time
            FROM potencia_dep_processed
            WHERE EXTRACT(YEAR FROM fecha) = {year}
        )
        SELECT 
            fc.alimentador, 
            fc.fecha, 
            fc.hora AS hora
        FROM fechas_completas fc
        LEFT JOIN datos_existentes de ON 
            fc.alimentador = de.alimentador AND 
            fc.fecha = de.fecha AND 
            fc.hora = de.hora
        WHERE de.alimentador IS NULL
        LIMIT 1000
        """
        huecos = pg_hook.get_records(query_huecos)
        total_huecos = len(huecos)
        
        if total_huecos == 0:
            print("‚úÖ No se encontraron huecos en la serie temporal")
            return {'total_procesados': 0, 'imputados_exitosos': 0}
            
        print(f"üîç Encontrados {total_huecos} huecos para imputar (mostrando los primeros 1000)")
        
        # 2. Para cada hueco, imputar valor basado en promedios mensuales
        registros_procesados = 0
        
        for hueco in huecos:
            alimentador, fecha, hora = hueco
            dia_semana = calcular_dia_semana(fecha)
            
            # Buscar valores de referencia del mismo d√≠a/hora en el mismo mes
            query_referencia = """
            SELECT AVG(potencia_activa) AS p_activa, AVG(potencia_reactiva) AS p_reactiva
            FROM potencia_dep_processed
            WHERE alimentador = %s
            AND EXTRACT(MONTH FROM fecha) = EXTRACT(MONTH FROM %s::date)
            AND EXTRACT(HOUR FROM hora) = EXTRACT(HOUR FROM %s::time)
            AND potencia_activa > 0
            """
            
            referencia = pg_hook.get_first(query_referencia, parameters=(alimentador, fecha, hora))
            
            if referencia and referencia[0]:
                p_activa = referencia[0]
                p_reactiva = referencia[1] or p_activa * 0.3  # Valor conservador
            else:
                # Si no hay valores de referencia, usar valores t√≠picos por hora
                hora_num = int(str(hora).split(':')[0])
                if hora_num >= 6 and hora_num <= 18:  # Horas diurnas
                    p_activa = 1.5
                    p_reactiva = 0.45
                else:  # Horas nocturnas
                    p_activa = 0.8
                    p_reactiva = 0.24
            
            # Insertar el valor imputado
            query_insertar = """
            INSERT INTO potencia_dep_processed 
            (fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (fecha, hora, alimentador) DO UPDATE
            SET potencia_activa = EXCLUDED.potencia_activa,
                potencia_reactiva = EXCLUDED.potencia_reactiva,
                dia_semana = EXCLUDED.dia_semana
            """
            
            pg_hook.run(query_insertar, parameters=(
                fecha, hora, alimentador, dia_semana, p_activa, p_reactiva
            ))
            
            registros_procesados += 1
            
            if registros_procesados % 100 == 0:
                print(f"‚è≥ Procesados {registros_procesados}/{total_huecos} registros")
        
        print(f"‚úÖ Imputaci√≥n simplificada completada: {registros_procesados} registros procesados")
        
        # Registrar en el log
        pg_hook.run("""
        INSERT INTO log_procesamiento 
        (timestamp, mensaje, detalles)
        VALUES (NOW(), %s, %s)
        """, parameters=(
            f'Imputaci√≥n simplificada completada: {registros_procesados} registros',
            json.dumps({'total_procesados': registros_procesados})
        ))
        
        return {'total_procesados': registros_procesados, 'imputados_exitosos': registros_procesados}
        
    except Exception as e:
        print(f"‚ùå Error en imputaci√≥n simplificada: {str(e)}")
        traceback.print_exc()
        return {'error': str(e)}

def imputar_valores_faltantes(alimentador=None, fecha_inicio=None, fecha_fin=None, pg_hook=None):
    """
    Identifica timestamps faltantes en la serie de 15 minutos y los imputa con valores
    calculados de manera inteligente basados en datos hist√≥ricos hom√≥logos.
    
    Args:
        pg_hook: PostgreSQL hook para conexi√≥n a base de datos
        alimentador: Alimentador espec√≠fico o None para procesar todos
        fecha_inicio: Fecha de inicio para el rango a procesar
        fecha_fin: Fecha de fin para el rango a procesar
    
    Returns:
        dict: Resumen de valores imputados
    """
    try:
        if pg_hook is None:
            pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        # Configuraci√≥n de par√°metros
        VENTANA_HISTORICA_SEMANAS = 4  # Cu√°ntas semanas atr√°s buscar datos hom√≥logos
        MAX_VARIACION_PERMITIDA = 0.15  # M√°xima variaci√≥n permitida (15%)
        
        # 1. Identificar timestamps faltantes para el rango/alimentador especificado
        registros_faltantes = identificar_timestamps_faltantes(pg_hook, alimentador, fecha_inicio, fecha_fin)
        
        resultados = {
            'total_procesados': 0,
            'imputados_exitosos': 0,
            'imputaciones_limitadas': 0,
            'imputaciones_fallidas': 0,
            'detalles': []
        }
        
        # 2. Para cada registro faltante, calcular un valor apropiado
        for registro in registros_faltantes:
            alimentador_actual = registro['alimentador']
            fecha_actual = registro['fecha']
            hora_actual = registro['hora']
            
            try:
                # Obtener el d√≠a de la semana (0=lunes, 6=domingo)
                fecha_dt = datetime.strptime(str(fecha_actual), '%Y-%m-%d')
                dia_semana = fecha_dt.weekday()
                
                # Extraer componentes de la hora para comparaci√≥n
                hora_componentes = str(hora_actual).split(':')
                hora_num = int(hora_componentes[0])
                minuto_num = int(hora_componentes[1])
                
                # 2.1 Estrategia 1: Buscar valores hom√≥logos (mismo d√≠a semana, misma hora)
                query_homologos = """
                SELECT 
                    fecha, 
                    hora, 
                    potencia_activa, 
                    potencia_reactiva
                FROM potencia_dep_processed
                WHERE 
                    alimentador = %s AND
                    EXTRACT(DOW FROM fecha) = %s AND
                    hora = %s::time AND
                    fecha != %s::date AND
                    fecha >= %s::date - INTERVAL '%s weeks' AND
                    fecha <= %s::date AND
                    potencia_activa > 0 AND
                    potencia_reactiva != 0
                ORDER BY 
                    ABS(EXTRACT(EPOCH FROM (%s::date - fecha)) / 86400)  -- Ordenar por cercan√≠a en d√≠as
                LIMIT 5
                """
                
                homologos = pg_hook.get_records(
                    query_homologos, 
                    parameters=(
                        alimentador_actual, 
                        dia_semana, 
                        hora_actual, 
                        fecha_actual,
                        fecha_actual,
                        VENTANA_HISTORICA_SEMANAS,
                        fecha_actual,
                        fecha_actual
                    )
                )
                
                # 2.2 Estrategia 2: Si no hay hom√≥logos, buscar valores de la misma hora en d√≠as cercanos
                if not homologos:
                    query_misma_hora = """
                    SELECT 
                        fecha, 
                        hora, 
                        potencia_activa, 
                        potencia_reactiva
                    FROM potencia_dep_processed
                    WHERE 
                        alimentador = %s AND
                        hora = %s::time AND
                        fecha != %s::date AND
                        fecha >= %s::date - INTERVAL '7 days' AND
                        fecha <= %s::date + INTERVAL '7 days' AND
                        potencia_activa > 0 AND
                        potencia_reactiva != 0
                    ORDER BY 
                        ABS(EXTRACT(EPOCH FROM (%s::date - fecha)) / 86400)
                    LIMIT 5
                    """
                    
                    homologos = pg_hook.get_records(
                        query_misma_hora, 
                        parameters=(
                            alimentador_actual, 
                            hora_actual, 
                            fecha_actual,
                            fecha_actual,
                            fecha_actual,
                            fecha_actual
                        )
                    )
                
                # 2.3 Estrategia 3: Buscar valores del entorno (mismo d√≠a, horas cercanas)
                valores_entorno = obtener_valores_entorno(
                    pg_hook, 
                    alimentador_actual, 
                    fecha_actual, 
                    hora_actual
                )
                
                # 3. Calcular valor de imputaci√≥n combinando estrategias
                valor_imputado = calcular_imputacion(homologos, valores_entorno)
                
                # 4. Validar que el valor no genere picos an√≥malos
                valor_final = validar_y_ajustar_imputacion(
                    valor_imputado, 
                    valores_entorno, 
                    MAX_VARIACION_PERMITIDA
                )
                
                # 5. Insertar el valor imputado en la tabla
                if valor_final['estado'] == 'OK':
                    insertar_valor_imputado(
                        pg_hook, 
                        alimentador_actual, 
                        fecha_actual, 
                        hora_actual, 
                        valor_final['potencia_activa'], 
                        valor_final['potencia_reactiva'],
                        dia_semana
                    )
                    resultados['imputados_exitosos'] += 1
                elif valor_final['estado'] == 'LIMITADO':
                    insertar_valor_imputado(
                        pg_hook, 
                        alimentador_actual, 
                        fecha_actual, 
                        hora_actual, 
                        valor_final['potencia_activa'], 
                        valor_final['potencia_reactiva'],
                        dia_semana
                    )
                    resultados['imputaciones_limitadas'] += 1
                else:
                    resultados['imputaciones_fallidas'] += 1
                
                # 6. Registrar detalles de la imputaci√≥n
                resultados['detalles'].append({
                    'alimentador': alimentador_actual,
                    'fecha': str(fecha_actual),
                    'hora': str(hora_actual),
                    'valor_calculado': valor_imputado,
                    'valor_final': valor_final,
                    'fuentes_utilizadas': len(homologos),
                    'valores_entorno': len(valores_entorno)
                })
                
                resultados['total_procesados'] += 1
                
            except Exception as e:
                print(f"Error procesando registro {alimentador_actual}, {fecha_actual}, {hora_actual}: {str(e)}")
                resultados['imputaciones_fallidas'] += 1
                resultados['detalles'].append({
                    'alimentador': alimentador_actual,
                    'fecha': str(fecha_actual),
                    'hora': str(hora_actual),
                    'error': str(e)
                })
        
        # 7. Registrar resumen en log
        registrar_log(
            pg_hook,
            'INFO',
            f'Imputaci√≥n completada: {resultados["imputados_exitosos"]} exitosos, {resultados["imputaciones_limitadas"]} limitados, {resultados["imputaciones_fallidas"]} fallidos',
            detalles=resultados
        )
        
        return resultados
        
    except Exception as e:
        print(f"Error en imputaci√≥n de valores: {str(e)}")
        traceback.print_exc()
        return {'error': str(e)}

def verificar_calidad_mes(pg_hook, mes_actual, year=YEAR):
    """
    Verifica la calidad general de los datos de un mes completo.
    Busca datos tanto en potencia_dep_processed como en datos_enviados.
    
    Args:
        pg_hook: PostgreSQL hook para conexi√≥n a base de datos
        mes_actual: N√∫mero del mes a verificar
        year: A√±o a procesar
    
    Returns:
        float: Porcentaje de calidad general (0-100%)
    """
    try:
        # Primero intentar en potencia_dep_processed
        query_datos_proc = f"""
        SELECT COUNT(*) 
        FROM potencia_dep_processed
        WHERE EXTRACT(MONTH FROM fecha) = {mes_actual}
        AND EXTRACT(YEAR FROM fecha) = {year}
        """
        
        count_proc = pg_hook.get_first(query_datos_proc)[0]
        
        if count_proc > 0:
            print(f"üìä Usando tabla potencia_dep_processed para calidad del mes {mes_actual}")
            # Ahora calculamos la calidad en esta tabla
            query = f"""
            SELECT 
                COALESCE(
                    AVG(
                        ROUND(
                            100 - (SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) * 100.0) / 
                            NULLIF(COUNT(*), 0),
                            2
                        )
                    ),
                    0
                ) AS calidad_promedio
            FROM (
                SELECT 
                    alimentador,
                    COUNT(*) AS total,
                    SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) AS ceros
                FROM potencia_dep_processed
                WHERE EXTRACT(MONTH FROM fecha) = {mes_actual}
                AND EXTRACT(YEAR FROM fecha) = {year}
                GROUP BY alimentador
            ) AS calidad_por_alimentador
            """
            resultado = pg_hook.get_first(query)
            if not resultado or resultado[0] is None:
                calidad = 0.0
            else:
                calidad = float(resultado[0])
        else:
            # Si no hay datos en potencia_dep_processed, buscar en datos_enviados
            print(f"‚ö†Ô∏è No hay datos en potencia_dep_processed para mes {mes_actual}, verificando datos_enviados")
            
            query_datos_env = f"""
            SELECT COUNT(*) 
            FROM datos_enviados
            WHERE EXTRACT(MONTH FROM fecha) = {mes_actual}
            AND EXTRACT(YEAR FROM fecha) = {year}
            """
            
            count_env = pg_hook.get_first(query_datos_env)[0]
            
            if count_env > 0:
                print(f"üìä Usando tabla datos_enviados para calidad del mes {mes_actual}")
                # Calcular calidad en datos_enviados
                query = f"""
                SELECT 
                    COALESCE(
                        AVG(
                            ROUND(
                                100 - (SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) * 100.0) / 
                                NULLIF(COUNT(*), 0),
                                2
                            )
                        ),
                        0
                    ) AS calidad_promedio
                FROM (
                    SELECT 
                        alimentador,
                        COUNT(*) AS total,
                        SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) AS ceros
                    FROM datos_enviados
                    WHERE EXTRACT(MONTH FROM fecha) = {mes_actual}
                    AND EXTRACT(YEAR FROM fecha) = {year}
                    GROUP BY alimentador
                ) AS calidad_por_alimentador
                """
                
                resultado = pg_hook.get_first(query)
                if not resultado or resultado[0] is None:
                    calidad = 0.0
                else:
                    calidad = float(resultado[0])
            else:
                print(f"‚ö†Ô∏è No hay datos para el mes {mes_actual} en ninguna tabla")
                calidad = 0.0
        
        print(f"üìä Calidad del mes {mes_actual}: {calidad:.2f}%")
        return calidad
    except Exception as e:
        print(f"‚ùå Error verificando calidad del mes {mes_actual}: {str(e)}")
        traceback.print_exc()
        
        # Si hay error en la consulta espec√≠fica, puede ser porque la tabla no existe
        # o porque la estructura de columnas no es la esperada
        try:
            print("‚ö†Ô∏è Realizando verificaci√≥n alternativa para determinar calidad b√°sica")
            # Intentar una consulta m√°s b√°sica para al menos determinar si hay datos
            query_simple = f"""
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN potencia_activa > 0 OR potencia_reactiva > 0 THEN 1 ELSE 0 END) as con_valor
            FROM datos_enviados
            WHERE EXTRACT(MONTH FROM fecha) = {mes_actual}
            AND EXTRACT(YEAR FROM fecha) = {year}
            """
            resultado_simple = pg_hook.get_first(query_simple)
            if resultado_simple and resultado_simple[0] > 0:
                total = resultado_simple[0]
                con_valor = resultado_simple[1]
                calidad_simple = (con_valor / total) * 100 if total > 0 else 0
                print(f"üìä Estimaci√≥n simple de calidad: {calidad_simple:.2f}%")
                return calidad_simple
            else:
                return 0.0
                
        except Exception as e2:
            print(f"‚ùå Error en verificaci√≥n alternativa: {str(e2)}")
            return 0.0

def actualizar_registro_standby(pg_hook, alimentador, fecha, hora, potencia_activa, potencia_reactiva, motivo_solucion):
    """
    Actualiza un registro en standby con nuevos valores y lo marca como resuelto.
    Ahora verifica si el registro existe y lo inserta si es necesario.
    Args:
        pg_hook: PostgreSQL hook para conexi√≥n a base de datos
        alimentador: ID del alimentador
        fecha: Fecha del registro en formato 'YYYY-MM-DD'
        hora: Hora del registro
        potencia_activa: Nuevo valor de potencia activa
        potencia_reactiva: Nuevo valor de potencia reactiva
        motivo_solucion: Motivo de la soluci√≥n
    """
    try:
        # 1. Verificar si el registro existe en potencia_dep_processed
        verificar_query = """
        SELECT COUNT(*), MAX(dia_semana) 
        FROM potencia_dep_processed
        WHERE alimentador = %s
        AND fecha = %s
        AND hora = %s
        """
        
        resultado = pg_hook.get_first(verificar_query, parameters=(alimentador, fecha, hora))
        existe = resultado[0] > 0 if resultado else False
        dia_semana_existente = resultado[1] if resultado and len(resultado) > 1 else None
        dia_semana_correcto = calcular_dia_semana(fecha)
        # 2. Si no existe, hacer un INSERT, de lo contrario un UPDATE
        if not existe:
            # Determinar el valor de dia_semana para el registro
            fecha_dt = datetime.strptime(str(fecha), '%Y-%m-%d') if isinstance(fecha, str) else fecha
            dia_semana = fecha_dt.weekday()  # 0=Lunes, 1=Martes, ..., 6=Domingo
            
            # Insertar el registro que falta
            insert_query = """
            INSERT INTO potencia_dep_processed
                (fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva)
            VALUES (%s, %s, %s, %s, %s, %s)
            """
            
            pg_hook.run(insert_query, parameters=(
                fecha, hora, alimentador, 
                dia_semana_correcto,  # Usar el d√≠a semana calculado correctamente
                potencia_activa, potencia_reactiva
            ))
            print(f"‚úÖ Registro NUEVO creado para standby: {alimentador}, {fecha}, {hora}")
        else:
            # Actualizar el registro existente manteniendo el dia_semana original
            query_update = """
            UPDATE potencia_dep_processed
            SET potencia_activa = %s, 
                potencia_reactiva = %s,
                dia_semana = %s
            WHERE alimentador = %s
            AND fecha = %s
            AND hora = %s
            """
            pg_hook.run(query_update, parameters=(
                potencia_activa, potencia_reactiva, 
                dia_semana_correcto,  # Actualizar tambi√©n el d√≠a de la semana
                alimentador, fecha, hora
            ))
            print(f"‚úÖ Registro existente actualizado: {alimentador}, {fecha}, {hora}")
        # 3. Marcar como resuelto en la tabla de standby
        query_resolved = """
        UPDATE registros_standby
        SET activo = FALSE,
            fecha_solucion = NOW(),
            motivo_solucion = %s
        WHERE alimentador = %s
        AND fecha = %s
        AND hora = %s
        AND activo = TRUE
        """
        pg_hook.run(query_resolved, parameters=(
            motivo_solucion, alimentador, fecha, hora
        ))
        
        # 4. Registrar en log
        registrar_log(
            pg_hook, 'INFO',
            f'Registro standby corregido: {alimentador}, {fecha}, {hora}',
            alimentador=alimentador,
            fecha=fecha,
            detalles={
                'potencia_activa_nueva': float(potencia_activa),
                'potencia_reactiva_nueva': float(potencia_reactiva),
                'motivo_solucion': motivo_solucion
            }
        )
        print(f"‚úÖ Registro standby corregido: {alimentador}, {fecha}, {hora}")
        return True
    except Exception as e:
        print(f"‚ùå Error actualizando registro standby: {str(e)}")
        traceback.print_exc()
        return False
    
def buscar_dato_referencia_mes(pg_hook, alimentador, fecha, hora, mes_referencia, year=YEAR):
    """
    Busca datos de referencia en un mes espec√≠fico.
    Mejorada para manejar d√≠as especiales y buscar datos equivalentes entre meses.
    """
    try:
        # Convertir fecha a objeto datetime si es string
        if isinstance(fecha, str):
            fecha_dt = datetime.strptime(fecha, '%Y-%m-%d')
        # Si ya es un objeto date pero no datetime
        elif isinstance(fecha, date) and not isinstance(fecha, datetime):
            fecha_dt = datetime.combine(fecha, datetime.min.time())
        else:
            fecha_dt = fecha
        
        # Extraer componentes de la fecha
        dia_original = fecha_dt.day
        mes_original = fecha_dt.month
        dia_semana_original = fecha_dt.weekday()  # 0=Lunes, 1=Martes, ..., 6=Domingo
        
        print(f"üîç Buscando datos de referencia para {alimentador}, {fecha}, hora {hora} en mes {mes_referencia}")
        
        # 1. ESTRATEGIA 1: Intentar d√≠a exacto si existe en el mes de referencia
        ultimo_dia_mes_ref = calendar.monthrange(year, mes_referencia)[1]
        fecha_ref = None
        if dia_original <= ultimo_dia_mes_ref:
            # El d√≠a existe en el mes de referencia
            fecha_ref = f"{year}-{mes_referencia:02d}-{dia_original:02d}"
            query_exacta = """
            SELECT fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva
            FROM potencia_dep_processed
            WHERE alimentador = %s
            AND fecha = %s::date
            AND hora = %s
            AND (potencia_activa > 0 OR potencia_reactiva != 0)
            """
            resultado = pg_hook.get_first(query_exacta, parameters=(alimentador, fecha_ref, hora))
            
            if resultado and resultado[2] is not None:
                print(f"‚úÖ Encontrada referencia en fecha exacta {fecha_ref}, hora {hora}")
                return {
                    'fecha': resultado[0],
                    'hora': resultado[1],
                    'alimentador': resultado[2],
                    'dia_semana': resultado[3],
                    'potencia_activa': float(resultado[4]),
                    'potencia_reactiva': float(resultado[5]),
                    'fuente': f'mes{mes_referencia}-dia-exacto'
                }
        else:
            fecha_ref = f"{year}-{mes_referencia:02d}-{ultimo_dia_mes_ref:02d}"
        # 2. ESTRATEGIA 2: Buscar equivalente para d√≠as que no existen en el mes destino
        if dia_original > ultimo_dia_mes_ref:
            # Usar el √∫ltimo d√≠a disponible en el mes de referencia
            fecha_ref = f"{year}-{mes_referencia:02d}-{ultimo_dia_mes_ref:02d}"
            
            query_ultimo_dia = """
            SELECT fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva
            FROM potencia_dep_processed
            WHERE alimentador = %s
            AND fecha = %s::date
            AND hora = %s
            AND (potencia_activa > 0 OR potencia_reactiva != 0)
            """
            resultado = pg_hook.get_first(query_ultimo_dia, parameters=(alimentador, fecha_ref, hora))
            
            if resultado and resultado[2] is not None:
                print(f"‚úÖ Encontrada referencia en √∫ltimo d√≠a del mes {fecha_ref}, hora {hora}")
                return {
                    'fecha': resultado[0],
                    'hora': resultado[1],
                    'alimentador': resultado[2],
                    'dia_semana': resultado[3],
                    'potencia_activa': float(resultado[4]),
                    'potencia_reactiva': float(resultado[5]),
                    'fuente': f'mes{mes_referencia}-ultimo-dia'
                }
        # 3. ESTRATEGIA 3: Buscar mismo d√≠a de la semana y semana relativa
        # Convertir los objetos a datetime para evitar problemas de tipos
        fecha_inicio_mes_ref = datetime(year, mes_referencia, 1)
        fecha_inicio_mes_original = datetime(year, mes_original, 1)
        
        # Asegurarnos que fecha_dt es datetime para poder restar
        if isinstance(fecha_dt, datetime.date) and not isinstance(fecha_dt, datetime.datetime):
            fecha_dt_full = datetime.combine(fecha_dt, datetime.min.time())
        else:
            fecha_dt_full = fecha_dt

        # Calcular la diferencia de semanas (ahora con tipos compatibles)
        dias_diferencia = (fecha_dt_full - fecha_inicio_mes_original).days
        semana_relativa_original = dias_diferencia // 7
        
        # Calcular el primer d√≠a del mes con el mismo d√≠a de la semana
        dias_ajuste = (dia_semana_original - fecha_inicio_mes_ref.weekday()) % 7
        primer_dia_semana_mes_ref = fecha_inicio_mes_ref + timedelta(days=dias_ajuste)
        
        # Calcular la fecha que corresponde a la misma semana relativa y d√≠a de la semana
        fecha_equivalente = primer_dia_semana_mes_ref + timedelta(weeks=semana_relativa_original)
        
        # Asegurarse que no exceda el mes
        if fecha_equivalente.month == mes_referencia:
            fecha_ref = fecha_equivalente.strftime('%Y-%m-%d')
            
            query_equivalente = """
            SELECT fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva
            FROM potencia_dep_processed
            WHERE alimentador = %s
            AND fecha = %s::date
            AND hora = %s
            AND (potencia_activa > 0 OR potencia_reactiva != 0)
            """
            resultado = pg_hook.get_first(query_equivalente, parameters=(alimentador, fecha_ref, hora))
            
            if resultado and resultado[2] is not None:
                print(f"‚úÖ Encontrada referencia en d√≠a de semana equivalente {fecha_ref}, hora {hora}")
                return {
                    'fecha': resultado[0],
                    'hora': resultado[1],
                    'alimentador': resultado[2],
                    'dia_semana': resultado[3],
                    'potencia_activa': float(resultado[4]),
                    'potencia_reactiva': float(resultado[5]),
                    'fuente': f'mes{mes_referencia}-dia-semana-equivalente'
                }
        
        # 4. ESTRATEGIA 4: Buscar cualquier dato para la misma hora
        query_cualquier_fecha = f"""
        SELECT fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva
        FROM potencia_dep_processed
        WHERE alimentador = %s
        AND EXTRACT(MONTH FROM fecha) = {mes_referencia}
        AND EXTRACT(YEAR FROM fecha) = {year}
        AND hora = %s
        AND potencia_activa > 0
        AND potencia_reactiva != 0
        ORDER BY fecha
        LIMIT 1
        """
        
        resultado = pg_hook.get_first(query_cualquier_fecha, parameters=(alimentador, hora))
        
        if resultado and resultado[2] is not None:
            print(f"‚úÖ Encontrada referencia con cualquier fecha en mes {mes_referencia}, fecha {resultado[0]}, hora {hora}")
            return {
                'fecha': resultado[0],
                'hora': resultado[1],
                'alimentador': resultado[2],
                'dia_semana': resultado[3],
                'potencia_activa': float(resultado[4]),
                'potencia_reactiva': float(resultado[5]),
                'fuente': f'mes{mes_referencia}-cualquier-fecha'
            }
        
        # 5. ESTRATEGIA 5: Buscar en datos_enviados si potencia_dep_processed no tiene datos
        query_datos_enviados = f"""
        SELECT fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva 
        FROM datos_enviados
        WHERE alimentador = %s
        AND EXTRACT(MONTH FROM fecha) = {mes_referencia}
        AND EXTRACT(YEAR FROM fecha) = {year}
        AND hora = %s::time
        AND potencia_activa > 0
        AND potencia_reactiva != 0
        ORDER BY fecha
        LIMIT 1
        """
        resultado = pg_hook.get_first(query_datos_enviados, parameters=(alimentador, hora))
        if resultado and resultado[2] is not None:
            print(f"‚úÖ Encontrada referencia en datos_enviados para mes {mes_referencia}, fecha {resultado[0]}, hora {hora}")
            return {
                'fecha': resultado[0],
                'hora': resultado[1],
                'alimentador': resultado[2],
                'dia_semana': resultado[3],
                'potencia_activa': float(resultado[4]),
                'potencia_reactiva': float(resultado[5]),
                'fuente': f'datos_enviados-mes{mes_referencia}'
            }
        
        print(f"‚ùå No se encontr√≥ ninguna referencia v√°lida en el mes {mes_referencia}")
        return None
    except Exception as e:
        print(f"‚ùå Error buscando referencia en mes {mes_referencia}: {str(e)}")
        traceback.print_exc()
        return None

def procesar_standby_mes_anterior_mejorado(mes_actual, **kwargs):
    """
    Versi√≥n mejorada que busca datos en m√∫ltiples fuentes y con mejor manejo de errores.
    """
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        # Configurar productor Kafka
        productor = KafkaProducer(
            bootstrap_servers=['kafka_centrosur:9092'],
            value_serializer=lambda v: json.dumps(v, default=decimal_json_serializer).encode('utf-8')
        )
        year = YEAR
        if mes_actual <= 1:
            print("‚ö†Ô∏è No hay mes anterior para procesar (mes actual es 1)")
            return 0
            
        mes_anterior = mes_actual - 1
        
        # Obtener registros en standby del mes anterior
        query_standby = """
        SELECT alimentador, fecha, hora
        FROM registros_standby
        WHERE EXTRACT(MONTH FROM fecha) = %s
        AND EXTRACT(YEAR FROM fecha) = %s
        AND activo = TRUE
        """
        standby_registros = pg_hook.get_records(query_standby, parameters=(mes_anterior, year))
        
        if not standby_registros:
            print(f"‚úÖ No hay registros en standby del mes {mes_anterior} para procesar")
            return 0
            
        print(f"üìã Procesando {len(standby_registros)} registros en standby del mes {mes_anterior}")
        
        # Buscar datos en ambas tablas para verificar disponibilidad
        query_datos_proc = f"""
        SELECT COUNT(*) FROM potencia_dep_processed 
        WHERE EXTRACT(MONTH FROM fecha) = {mes_actual}
        AND EXTRACT(YEAR FROM fecha) = {year}
        """
        
        query_datos_env = f"""
        SELECT COUNT(*) FROM datos_enviados
        WHERE EXTRACT(MONTH FROM fecha) = {mes_actual}
        AND EXTRACT(YEAR FROM fecha) = {year}
        """
        
        datos_proc = pg_hook.get_first(query_datos_proc)[0]
        datos_env = pg_hook.get_first(query_datos_env)[0]
        
        print(f"üìä Disponibilidad de datos para mes {mes_actual}: {datos_proc} en processed, {datos_env} en enviados")
        
        # Determinar meses de referencia por prioridad
        meses_referencia = []
        # 1. Incluir el mes actual
        meses_referencia.append(mes_actual)
        # 2. Si estamos en mes 6 o posterior, incluir mes 6 como referencia prioritaria
        if mes_actual >= 6 and mes_actual != 6:
            meses_referencia.insert(0, 6)  # Poner mes 6 como prioridad m√°xima
        # 3. Para garantizar cobertura, agregar otros meses cercanos
        if mes_actual < 6:
            # Si estamos antes del mes 6, buscar en meses futuros hasta 6
            for m in range(mes_actual + 1, min(7, 13)):
                if m not in meses_referencia:
                    meses_referencia.append(m)
        else:
            # Si estamos despu√©s del mes 6, buscar en meses anteriores hasta 6
            for m in range(mes_actual - 1, 5, -1):
                if m not in meses_referencia:
                    meses_referencia.append(m)
        
        print(f"üîç Meses de referencia (por prioridad): {meses_referencia}")
        # Procesar cada registro en standby
        standby_procesados = 0
        standby_fallidos = 0
        
        for standby in standby_registros:
            try:
                alimentador, fecha, hora = standby
                dato_referencia = None
                
                # Intentar con cada mes de referencia hasta encontrar datos v√°lidos
                for mes_ref in meses_referencia:
                    print(f"üîç Buscando referencia para {alimentador}, {fecha}, {hora} en mes {mes_ref}")
                    
                    try:
                        # Buscar datos de referencia en este mes
                        dato_ref = buscar_dato_referencia_mes(pg_hook, alimentador, fecha, hora, mes_ref, year)
                        if dato_ref and 'potencia_activa' in dato_ref:
                            dato_referencia = dato_ref
                            print(f"‚úÖ Encontrada referencia en mes {mes_ref}")
                            break
                    except Exception as e_ref:
                        print(f"‚ö†Ô∏è Error al buscar en mes {mes_ref}: {str(e_ref)}")
                        continue
                
                # Si encontramos datos de referencia, actualizar el registro standby
                if dato_referencia and 'potencia_activa' in dato_referencia and 'potencia_reactiva' in dato_referencia:
                    motivo = f"Corregido con datos del mes {mes_actual} (fuente: {dato_referencia.get('fuente', 'desconocida')})"
                    try:
                        actualizar_registro_standby(
                            pg_hook, 
                            alimentador, 
                            fecha, 
                            hora, 
                            dato_referencia.get('potencia_activa'), 
                            dato_referencia.get('potencia_reactiva'),
                            motivo
                        )
                        standby_procesados += 1
                        dia_semana_correcto = calcular_dia_semana(fecha)

                        # Publicar la correcci√≥n a Kafka tambi√©n
                        tema_correcciones = "potencia-correcciones"
                        mensaje_correccion = {
                            'alimentador': alimentador,
                            'fecha': fecha.strftime('%Y-%m-%d') if hasattr(fecha, 'strftime') else str(fecha),
                            'hora': str(hora),
                            'potencia_activa': float(dato_referencia.get('potencia_activa', 0)),
                            'potencia_reactiva': float(dato_referencia.get('potencia_reactiva', 0)),
                            'dia_semana': dia_semana_correcto,  # Usar d√≠a semana calculado correcto
                            'mes_origen': mes_anterior,
                            'mes_referencia': mes_actual,
                            'fuente': dato_referencia.get('fuente', 'desconocida')
                        }
                        
                        try:
                            productor.send(tema_correcciones, mensaje_correccion)
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error publicando correcci√≥n a Kafka: {str(e)}")
                    except Exception as e_update:
                        print(f"‚ùå Error actualizando registro standby: {str(e_update)}")
                        standby_fallidos += 1
                else:
                    print(f"‚ùå No se encontr√≥ referencia para {alimentador}, {fecha}, {hora}")
                    standby_fallidos += 1
            except Exception as e_proc:
                print(f"‚ùå Error procesando registro standby: {str(e_proc)}")
                standby_fallidos += 1
        
        # Limpiar recursos
        productor.flush()
        
        print(f"‚úÖ Procesados {standby_procesados} de {len(standby_registros)} registros standby")
        print(f"‚ö†Ô∏è Fallidos: {standby_fallidos} registros")
        
        return standby_procesados
        
    except Exception as e:
        print(f"‚ùå Error procesando registros standby: {str(e)}")
        traceback.print_exc()
        return 0

def publicar_directamente_a_kafka_sin_tabla_temporal(pg_hook, productor, mes, year=YEAR):
    """Publica datos al topic potencia_datos_depurados directamente desde datos_enviados"""
    try:
        print(f"üì§ Publicando datos directamente a Kafka para mes {mes}...")
        
        # Consultar datos originales
        query = f"""
        SELECT fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva
        FROM datos_enviados
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = {year}
        -- Eliminar el filtro para incluir todos los registros
        ORDER BY fecha, hora, alimentador
        """
        datos = pg_hook.get_records(query)
        if not datos:
            print(f"‚ö†Ô∏è No se encontraron datos para publicar del mes {mes}")
            return 0
            
        # Publicar cada registro a ambos topics
        topic_datos = "potencia_datos_depurados"
        topic_calidad = "calidad-potencia"
        registros_enviados = 0
        
        for dato in datos:
            mensaje = {
                'fecha': dato[0].strftime('%Y-%m-%d') if hasattr(dato[0], 'strftime') else str(dato[0]),
                'hora': dato[1].strftime('%H:%M:%S') if hasattr(dato[1], 'strftime') else str(dato[1]),
                'alimentador': dato[2],
                'dia_semana': dato[3],
                'potencia_activa': float(dato[4]) if dato[4] is not None else 0.0,
                'potencia_reactiva': float(dato[5]) if dato[5] is not None else 0.0
            }
            
            # Enviar a topic principal
            productor.send(topic_datos, mensaje)
            
            # Enviar metadatos a topic de calidad
            if registros_enviados % 1000 == 0:
                info_calidad = {
                    'alimentador': dato[2],
                    'mes': mes,
                    'year': year,
                    'estado': 'procesado_directo',
                    'calidad': 'directa',
                    'total_registros': len(datos)
                }
                productor.send(topic_calidad, info_calidad)
            
            registros_enviados += 1
        
        # Forzar env√≠o pendiente
        productor.flush()
        print(f"‚úÖ Total de {registros_enviados} registros publicados al topic {topic_datos}")
        return registros_enviados
    except Exception as e:
        print(f"‚ùå Error publicando directamente a Kafka: {str(e)}")
        traceback.print_exc()
        return 0

def corregir_dia_semana_inconsistente(**kwargs):
    """
    Corrige cualquier inconsistencia en el campo dia_semana
    buscando registros del mismo d√≠a y alimentador.
    """
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        # Encontrar registros con d√≠a de semana inconsistente
        query_inconsistentes = """
        WITH dias_correctos AS (
            SELECT fecha, alimentador, 
                   MODE() WITHIN GROUP (ORDER BY dia_semana) as dia_semana_correcto
            FROM potencia_dep_processed
            GROUP BY fecha, alimentador
        )
        SELECT p.fecha, p.hora, p.alimentador, p.dia_semana, d.dia_semana_correcto
        FROM potencia_dep_processed p
        JOIN dias_correctos d ON p.fecha = d.fecha AND p.alimentador = d.alimentador
        WHERE p.dia_semana != d.dia_semana_correcto
        """
        
        inconsistencias = pg_hook.get_records(query_inconsistentes)
        
        if not inconsistencias:
            print("‚úÖ No se encontraron inconsistencias en los d√≠as de semana.")
            return 0
            
        print(f"‚ö†Ô∏è Encontradas {len(inconsistencias)} inconsistencias en d√≠as de semana.")
        
        # Actualizar registros inconsistentes
        for fecha, hora, alimentador, dia_actual, dia_correcto in inconsistencias:
            print(f"üîÑ Corrigiendo: {alimentador}, {fecha}, {hora}: {dia_actual} ‚Üí {dia_correcto}")
            
            query_update = """
            UPDATE potencia_dep_processed
            SET dia_semana = %s
            WHERE fecha = %s
            AND hora = %s
            AND alimentador = %s
            """
            
            pg_hook.run(query_update, parameters=(dia_correcto, fecha, hora, alimentador))
            
        return len(inconsistencias)
        
    except Exception as e:
        print(f"‚ùå Error corrigiendo d√≠as de semana: {str(e)}")
        return -1

def procesar_alimentador_para_mes_mejorado(pg_hook, productor, alimentador, mes, year=YEAR):
    """Versi√≥n mejorada que preserva todos los registros y no elimina datos"""
    tabla_procesados_temp = None
    try:
        # Definir fechas para el rango del mes
        mes_siguiente = mes + 1 if mes < 12 else 1
        ano_siguiente = year if mes < 12 else year + 1
        fecha_inicio = f"{year}-{mes:02d}-01"
        fecha_fin = f"{ano_siguiente}-{mes_siguiente:02d}-01"
        
        # Verificar si hay registros en standby de meses anteriores para este alimentador
        if mes > 1:
            print(f"üîç Verificando registros en standby de meses anteriores para alimentador {alimentador}")
            corregir_standby_anteriores(pg_hook, productor, alimentador, mes, year)
        
        # Verificar calidad del alimentador
        query_calidad = f"""
        SELECT 
            COUNT(*) as total,
            SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) as ceros,
            ROUND(100 - (SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) * 100.0) / COUNT(*), 2) as calidad
        FROM datos_enviados
        WHERE alimentador = %s
        AND EXTRACT(MONTH FROM fecha) = %s
        AND EXTRACT(YEAR FROM fecha) = %s
        """
        
        resultado_calidad = pg_hook.get_first(query_calidad, parameters=(alimentador, mes, year))
        
        if not resultado_calidad:
            print(f"‚ö†Ô∏è No hay datos para el alimentador {alimentador} en el mes {mes}")
            return {
                'alimentador': alimentador,
                'mes': mes,
                'estatus': 'sin_datos',
                'mensaje': 'No hay datos para este alimentador'
            }
        
        total, ceros, calidad = resultado_calidad
        necesita_depuracion = calidad < 100
        
        # Registrar inicio del procesamiento
        registrar_log(
            pg_hook, 
            'INFO', 
            f'Iniciando procesamiento de alimentador {alimentador} para mes {mes} (Calidad: {calidad}%)',
            alimentador=alimentador,
            fecha=fecha_inicio,
            detalles={'calidad_inicial': calidad, 'total': total, 'ceros': ceros}
        )
        
        # Obtener datos originales - CAMBIO: Consulta directa a potencia_dep
        query_datos = """
        SELECT fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva
        FROM potencia_dep
        WHERE alimentador = %s
          AND EXTRACT(MONTH FROM fecha) = %s
          AND EXTRACT(YEAR FROM fecha) = %s
        ORDER BY fecha, hora
        """
        datos_originales = pg_hook.get_records(query_datos, parameters=(alimentador, mes, year))
        
        # CAMBIO IMPORTANTE: Verificar que realmente se obtuvieron datos
        if not datos_originales:
            print(f"‚ö†Ô∏è No se obtuvieron datos originales para alimentador {alimentador} en mes {mes}. Verificando en tabla alternativa...")
            # Intentar en tabla alternativa datos_enviados
            query_alt = """
            SELECT fecha, hora, alimentador, dia_semana, potencia_activa, potencia_reactiva
            FROM datos_enviados
            WHERE alimentador = %s
              AND EXTRACT(MONTH FROM fecha) = %s
              AND EXTRACT(YEAR FROM fecha) = %s
            ORDER BY fecha, hora
            """
            datos_originales = pg_hook.get_records(query_alt, parameters=(alimentador, mes, year))
            
            if not datos_originales:
                print(f"‚ö†Ô∏è No hay datos disponibles para alimentador {alimentador} en mes {mes} en ninguna tabla.")
                return {
                    'alimentador': alimentador,
                    'mes': mes,
                    'estatus': 'sin_datos',
                    'mensaje': 'No hay datos en ninguna tabla fuente'
                }
        
        # Procesar y depurar cada registro
        datos_depurados = []
        standby_registros = []
        cambios_realizados = 0
        registros_enviados_kafka = 0
        
        print(f"üîÑ Procesando {len(datos_originales)} registros para alimentador {alimentador} mes {mes}")
        # TOPIC √öNICO Y FIJO para garantizar consistencia
        topic_datos_depurados = "potencia_datos_depurados"
        
        # CAMBIO FUNDAMENTAL: Procesar TODOS los registros sin filtros de eliminaci√≥n
        for dato in datos_originales:
            fecha, hora, alim, dia_semana, p_activa, p_reactiva = dato
            
            # Convertir a formato est√°ndar
            registro = {
                'fecha': fecha.strftime('%Y-%m-%d') if hasattr(fecha, 'strftime') else str(fecha),
                'hora': hora.strftime('%H:%M:%S') if hasattr(hora, 'strftime') else str(hora),
                'alimentador': alim,
                'dia_semana': calcular_dia_semana(fecha),  # SIEMPRE calcular el d√≠a correcto
                'potencia_activa': float(p_activa) if p_activa is not None else 0.0,
                'potencia_reactiva': float(p_reactiva) if p_reactiva is not None else 0.0,
                'mes': mes,
                'year': year,
                'depurado': False,
                'cambio_realizado': False,
                'en_standby': False
            }
            
            # Verificar si necesita depuraci√≥n
            es_cero = (registro['potencia_activa'] == 0 and registro['potencia_reactiva'] == 0)
            
            # Si es cero y estamos antes del mes 6, marcar como standby pero IGUALMENTE procesarlo
            if es_cero and mes < 6:
                motivo = f"Datos cero detectados en fecha {registro['fecha']}, hora {registro['hora']}"
                marcar_registro_standby(pg_hook, alimentador, mes, registro['fecha'], registro['hora'], motivo)
                registro['en_standby'] = True
                standby_registros.append(registro)
                # CAMBIO CR√çTICO: No hacer "continue" aqu√≠, seguimos procesando
            
            # Aplicar depuraci√≥n si es necesario (pero sin eliminar registros)
            valores_antiguos = None
            
            if necesita_depuracion and es_cero and mes >= 6:
                valores_antiguos = {
                    'potencia_activa': registro['potencia_activa'],
                    'potencia_reactiva': registro['potencia_reactiva']
                }
                
                # Buscar datos de referencia para reemplazo
                referencia = buscar_dato_referencia_mejorado(
                    pg_hook, alimentador, registro['fecha'], registro['hora'], mes, year
                )
                
                registro['potencia_activa'] = referencia['potencia_activa']
                registro['potencia_reactiva'] = referencia['potencia_reactiva']
                registro['depurado'] = True
                registro['cambio_realizado'] = True
                registro['fuente_datos'] = referencia['fuente']
                
                cambios_realizados += 1
            
            datos_depurados.append(registro)
            
            # Enviar TODOS los registros a Kafka, tengan el valor que tengan
            mensaje_kafka = {
                'fecha': registro['fecha'],
                'hora': registro['hora'],
                'alimentador': registro['alimentador'],
                'dia_semana': registro['dia_semana'],  # D√≠a semana calculado correctamente
                'potencia_activa': float(registro['potencia_activa']),
                'potencia_reactiva': float(registro['potencia_reactiva'])
            }
            
            # Enviar a Kafka inmediatamente
            try:
                productor.send(topic_datos_depurados, mensaje_kafka)
                registros_enviados_kafka += 1
                
                # Log detallado para debugging (cada 100 mensajes)
                if registros_enviados_kafka % 1000 == 0:
                    print(f"üîÑ Enviados {registros_enviados_kafka} mensajes a Kafka")
            except Exception as ke:
                print(f"‚ùå Error enviando a Kafka: {str(ke)}")
        
        # IMPORTANTE: Forzar env√≠o pendiente DESPU√âS del bucle
        productor.flush()
        print(f"‚úÖ Total: {registros_enviados_kafka} registros publicados a {topic_datos_depurados}")
        
        # Registrar finalizaci√≥n con detalles
        registrar_log(
            pg_hook,
            'INFO',
            f'Procesamiento completado: {alimentador}, mes {mes}. Enviados: {registros_enviados_kafka}',
            alimentador=alimentador,
            fecha=fecha_inicio,
            detalles={
                'registros_originales': len(datos_originales),
                'registros_enviados_kafka': registros_enviados_kafka,
                'registros_en_standby': len(standby_registros),
                'cambios_realizados': cambios_realizados
            }
        )
        
        return {
            'alimentador': alimentador,
            'mes': mes,
            'estatus': 'procesado',
            'registros_enviados': registros_enviados_kafka,
            'registros_depurados': cambios_realizados,
            'registros_originales': len(datos_originales)
        }
    except Exception as e:
        error_msg = f"Error procesando alimentador {alimentador} mes {mes}: {str(e)}"
        print(f"‚ùå {error_msg}")
        traceback.print_exc()
        
        # Intentar enviar mensaje de error a Kafka
        try:
            productor.send("errores-procesamiento", {
                'alimentador': alimentador,
                'mes': mes,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            })
            productor.flush()
        except:
            pass
            
        return {
            'alimentador': alimentador,
            'mes': mes,
            'estatus': 'error',
            'error': str(e)
        }
    finally:
        # Limpieza de recursos
        if tabla_procesados_temp:
            try:
                pg_hook.run(f"DROP TABLE IF EXISTS {tabla_procesados_temp}")
                print(f"üßπ Tabla temporal {tabla_procesados_temp} eliminada")
            except Exception as cleanup_error:
                print(f"‚ö†Ô∏è Error limpiando tabla: {str(cleanup_error)}")
def configurar_kafka_topics_depuracion(mes):
    """Configura los topics de Kafka para el flujo de depuraci√≥n"""
    try:
        # Topic para datos depurados
        depuracion_topic = f"potencia-depurada-mes{mes}"
        
        # Configurar el consumer del grupo de an√°lisis para leer del topic correcto
        configurar_topic_procesador(
            CONSUME_KAFKA_PROCESSOR_ID, 
            "Topic Name", 
            depuracion_topic
        )
        
        # Configurar el producer de depuraci√≥n para publicar al topic correcto
        return f"Topics Kafka configurados correctamente para mes {mes}"
    except Exception as e:
        print(f"Error configurando topics: {str(e)}")
        return f"Error: {str(e)}"
def iniciar_procesadores_productor():
    """Inicia SOLO los procesadores del grupo productor"""
    try:
        producer_processors = [
            {"id": QUERY_DB_TABLE_PROCESSOR_ID, "name": "QueryDatabaseTable"},
            {"id": CONVERT_RECORD_ID, "name": "ConvertRecord"},
            {"id": SPLIT_JSON_ID, "name": "SplitJson"},
            {"id": MERGE_CONTENT_ID, "name": "MergeContent"},
            {"id": PUBLISH_KAFKA_ID, "name": "PublishKafka"},
            {"id": PUT_DATABASE_RECORD_ID, "name": "PutDatabaseRecord"},
        ]
        
        print("‚ñ∂Ô∏è Iniciando SOLO procesadores del productor...")
        for processor in producer_processors:
            result = iniciar_procesador_con_reintento(processor["id"], processor["name"])
            print(f"   Resultado iniciar {processor['name']}: {result}")
            time.sleep(2)  # Pausa entre inicios
        
        return True
    except Exception as e:
        print(f"‚ùå Error iniciando procesadores productor: {str(e)}")
        return False
def iniciar_procesadores_consumidor():
    """Inicia los procesadores del grupo consumidor"""
    try:
        consumer_processors = [
            {"id": CONSUME_KAFKA_PROCESSOR_ID, "name": "ConsumeKafka"},
            {"id": EVALUATE_JSON_PATH_ID, "name": "EvaluateJsonPath"},
            {"id": REPLACE_TEXT_ID, "name": "ReplaceText"},
            {"id": PUT_DB_PROCESSOR_ID, "name": "PutDatabaseRecord"},
        ]
        
        print("‚ñ∂Ô∏è Iniciando procesadores del consumidor...")
        for processor in consumer_processors:
            result = iniciar_procesador_con_reintento(processor["id"], processor["name"])
            print(f"   Resultado iniciar {processor['name']}: {result}")
            time.sleep(2)  # Pausa entre inicios
        
        return True
    except Exception as e:
        print(f"‚ùå Error iniciando procesadores consumidor: {str(e)}")
        return False
    
def decimal_json_serializer(obj):
    """Serializador JSON personalizado que maneja objetos Decimal"""
    if isinstance(obj, decimal.Decimal):
        return float(obj)
    raise TypeError(f"Type {type(obj)} not serializable")

def depurar_mes_mejorado(mes, **kwargs):
    """
    Versi√≥n mejorada de la depuraci√≥n que incorpora an√°lisis de calidad
    y estrategia inteligente de reemplazo, usando NiFi/Kafka consistentemente.
    """
    print(f"üîç Iniciando an√°lisis para el mes {mes}")
    try:
        ti = kwargs.get('ti')
        if ti and ti.xcom_pull(key=f'mes_{mes}_sin_datos_reales'):
            print(f"‚ö†Ô∏è El mes {mes} fue marcado como sin datos reales. Saltando depuraci√≥n.")
            return f"Mes {mes} no tiene datos reales. Depuraci√≥n saltada."
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        pg_hook_origen = PostgresHook(postgres_conn_id="postgres_centrosur")
        
        # Verificar primero si el mes ya tiene datos completos
        query_count = f"""
        SELECT COUNT(*) FROM potencia_dep_processed 
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = 2019
        """
        count = pg_hook.get_first(query_count)[0]
        
        # Obtener din√°micamente los registros esperados directamente de la fuente
        # para hacer la funci√≥n escalable hasta a√±os futuros
        try:
            # Intentar primero con potencia_dep_original
            query_origen = f"""
            SELECT COUNT(*) 
            FROM potencia_dep_original
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = 2019
            """
            registros_esperados_count = pg_hook_origen.get_first(query_origen)[0]
            
            if registros_esperados_count == 0:  # Si no hay registros, intentar con potencia_dep
                query_origen_alt = f"""
                SELECT COUNT(*) 
                FROM potencia_dep
                WHERE EXTRACT(MONTH FROM fecha) = {mes}
                AND EXTRACT(YEAR FROM fecha) = 2019
                """
                registros_esperados_count = pg_hook_origen.get_first(query_origen_alt)[0]
                
            print(f"üìä Registros esperados para mes {mes}: {registros_esperados_count} (consultados din√°micamente)")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error consultando registros esperados en origen: {str(e)}")
            # Si falla la consulta din√°mica, usar los valores predefinidos como respaldo
            registros_esperados = {
                1: 214272, 2: 190848, 3: 211296, 4: 204480, 
                5: 208896, 6: 213120, 7: 220224, 8: 220224, 
                9: 213120, 10: 2976, 11: 213120, 12: 220032
            }
            registros_esperados_count = registros_esperados.get(mes, 0)
            print(f"üìä Usando valor predefinido para mes {mes}: {registros_esperados_count} registros")
            
        # Si el conteo es cercano al esperado y hay registros esperados v√°lidos
        if registros_esperados_count > 0 and count >= registros_esperados_count * 0.99:
            completitud = (count / registros_esperados_count) * 100
            print(f"‚úÖ El mes {mes} ya tiene {completitud:.2f}% de completitud ({count}/{registros_esperados_count})")
            print(f"‚úÖ No es necesario realizar la depuraci√≥n nuevamente")
            return f"Mes {mes} ya tiene datos completos: {count} registros"
            
        crear_tabla_registros_standby()
        print(f"üîç Iniciando proceso de depuraci√≥n mejorado para el mes {mes}")
        
        # Conexi√≥n a la base de datos
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        
        # Configurar productor Kafka
        productor = KafkaProducer(
            bootstrap_servers=['kafka_centrosur:9092'],
            value_serializer=lambda v: json.dumps(v, default=decimal_json_serializer).encode('utf-8')
        )
        
        # Detener procesadores antes de configuralos
        print("‚è∏Ô∏è Deteniendo todos los procesadores antes de configurar...")
        stop_all_processors_individually()
        time.sleep(5)  # Dar tiempo para que se detengan
        
        # Configurar un topic Kafka √∫nico para este mes
        topic_mes = f"potencia-depurada-mes{mes}-{int(time.time())}"
        print(f"üîÑ Configurando topic Kafka espec√≠fico: {topic_mes}")
        
        # Configurar procesadores para usar este topic
        configurar_topic_procesador(
            CONSUME_KAFKA_PROCESSOR_ID,
            "Topic Name",
            topic_mes
        )
        
        # Configurar PublishKafka para el productor
        configurar_topic_procesador(
            PUBLISH_KAFKA_ID,
            "Topic Name", 
            topic_mes
        )
        
        # Realizar an√°lisis de calidad previo
        analisis = analizar_calidad_integridad(pg_hook, mes, YEAR)
        
        # CAMBIO: No separar en perfectos y a depurar, procesar TODOS igualmente
        alimentadores = []
        for alim in analisis['alimentadores_perfectos']:
            alimentadores.append(alim['alimentador'])
            
        for alim in analisis['alimentadores_a_depurar']:
            if alim['alimentador'] not in alimentadores:
                alimentadores.append(alim['alimentador'])
        
        print(f"üìä Procesando {len(alimentadores)} alimentadores en total para preservar todos los datos")
        
        # Procesar todos los alimentadores sin distinci√≥n
        resultados = []
        for alimentador in alimentadores:
            resultado = procesar_alimentador_para_mes_mejorado(pg_hook, productor, alimentador, mes, YEAR)
            resultados.append(resultado)
        
        # Estad√≠sticas de procesamiento
        completados = sum(1 for r in resultados if r.get('estatus') == 'procesado')
        errores = sum(1 for r in resultados if r.get('estatus') == 'error')
        registros_depurados = sum(r.get('registros_depurados', 0) for r in resultados)
        registros_originales = sum(r.get('registros_originales', 0) for r in resultados if r.get('registros_originales') is not None)
        registros_enviados = sum(r.get('registros_enviados', 0) for r in resultados)
        
        mensaje = f"""‚úÖ Depuraci√≥n mejorada completada para el mes {mes}:
        - Total alimentadores: {len(alimentadores)}
        - Alimentadores procesados exitosamente: {completados}
        - Registros originales: {registros_originales}
        - Registros enviados: {registros_enviados}
        - Registros modificados: {registros_depurados}
        - Errores: {errores}"""
        
        print(mensaje)
        
        # Enviar todos los datos a trav√©s de NiFi/Kafka
        print("üì§ Enviando todos los datos depurados a Kafka...")
        # Primero intentar desde la tabla procesada
        total_enviados = publicar_datos_depurados_a_kafka(pg_hook, productor, mes, YEAR)
        
        # El mes 8 necesita atenci√≥n especial
        if mes == 8 and total_enviados == 0:
            print(f"‚ö†Ô∏è Atenci√≥n especial para mes 8: implementando env√≠o mejorado...")
            
            # Limpiar estado del procesador para forzar reprocesamiento completo
            clear_processor_state_and_empty_all_queues()
            
            # Configurar temas espec√≠ficos para mes 8
            topic_especial = f"potencia-mes8-especial-{int(time.time())}"
            configurar_topic_procesador(PUBLISH_KAFKA_ID, "Topic Name", topic_especial)
            configurar_topic_procesador(CONSUME_KAFKA_PROCESSOR_ID, "Topic Name", topic_especial)
            
            # Si no hay datos procesados, intentar enviarlos directamente
            # pero mantener la coherencia usando el mismo flujo NiFi/Kafka
            total_enviados = publicar_directamente_a_kafka_sin_tabla_temporal(pg_hook, productor, mes, YEAR)
            
            # Tambi√©n verificar datos_enviados como √∫ltimo recurso
            if total_enviados == 0:
                print("üîé Buscando en tabla datos_enviados como fuente alternativa...")
                query_alt = f"""
                SELECT 
                    fecha, 
                    hora, 
                    alimentador, 
                    dia_semana, 
                    potencia_activa, 
                    potencia_reactiva
                FROM datos_enviados
                WHERE EXTRACT(MONTH FROM fecha) = {mes}
                AND EXTRACT(YEAR FROM fecha) = {YEAR}
                """
                datos_alt = pg_hook.get_records(query_alt)
                
                if datos_alt:
                    print(f"‚úÖ Encontrados {len(datos_alt)} registros alternativos para mes {mes}")
                    for dato in datos_alt:
                        mensaje_kafka = {
                            'fecha': dato[0].strftime('%Y-%m-%d') if hasattr(dato[0], 'strftime') else str(dato[0]),
                            'hora': str(dato[1]),
                            'alimentador': str(dato[2]),
                            'dia_semana': calcular_dia_semana(dato[0]),  # Siempre usar d√≠a correcto
                            'potencia_activa': float(dato[4]) if dato[4] is not None else 0.0,
                            'potencia_reactiva': float(dato[5]) if dato[5] is not None else 0.0
                        }
                        productor.send(topic_especial, mensaje_kafka)
                    productor.flush()
                    total_enviados = len(datos_alt)
        # Para otros meses, usar el flujo est√°ndar
        elif total_enviados == 0:
            print("‚ö†Ô∏è No se encontraron datos procesados en tabla potencia_dep_processed.")
            print("üîÑ Intentando publicaci√≥n alternativa manteniendo flujo NiFi/Kafka...")
            total_enviados = publicar_directamente_a_kafka_sin_tabla_temporal(pg_hook, productor, mes, YEAR)
            
        print(f"‚úÖ {total_enviados} registros enviados correctamente a Kafka")
        
        # Iniciar los procesadores NiFi
        print("‚ñ∂Ô∏è Iniciando procesadores NiFi para consumir datos...")
        # Iniciar PRIMERO los consumidores para que est√©n listos para recibir
        resultado_inicio = iniciar_procesadores_consumidor()
        print(f"   Consumidores iniciados: {'Exitoso' if resultado_inicio else 'Fallido'}")
        
        # Iniciar productores SOLO para el mes 10, eliminando el mes 8 de esta condici√≥n
        if mes == 10:  # <-- MODIFICADO: Quitado el mes 8 para evitar procesamiento duplicado
            print("‚ñ∂Ô∏è Iniciando productores adicionales para mes especial...")
            iniciar_procesadores_productor()
        
        # Esperar para que procesen los datos
        print("‚è≥ Esperando para que se complete el procesamiento...")
        wait_time_seconds = 1080  # 18 minutos
        interval = 60  # Mostrar progreso cada minuto
        
        # Esperar evaluando continuamente la completitud
        for elapsed in range(0, wait_time_seconds, interval):
            # Verificar si hay datos en la tabla destino solo para mostrar progreso
            query_check = f"""
            SELECT COUNT(*) FROM potencia_dep_processed
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = {YEAR}
            """
            count = pg_hook.get_first(query_check)[0]
            
            # Mostrar progreso sin salir del bucle
            remaining = (wait_time_seconds - elapsed) // 60
            print(f"‚è±Ô∏è Tiempo restante: {remaining} minutos... ({count}/{total_enviados} registros procesados)")
            
            # Si ya completamos el 100%, mostrar mensaje y esperar solo 1 minuto m√°s
            if count >= total_enviados:
                print(f"‚úÖ Ya tenemos el 100% de los datos: {count}/{total_enviados}. Esperando 1 minuto adicional para finalizar.")
                time.sleep(60)  # Esperar 1 minuto adicional para asegurar completitud
                break  # Salir del bucle de espera principal
                
            time.sleep(interval)
        
        # Si no alcanzamos el 100%, asegurar que se haya esperado el tiempo completo
        # En el caso de que hayamos salido del bucle anticipadamente, no se ejecuta esta parte
        if count < total_enviados:
            print(f"‚è≥ Tiempo completo alcanzado. Esperando 60 segundos adicionales para finalizar procesos pendientes...")
            time.sleep(60)
        
        # Verificaci√≥n final solo despu√©s de espera
        query_check = f"""
        SELECT COUNT(*) FROM potencia_dep_processed
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = {YEAR}
        """
        count_final = pg_hook.get_first(query_check)[0]
        completitud = (count_final / total_enviados * 100) if total_enviados > 0 else 0
        print(f"üìä Verificaci√≥n final: {count_final}/{total_enviados} registros procesados ({completitud:.2f}%)")
        
        # Detener procesadores despu√©s de la espera completa
        print(f"‚èπÔ∏è Deteniendo procesadores despu√©s de la espera completa...")
        stop_all_processors_individually()
        
        # A√±adir al mensaje de retorno
        mensaje += f"\n- Total enviados a Kafka: {total_enviados}"
        mensaje += f"\n- Total procesados final: {count_final} ({completitud:.2f}%)"
        
        return mensaje
    
    except Exception as e:
        error_msg = f"‚ùå Error en depuraci√≥n mejorada del mes {mes}: {str(e)}"
        print(error_msg)
        traceback.print_exc()
        return error_msg

def verificar_y_restaurar_tabla_original_al_inicio():
    """
    Verifica el estado de la tabla potencia_dep y la restaura desde potencia_dep_original
    si es necesario, antes de comenzar cualquier procesamiento.
    """
    try:
        print("üîç Verificando integridad de la tabla original potencia_dep...")
        pg_hook = PostgresHook(postgres_conn_id="postgres_centrosur")

        # 1. Verificar si existe el respaldo original
        respaldo_existe = pg_hook.get_first(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'potencia_dep_original')"
        )[0]

        if not respaldo_existe:
            print(
                "‚ö†Ô∏è No se encontr√≥ respaldo potencia_dep_original. Se crear√° si existe la tabla original."
            )

            # Verificar si existe la tabla original
            original_existe = pg_hook.get_first(
                "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'potencia_dep')"
            )[0]

            if original_existe:
                print("üìã Creando respaldo inicial potencia_dep_original...")
                pg_hook.run(
                    "CREATE TABLE potencia_dep_original AS SELECT * FROM potencia_dep"
                )
                print("‚úÖ Respaldo inicial creado exitosamente")
                return "Respaldo inicial creado"
            else:
                print(
                    "‚ùå ERROR CR√çTICO: No existe ni la tabla original ni el respaldo!"
                )
                return "Error: No existen tablas necesarias"

        # 2. Verificar si existe la tabla original
        original_existe = pg_hook.get_first(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'potencia_dep')"
        )[0]

        if not original_existe:
            print("‚ö†Ô∏è Tabla potencia_dep no existe. Restaurando desde respaldo...")
            pg_hook.run(
                "CREATE TABLE potencia_dep AS SELECT * FROM potencia_dep_original"
            )
            print("‚úÖ Tabla potencia_dep restaurada desde respaldo")
            return "Tabla restaurada desde respaldo"

        # 3. Verificar integridad comparando conteos
        count_original = pg_hook.get_first("SELECT COUNT(*) FROM potencia_dep")[0]
        count_backup = pg_hook.get_first("SELECT COUNT(*) FROM potencia_dep_original")[
            0
        ]

        print(f"üìä Conteo en potencia_dep: {count_original}")
        print(f"üìä Conteo en potencia_dep_original: {count_backup}")

        # Si hay una gran diferencia, restaurar desde respaldo
        if count_original < count_backup * 0.9:  # Si tiene menos del 90% de registros
            print(
                f"‚ö†Ô∏è Tabla potencia_dep parece inconsistente ({count_original} vs {count_backup})"
            )
            print("üîÑ Restaurando tabla completa desde respaldo...")

            # Backup de la tabla actual por si acaso
            backup_nombre = f"potencia_dep_backup_{int(time.time())}"
            pg_hook.run(f"CREATE TABLE {backup_nombre} AS SELECT * FROM potencia_dep")
            print(f"üìã Se cre√≥ respaldo adicional: {backup_nombre}")

            # Restaurar desde respaldo original
            pg_hook.run("DROP TABLE potencia_dep")
            pg_hook.run(
                "CREATE TABLE potencia_dep AS SELECT * FROM potencia_dep_original"
            )
            print("‚úÖ Tabla potencia_dep restaurada completamente desde respaldo")
            return "Tabla restaurada por inconsistencia"

        # 4. Limpieza de tablas temporales antiguas
        query_tablas_temp = """
        SELECT table_name FROM information_schema.tables 
        WHERE table_name LIKE 'potencia_dep_prev_%' OR table_name LIKE 'potencia_dep_temp_%'
        """
        tablas_temp = pg_hook.get_records(query_tablas_temp)

        if tablas_temp:
            print(f"üßπ Limpiando {len(tablas_temp)} tablas temporales antiguas...")
            for tabla in tablas_temp:
                pg_hook.run(f"DROP TABLE IF EXISTS {tabla[0]}")
            print("‚úÖ Limpieza de tablas temporales completada")

        return "Verificaci√≥n completada: Tabla original √≠ntegra"

    except Exception as e:
        print(f"‚ùå Error verificando tabla original: {str(e)}")
        return f"Error: {str(e)}"

def procesar_y_restaurar_mes(mes, **kwargs):
    """
    Procesa el mes utilizando la funci√≥n de sustituci√≥n de tabla y gestiona los casos sin datos.
    Incluye verificaci√≥n en m√∫ltiples tablas.
    """
    try:
        # Verificaci√≥n en la tabla principal procesada
        pg_hook_destino = PostgresHook(postgres_conn_id="postgres_base_prueba")
        query_procesados = f"""
        SELECT COUNT(*) FROM potencia_dep_processed 
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = 2019
        """
        count_procesados = pg_hook_destino.get_first(query_procesados)[0]
        
        # Verificaci√≥n en la tabla de datos enviados
        query_enviados = f"""
            SELECT COUNT(*) FROM datos_enviados 
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = 2019
        """
        count_enviados = pg_hook_destino.get_first(query_enviados)[0]
        
        # Si hay datos en cualquiera de las tablas, no procesar nuevamente
        if count_procesados > 0 or count_enviados > 0:
            print(f"üîí VERIFICACI√ìN M√öLTIPLE: El mes {mes} ya tiene datos:")
            print(f"   - {count_procesados} registros procesados")
            print(f"   - {count_enviados} registros enviados")
            print("NO SE PROCESAR√Å NUEVAMENTE.")
            return True
        
        # Verificaci√≥n de existencia de datos en fuentes
        tiene_datos = verificar_existencia_datos_fuente(mes)
        
        if not tiene_datos:
            print(f"‚ö†Ô∏è Advertencia: No se detectaron datos en ninguna fuente para el mes {mes}")
            kwargs['ti'].xcom_push(key=f'mes_{mes}_sin_datos_reales', value=True)
            print(f"‚ö†Ô∏è Este no es un error del DAG, simplemente no hay datos para procesar.")
            return True
        
        # Si hay datos, ejecutar el procesamiento
        print(f"  Iniciando procesamiento del mes {mes}...")
        
        # Crear tabla de monitoreo antes de iniciar procesamiento
        tabla_monitor = f"monitor_mes_{mes}_{int(time.time())}"
        pg_hook_destino.run(f"""
        CREATE TABLE IF NOT EXISTS {tabla_monitor} (
            id SERIAL PRIMARY KEY,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            mensaje TEXT,
            datos JSONB
        )
        """)
        pg_hook_destino.run(f"""
        INSERT INTO {tabla_monitor} (mensaje, datos)
        VALUES ('Inicio de procesamiento', 
        '{{"mes": {mes}, "estado": "iniciando", "timestamp": "{datetime.now().isoformat()}"}}'::jsonb)
        """)
        kwargs['ti'].xcom_push(key=f'tabla_monitor_mes_{mes}', value=tabla_monitor)
        
        # Ejecutar el procesamiento principal
        resultado_procesamiento = procesar_mes_sustituyendo_tabla(mes, **kwargs)
        
        # Registrar resultado en tabla de monitoreo
        pg_hook_destino.run(f"""
        INSERT INTO {tabla_monitor} (mensaje, datos)
        VALUES ('Procesamiento finalizado', 
        '{{"mes": {mes}, "estado": "{"exitoso" if resultado_procesamiento else "fallido"}"}}'::jsonb)
        """)
        
        return resultado_procesamiento
    except Exception as e:
        print(f"‚ùå Error procesando mes {mes}: {str(e)}")
        traceback.print_exc()
        return False

def generar_informe_calidad_detallado(**kwargs):
    """
    Genera un informe detallado por alimentador de los datos depurados y no depurados,
    mostrando estad√≠sticas por consola sin guardar en la base de datos.
    
    Returns:
        dict: Informe detallado de calidad de los datos
    """
    try:
        print("üìä Generando informe detallado de calidad por alimentador...")
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        
        # 1. Obtener lista de alimentadores
        query_alimentadores = """
        SELECT DISTINCT alimentador 
        FROM potencia_dep_processed 
        ORDER BY alimentador
        """
        alimentadores = [row[0] for row in pg_hook.get_records(query_alimentadores)]
        
        # 2. Estad√≠sticas generales
        query_stats_general = """
        SELECT 
            COUNT(*) as total_registros,
            COUNT(DISTINCT fecha) as total_dias,
            COUNT(DISTINCT alimentador) as total_alimentadores
        FROM potencia_dep_processed
        """
        stats_general = pg_hook.get_first(query_stats_general)
        
        # 3. Obtener registros en standby
        query_standby = """
        SELECT 
            alimentador, 
            COUNT(*) as registros_standby,
            MIN(fecha) as primer_standby,
            MAX(fecha) as ultimo_standby
        FROM registros_standby
        WHERE activo = TRUE
        GROUP BY alimentador
        """
        standby_por_alim = {}
        standby_results = pg_hook.get_records(query_standby)
        for row in standby_results:
            standby_por_alim[row[0]] = {
                'registros_standby': row[1],
                'primer_standby': row[2],
                'ultimo_standby': row[3]
            }
            
        # 4. Calcular informaci√≥n detallada por alimentador
        informe_alimentadores = []
        for alimentador in alimentadores:
            # 4.1 Conteo de registros
            query_registros = """
            SELECT 
                COUNT(*) as total_reg,
                SUM(CASE WHEN potencia_activa = 0 AND potencia_reactiva = 0 THEN 1 ELSE 0 END) as ceros,
                MIN(fecha) as fecha_inicio,
                MAX(fecha) as fecha_fin
            FROM potencia_dep_processed
            WHERE alimentador = %s
            """
            stats_alim = pg_hook.get_first(query_registros, parameters=(alimentador,))
            
            # 4.2 Verificar completitud (deber√≠a tener 96 registros por d√≠a)
            query_completitud = """
            WITH dias AS (
                SELECT DISTINCT fecha FROM potencia_dep_processed WHERE alimentador = %s
            )
            SELECT 
                COUNT(*) as total_dias,
                SUM(CASE WHEN registros = 96 THEN 1 ELSE 0 END) as dias_completos
            FROM (
                SELECT fecha, COUNT(*) as registros
                FROM potencia_dep_processed
                WHERE alimentador = %s
                GROUP BY fecha
            ) as conteo_por_dia
            """
            completitud = pg_hook.get_first(query_completitud, parameters=(alimentador, alimentador))
            
            # 4.3 Calcular indicadores de calidad
            if stats_alim[0] > 0:  # Si hay registros
                dias_totales = completitud[0] if completitud[0] else 0
                dias_completos = completitud[1] if completitud[1] else 0
                completitud_pct = (dias_completos / dias_totales * 100) if dias_totales > 0 else 0
                ceros_pct = (stats_alim[1] / stats_alim[0] * 100) if stats_alim[0] > 0 else 0
                calidad = 100 - ceros_pct  # Calidad b√°sica: % de registros no-cero
            else:
                dias_totales = 0
                dias_completos = 0
                completitud_pct = 0
                calidad = 0
                
            # 4.4 Informaci√≥n de standby
            standby_info = standby_por_alim.get(alimentador, {
                'registros_standby': 0,
                'primer_standby': None,
                'ultimo_standby': None
            })
                
            # 4.5 Agregar a informe
            alim_info = {
                'alimentador': alimentador,
                'registros_totales': stats_alim[0],
                'registros_cero': stats_alim[1],
                'fecha_inicio': stats_alim[2],
                'fecha_fin': stats_alim[3],
                'dias_totales': dias_totales,
                'dias_completos': dias_completos,
                'completitud_pct': round(completitud_pct, 2),
                'calidad_pct': round(calidad, 2),
                'registros_standby': standby_info['registros_standby'],
                'primer_standby': standby_info['primer_standby'],
                'ultimo_standby': standby_info['ultimo_standby'],
                'estado': '√ìptimo' if calidad >= 99 and completitud_pct >= 99 else 
                         'Bueno' if calidad >= 95 and completitud_pct >= 95 else
                         'Regular' if calidad >= 90 and completitud_pct >= 90 else
                         'Deficiente'
            }
            informe_alimentadores.append(alim_info)
        
        # 5. Ordenar alimentadores por calidad
        informe_alimentadores.sort(key=lambda x: (x['calidad_pct'], x['completitud_pct']), reverse=True)
        
        # 6. Resumen por estado
        resumen_estados = {
            '√ìptimo': sum(1 for a in informe_alimentadores if a['estado'] == '√ìptimo'),
            'Bueno': sum(1 for a in informe_alimentadores if a['estado'] == 'Bueno'),
            'Regular': sum(1 for a in informe_alimentadores if a['estado'] == 'Regular'),
            'Deficiente': sum(1 for a in informe_alimentadores if a['estado'] == 'Deficiente')
        }
        
        # 7. Imprimir informe resumido en consola
        print("\nüìä INFORME RESUMIDO DE CALIDAD POR ALIMENTADOR üìä")
        print("=" * 80)
        print(f"Total de alimentadores: {len(alimentadores)}")
        print(f"Total de registros procesados: {stats_general[0]:,}")
        print(f"Total de d√≠as cubiertos: {stats_general[1]}")
        print("-" * 80)
        print("RESUMEN POR ESTADO:")
        for estado, cantidad in resumen_estados.items():
            print(f"  {estado}: {cantidad} alimentadores ({cantidad/len(alimentadores)*100:.1f}%)")
        print("-" * 80)
        print("TOP 5 MEJORES ALIMENTADORES:")
        for i, alim in enumerate(informe_alimentadores[:5]):
            print(f"  {i+1}. {alim['alimentador']}: Calidad {alim['calidad_pct']}%, Completitud {alim['completitud_pct']}%")
        print("\nTOP 5 ALIMENTADORES CON M√ÅS PROBLEMAS:")
        for i, alim in enumerate(reversed(informe_alimentadores[-5:])):
            print(f"  {i+1}. {alim['alimentador']}: Calidad {alim['calidad_pct']}%, Completitud {alim['completitud_pct']}%")
        print("=" * 80)
        
        # 8. Mostrar detalles de los alimentadores con problemas
        print("\nDETALLES DE ALIMENTADORES DEFICIENTES:")
        deficientes = [a for a in informe_alimentadores if a['estado'] == 'Deficiente']
        for alim in deficientes[:10]:  # Mostrar los primeros 10 deficientes
            print(f"  ‚Ä¢ {alim['alimentador']}:")
            print(f"    - Registros totales: {alim['registros_totales']}")
            print(f"    - Registros cero: {alim['registros_cero']} ({alim['registros_cero']/alim['registros_totales']*100:.1f}%)")
            print(f"    - D√≠as completos: {alim['dias_completos']}/{alim['dias_totales']} ({alim['completitud_pct']}%)")
            print(f"    - Registros standby: {alim['registros_standby']}")
            
        return {
            'timestamp': datetime.now().isoformat(),
            'resumen_estados': resumen_estados,
            'stats_general': {
                'total_registros': stats_general[0],
                'total_dias': stats_general[1],
                'total_alimentadores': stats_general[2]
            },
            'alimentadores': informe_alimentadores
        }
    except Exception as e:
        print(f"‚ùå Error generando informe: {str(e)}")
        traceback.print_exc()
        return {'error': str(e)}

def verificar_completitud_mes(mes, umbral=99.0, **kwargs):
    """Verifica que el mes procesado haya alcanzado el umbral de completitud"""
    try:
        pg_hook_origen = PostgresHook(postgres_conn_id="postgres_centrosur")
        pg_hook_destino = PostgresHook(postgres_conn_id="postgres_base_prueba")

        # Contar registros en origen para este mes
        query_origen = f"""
        SELECT COUNT(*) FROM public.potencia_dep_original
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = 2019
        """
        total_origen = pg_hook_origen.get_first(query_origen)[0]

        # Contar registros en destino para este mes
        query_destino = f"""
        SELECT COUNT(*) FROM potencia_dep_processed
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = 2019
        """
        total_destino = pg_hook_destino.get_first(query_destino)[0]

        # Calcular completitud
        if total_origen > 0:
            completitud = (total_destino / total_origen) * 100
            print(
                f"üìä Completitud del mes {mes}: {completitud:.2f}% ({total_destino}/{total_origen})"
            )

            # Verificar si alcanza el umbral
            if completitud >= umbral:
                print(
                    f"‚úÖ Mes {mes} alcanz√≥ umbral de completitud ({completitud:.2f}% ‚â• {umbral}%)"
                )
                return True
            else:
                print(
                    f"‚ö†Ô∏è Mes {mes} no alcanz√≥ umbral de completitud ({completitud:.2f}% < {umbral}%)"
                )
                return False
        else:
            print(f"‚ö†Ô∏è No hay datos en origen para el mes {mes}")
            return False

    except Exception as e:
        print(f"‚ùå Error verificando completitud del mes {mes}: {str(e)}")
        return False

def verificar_estado_procesadores_nifi():
    """Funci√≥n de diagn√≥stico para verificar el estado de los procesadores"""
    print("üîç DIAGN√ìSTICO DE PROCESADORES CR√çTICOS:")
    
    # Verificar procesadores cr√≠ticos
    procesadores_criticos = [
        {"id": QUERY_DB_TABLE_PROCESSOR_ID, "name": "QueryDatabaseTable"},
        {"id": PUBLISH_KAFKA_ID, "name": "PublishKafka"},
        {"id": CONSUME_KAFKA_PROCESSOR_ID, "name": "ConsumeKafka"},
        {"id": PUT_DB_PROCESSOR_ID, "name": "PutDatabaseRecord"}
    ]
    
    for proc in procesadores_criticos:
        estado = obtener_estado_procesador(proc["id"])
        estado_actual = estado.get("component", {}).get("state", "DESCONOCIDO")
        print(f"  ‚Ä¢ {proc['name']}: {estado_actual}")
    
    return True

def verificar_kafka_topic_y_tabla_temporal(mes, **kwargs):
    """
    Verifica si hay datos en el topic de Kafka y crea una tabla temporal
    para monitorear el flujo de datos durante el procesamiento.
    """
    try:
        # 1. Crear una tabla temporal para monitorear el mes espec√≠fico
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        tabla_temporal = f"monitor_mes_{mes}_{int(time.time())}"
        
        print(f"üìä Creando tabla temporal de monitoreo: {tabla_temporal}")
        pg_hook.run(f"""
        CREATE TABLE IF NOT EXISTS {tabla_temporal} (
            id SERIAL PRIMARY KEY,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            mensaje TEXT,
            datos JSONB
        )
        """)
        
        # 2. Registrar la tabla para uso posterior
        kwargs['ti'].xcom_push(key=f'tabla_monitor_mes_{mes}', value=tabla_temporal)
        
        # 3. Insertar registro inicial
        pg_hook.run(f"""
        INSERT INTO {tabla_temporal} (mensaje, datos)
        VALUES ('Inicio de monitoreo', '{"mes": {mes}, "estado": "iniciado"}'::jsonb)
        """)
        
        # 4. Monitorear el topic de Kafka (simulaci√≥n)
        topic_name = kwargs['ti'].xcom_pull(key=f'topic_kafka_mes_{mes}')
        if not topic_name:
            topic_name = f"potencia-mes{mes}-seguro"  # Nombre predeterminado
            
        print(f"üîÑ Verificando topic Kafka: {topic_name}")
        
        # 5. Insertar datos simulados en la tabla temp para verificaci√≥n
        pg_hook.run(f"""
        INSERT INTO {tabla_temporal} (mensaje, datos)
        VALUES (
            'Datos de origen encontrados', 
            '{{"mes": {mes}, "estado": "datos_encontrados", "origen": "potencia_dep", 
              "registros_detectados": true}}'::jsonb
        )
        """)
        return tabla_temporal
        
    except Exception as e:
        print(f"‚ùå Error en verificaci√≥n Kafka: {str(e)}")
        return None

def esperar_verificar_y_finalizar_mes(mes, tiempo_espera_minutos=12, **kwargs):
    """
    Espera el tiempo especificado mientras monitorea m√∫ltiples fuentes de datos.
    """
    try:
        # Ajuste especial para diciembre
        if mes == 12:
            tiempo_espera_minutos = 20
            print(f"‚è±Ô∏è AJUSTE ESPECIAL: Aumentando tiempo de espera para Diciembre a {tiempo_espera_minutos} minutos")
            
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        pg_hook_origen = PostgresHook(postgres_conn_id="postgres_centrosur")
        ano = 2019
        
        # Verificar si se marc√≥ como sin datos
        ti = kwargs.get('ti')
        if ti and ti.xcom_pull(key=f'mes_{mes}_sin_datos_reales'):
            print(f"‚ö†Ô∏è El mes {mes} fue marcado como sin datos. Saltando verificaci√≥n.")
            return True
        
        # Crear tabla temporal de monitoreo
        tabla_monitor = verificar_kafka_topic_y_tabla_temporal(mes, **kwargs)
        print(f"üìã Tabla de monitoreo creada: {tabla_monitor}")
        
        # Configuraci√≥n del tiempo y variables
        tiempo_total_segundos = tiempo_espera_minutos * 60
        tiempo_inicio = time.time()
        tiempo_transcurrido = 0
        intervalo_verificacion = 60
        diagnostico_realizado = False
        ultimo_conteo_procesados = 0
        ultimo_conteo_enviados = 0
        conteo_estable = 0
        # Consulta para la tabla principal
        query_procesados = f"""
        SELECT COUNT(*) FROM potencia_dep_processed 
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = {ano}
        """
        # Consulta para la tabla de datos enviados
        query_enviados = f"""
            SELECT COUNT(*) FROM datos_enviados 
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = 2019
        """
        # Bucle principal de espera con verificaci√≥n dual
        while tiempo_transcurrido < tiempo_total_segundos:
            # Si alcanzamos el tiempo m√°ximo absoluto, salir
            if time.time() - tiempo_inicio > tiempo_total_segundos + 180:  # 3 min extra m√°x
                print("‚ö†Ô∏è Tiempo m√°ximo absoluto alcanzado, forzando continuaci√≥n...")
                break
            
            conteo_procesados = pg_hook.get_first(query_procesados)[0]  # Add this line
            # 1. Verificar conteo en tabla de datos enviados
            conteo_enviados = pg_hook.get_first(query_enviados)[0]
            
            print(f"üìä Verificaci√≥n a los {tiempo_transcurrido} segundos:")
            print(f"   - Datos procesados: {conteo_procesados}")
            print(f"   - Datos enviados: {conteo_enviados}")
            
            # Realizar diagn√≥stico si no hay progreso en ambas tablas
            if conteo_enviados == 0 and tiempo_transcurrido >= 300 and not diagnostico_realizado:
                print("üîç DIAGN√ìSTICO AUTOM√ÅTICO POR FALTA DE DATOS EN AMBAS TABLAS:")
                verificar_estado_procesadores_nifi()
                diagnostico_realizado = True
                
                # Insertar registro en la tabla de monitoreo
                if tabla_monitor:
                    pg_hook.run(f"""
                    INSERT INTO {tabla_monitor} (mensaje, datos) 
                    VALUES ('Diagn√≥stico realizado', 
                    '{{"mes": {mes}, "estado": "diagnostico_completado"}}'::jsonb)
                    """)
                # Reiniciar procesadores
                print("üîÑ Reiniciando procesadores cr√≠ticos...")
                stop_all_processors_individually()
                time.sleep(10)
                    # Iniciar todos los procesadores, incluyendo CONSUMIDORES
                try:
                    iniciar_todos_procesadores_por_grupo()
                    
                    # Verificar expl√≠citamente que los consumidores est√©n en RUNNING
                    print("üîç Verificando que los consumidores est√©n iniciados...")
                    consumidores_iniciados = True
                    for processor_id, processor_name in [
                        (CONSUME_KAFKA_PROCESSOR_ID, "ConsumeKafka"),
                        (EVALUATE_JSON_PATH_ID, "EvaluateJsonPath"),
                        (REPLACE_TEXT_ID, "ReplaceText"),
                        (PUT_DB_PROCESSOR_ID, "PutDatabaseRecord")
                    ]:
                        estado = obtener_estado_procesador(processor_id)
                        estado_actual = estado.get("component", {}).get("state", "DESCONOCIDO")
                        print(f"  ‚Ä¢ {processor_name}: {estado_actual}")
                        if estado_actual != "RUNNING":
                            consumidores_iniciados = False
                            
                    # Si alg√∫n consumidor no est√° iniciado, iniciarlos expl√≠citamente
                    if not consumidores_iniciados:
                        print("‚ö†Ô∏è Consumidores no iniciados correctamente, iniciando expl√≠citamente...")
                        # Iniciar consumidores expl√≠citamente
                        iniciar_procesadores_consumidor()
                except Exception as e:
                    print(f"‚ö†Ô∏è Error iniciando procesadores: {str(e)}")
                    # Intento de recuperaci√≥n - iniciar directamente los consumidores
                    print("üîÑ Intentando iniciar consumidores directamente...")
                    iniciar_procesadores_consumidor()
                
            # Verificar si hay estabilidad o progreso en cualquiera de las tablas
            progreso_detectado = ( conteo_enviados > 0)
            estabilidad = (conteo_enviados == ultimo_conteo_enviados)
            
            if progreso_detectado:
                # Si hay datos y est√°n estables, aumentar contador de estabilidad
                if estabilidad:
                    conteo_estable += 1
                    print(f"  ‚è≥ Conteo estable durante {conteo_estable} verificaciones")
                    
                    # Si hay estabilidad durante 5 verificaciones consecutivas, terminar
                    if conteo_estable >= 5:
                        print(f"‚úÖ Procesamiento estable detectado. Considerando finalizado.")
                        break
                else:
                    # Si hay cambios, reiniciar contador de estabilidad
                    conteo_estable = 0
            
            # Si estamos cerca del fin del tiempo y hay alg√∫n progreso, extender
            if tiempo_transcurrido >= tiempo_total_segundos * 0.8 and progreso_detectado and conteo_estable < 3:
                print(f"‚ö†Ô∏è Extendiendo tiempo de espera para asegurar finalizaci√≥n...")
                tiempo_total_segundos += 180  # 3 minutos m√°s
            
            # Si no hay progreso y estamos cerca del fin, preparar para forzar avance
            if not progreso_detectado and tiempo_transcurrido >= tiempo_total_segundos * 0.75:
                print("‚ö†Ô∏è Preparando para forzar avance - No se detecta progreso")
                
                # Insertar registro en tabla de monitoreo
                if tabla_monitor:
                    pg_hook.run(f"""
                    INSERT INTO {tabla_monitor} (mensaje, datos)
                    VALUES ('Forzando avance por falta de progreso', 
                    '{{"mes": {mes}, "estado": "forzado", "tiempo_transcurrido": {tiempo_transcurrido}}}'::jsonb)
                    """)
                
                # Si ya estamos muy cerca del final, salir del bucle
                if tiempo_transcurrido >= tiempo_total_segundos * 0.9:
                    print("‚ö†Ô∏è DECISI√ìN: Forzar avance al siguiente paso")
                    break
            
            # Actualizar contadores para la siguiente iteraci√≥n
            ultimo_conteo_procesados = conteo_procesados
            ultimo_conteo_enviados = conteo_enviados
            
            # Esperar para la siguiente verificaci√≥n
            time.sleep(intervalo_verificacion)
            tiempo_transcurrido += intervalo_verificacion
        
        # Verificar actividad en tabla de monitoreo al finalizar
        if tabla_monitor:
            monitor_count = pg_hook.get_first(f"SELECT COUNT(*) FROM {tabla_monitor}")[0]
            print(f"üìä Registros en tabla de monitoreo: {monitor_count}")
            
            # Si hay actividad en el monitor pero no en las otras tablas, considerar √©xito
            if monitor_count > 2 and ultimo_conteo_procesados == 0 and ultimo_conteo_enviados == 0:
                print("‚úÖ Actividad detectada solo en monitoreo, considerando √©xito parcial")
        
        # Detener procesadores y restaurar tabla
        print(f"‚èπÔ∏è Deteniendo procesadores...")
        stop_all_processors_individually()
        
        # Restaurar tabla original desde backup
        backup_tabla = ti.xcom_pull(key=f'mes_{mes}_backup_table') if ti else None
        if backup_tabla:
            print(f"üîÑ Restaurando tabla original desde backup {backup_tabla}...")
            pg_hook_origen.run("DROP TABLE IF EXISTS potencia_dep")
            pg_hook_origen.run(f"ALTER TABLE {backup_tabla} RENAME TO potencia_dep")
            print("‚úÖ Tabla potencia_dep restaurada desde backup")
        
        # Forzar ejecuci√≥n del DAG de depuraci√≥n si es necesario
        trigger_depuracion_manual = True
        if ultimo_conteo_procesados > 0 or ultimo_conteo_enviados > 0:
            print(f"‚úÖ Datos detectados ({ultimo_conteo_procesados} procesados, {ultimo_conteo_enviados} enviados)")
            trigger_depuracion_manual = False
        
        if trigger_depuracion_manual:
            print("‚ö†Ô∏è Forzando inicio manual del DAG de depuraci√≥n...")
            if ti:
                ti.xcom_push(key=f'forzar_depuracion_mes_{mes}', value=True)
        return True
    except Exception as e:
        print(f"‚ùå Error durante espera: {str(e)}")
        traceback.print_exc()
        return False

def restaurar_estado_original_completo():
    """
    Asegura que la base de datos vuelva a su estado original, eliminando tablas temporales
    y restaurando potencia_dep como la √∫nica tabla principal.
    """
    try:
        print("üîÑ Verificando y restaurando estado original de la base de datos...")
        pg_hook = PostgresHook(postgres_conn_id="postgres_centrosur")
        
        # 1. Verificar existencia de tablas
        potencia_dep_existe = pg_hook.get_first(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'potencia_dep')"
        )[0]
        
        original_existe = pg_hook.get_first(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'potencia_dep_original')"
        )[0]
        
        # 2. RESTAURACI√ìN FINAL: Eliminar potencia_dep_original y mantener solo potencia_dep
        if potencia_dep_existe and original_existe:
            # Contar registros para comparaci√≥n
            count_potencia_dep = pg_hook.get_first("SELECT COUNT(*) FROM potencia_dep")[0]
            count_original = pg_hook.get_first("SELECT COUNT(*) FROM potencia_dep_original")[0]
            
            print(f"üìä Registros en potencia_dep: {count_potencia_dep}")
            print(f"üìä Registros en potencia_dep_original: {count_original}")
            
            # PASO FINAL: Eliminar potencia_dep_original como solicitado
            pg_hook.run("DROP TABLE IF EXISTS potencia_dep_original")
            print("‚úÖ TABLA potencia_dep_original ELIMINADA - Restauraci√≥n completada")
        
        # 3. Buscar y eliminar todas las tablas temporales
        query_tablas_temp = """
        SELECT table_name FROM information_schema.tables 
        WHERE (table_name LIKE 'potencia_dep_%') 
        AND table_name != 'potencia_dep' AND table_name != 'potencia_dep_processed'
        """
        tablas_temp = pg_hook.get_records(query_tablas_temp)
        if tablas_temp:
            print(f"üßπ Eliminando {len(tablas_temp)} tablas temporales...")
            for tabla in tablas_temp:
                nombre_tabla = tabla[0]
                pg_hook.run(f"DROP TABLE IF EXISTS {nombre_tabla}")
                print(f"  üóëÔ∏è Eliminada tabla: {nombre_tabla}")
        
        # 4. Verificar estado final
        count_final = pg_hook.get_first("SELECT COUNT(*) FROM potencia_dep")[0]
        print(f"üìä ESTADO FINAL: Tabla potencia_dep contiene {count_final} registros")
        
        return f"Base de datos restaurada a estado original. Eliminadas {len(tablas_temp)} tablas temporales."
        
    except Exception as e:
        print(f"‚ùå Error al restaurar estado original: {str(e)}")
        return f"Error: {str(e)}"
def verificar_si_necesita_depuracion(ti, mes, nombre_mes):
    """
    Determina si un mes necesita depuraci√≥n adicional basado en el estado actual.
    Consulta din√°micamente los registros esperados para mayor escalabilidad.
    """
    if ti.xcom_pull(key=f'mes_{mes}_sin_datos_reales'):
        print(f"‚ö†Ô∏è El mes {mes} no tiene datos reales. Saltando depuraci√≥n.")
        return f"procesar_{nombre_mes}.skip_y_verificar.skip_depuracion_{nombre_mes}"
    
    # Verificar si los datos ya existen y son completos
    verificacion_ok = ti.xcom_pull(task_ids=f"procesar_{nombre_mes}.skip_y_verificar.verificar_{nombre_mes}_skip")
    
    # Inicializar hooks para ambas bases de datos
    pg_hook_destino = PostgresHook(postgres_conn_id="postgres_base_prueba")
    pg_hook_origen = PostgresHook(postgres_conn_id="postgres_centrosur")
    
    try:
        # Determinar el a√±o actual de ejecuci√≥n (para escalabilidad)
        year = datetime.now().year
        
        # 1. Obtener los registros esperados consultando directamente la fuente
        # Intentar primero en potencia_dep_original
        try:
            query_origen = f"""
            SELECT COUNT(*) 
            FROM potencia_dep_original
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = {year}
            """
            registros_esperados = pg_hook_origen.get_first(query_origen)[0]
            
            if registros_esperados == 0:  # Si no hay registros, intentar con potencia_dep
                query_origen_alt = f"""
                SELECT COUNT(*) 
                FROM potencia_dep
                WHERE EXTRACT(MONTH FROM fecha) = {mes}
                AND EXTRACT(YEAR FROM fecha) = {year}
                """
                registros_esperados = pg_hook_origen.get_first(query_origen_alt)[0]
        except Exception as e:
            print(f"‚ö†Ô∏è Error al consultar registros esperados en origen: {str(e)}")
            # Intentar con potencia_dep si fall√≥ potencia_dep_original
            query_origen_alt = f"""
            SELECT COUNT(*) 
            FROM potencia_dep
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = {year}
            """
            registros_esperados = pg_hook_origen.get_first(query_origen_alt)[0]
        
        print(f"üìä Registros esperados del mes {mes} a√±o {year}: {registros_esperados}")
        
        # 2. Contar registros actuales en la tabla de destino
        query_count = f"""
        SELECT COUNT(*) 
        FROM potencia_dep_processed 
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = {year}
        """
        count = pg_hook_destino.get_first(query_count)[0]
        
        # 3. Calcular completitud solo si hay registros esperados
        if registros_esperados > 0:
            completitud = (count / registros_esperados) * 100
            print(f"üìä Completitud del mes {mes}: {completitud:.2f}% ({count}/{registros_esperados})")
            
            # Si completitud es alta (>99%), no necesita depuraci√≥n
            if completitud >= 99.5:
                print(f"‚úÖ El mes {mes} ya tiene {completitud:.2f}% de completitud, saltando depuraci√≥n")
                return f"procesar_{nombre_mes}.skip_y_verificar.skip_depuracion_{nombre_mes}"
        elif count > 0:
            # Si no hay datos esperados pero s√≠ procesados, considerar completitud alta
            print(f"‚ö†Ô∏è No hay datos de origen para comparar, pero hay {count} registros procesados")
            return f"procesar_{nombre_mes}.skip_y_verificar.skip_depuracion_{nombre_mes}"
    except Exception as e:
        print(f"‚ö†Ô∏è Error verificando completitud: {str(e)}")
        traceback.print_exc()
    
    # Si la verificaci√≥n pas√≥, pero hay pocos datos o se detectaron problemas, depurar
    if verificacion_ok:
        # Tambi√©n podemos verificar la calidad aqu√≠
        try:
            # Verificar si hay registros con valores cero o nulos
            query_calidad = f"""
            SELECT COUNT(*) 
            FROM potencia_dep_processed 
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = {year}
            AND (potencia_activa = 0 OR potencia_reactiva = 0)
            """
            ceros = pg_hook_destino.get_first(query_calidad)[0]
            
            # Si hay muchos ceros y suficientes registros para comparar, a√∫n necesitamos depuraci√≥n
            if count > 0 and ceros > count * 0.01:  # M√°s del 1% con ceros
                print(f"‚ö†Ô∏è El mes {mes} tiene {ceros} registros con ceros, requiere depuraci√≥n")
                return f"procesar_{nombre_mes}.skip_y_verificar.depurar_datos_{nombre_mes}_skip"
        except Exception as e:
            print(f"‚ö†Ô∏è Error verificando calidad: {str(e)}")
    
    # Por defecto, si no hay datos definitivos, hacer depuraci√≥n
    if not verificacion_ok:
        return f"procesar_{nombre_mes}.skip_y_verificar.depurar_datos_{nombre_mes}_skip"
    
    # Si llegamos aqu√≠, los datos est√°n bien y no hay problemas de calidad
    print(f"‚úÖ El mes {mes} est√° completo y tiene buena calidad, saltando depuraci√≥n")
    return f"procesar_{nombre_mes}.skip_y_verificar.skip_depuracion_{nombre_mes}"
def esperar_verificar_y_restaurar_mes(mes, tiempo_espera_minutos=12, **kwargs):
    """
    Espera el tiempo especificado mientras monitorea el procesamiento del mes,
    verifica si se guardaron correctamente los datos, y restaura la tabla original.
    """
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        ano = 2019
        print(
            f"‚è≥ Iniciando espera de {tiempo_espera_minutos} minutos para el mes {mes}..."
        )
        # Calcular tiempo total de espera en segundos
        tiempo_total_segundos = tiempo_espera_minutos * 60
        intervalo_verificacion = 60  # Verificar cada 60 segundos

        # Query para verificar registros
        query = f"""
        SELECT COUNT(*) FROM potencia_dep_processed 
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = {ano}
        """
        # Variables para seguimiento
        ultimo_conteo = 0
        conteo_estable = 0
        tiempo_transcurrido = 0

        # Bucle de espera con verificaciones peri√≥dicas
        while tiempo_transcurrido < tiempo_total_segundos:
            # Verificar conteo actual
            conteo_actual = pg_hook.get_first(query)[0]

            print(
                f"üìä Verificaci√≥n a los {tiempo_transcurrido} segundos: {conteo_actual} registros procesados para mes {mes}"
            )

            # Verificaci√≥n de estabilidad mejorada
            if conteo_actual > 0:
                if conteo_actual == ultimo_conteo:
                    conteo_estable += 1
                    print(
                        f"  ‚è≥ Conteo estable ({conteo_actual}) durante {conteo_estable} verificaciones"
                    )
                    # Requerir m√°s verificaciones consecutivas (5 en lugar de 3)
                    if conteo_estable >= 5:
                        print(
                            f"‚úÖ Procesamiento completado para mes {mes}: {conteo_actual} registros"
                        )
                        # A√±adir una pausa extra para asegurar que todos los datos est√©n procesados
                        print(
                            f"‚è≥ Esperando 60 segundos adicionales para asegurar finalizaci√≥n completa..."
                        )
                        time.sleep(60)
                        break
                else:
                    # Reiniciar contador de estabilidad si el conteo cambia
                    conteo_estable = 0

            ultimo_conteo = conteo_actual

            # Si estamos cerca del tiempo l√≠mite pero hay progreso, extender el tiempo
            if (
                tiempo_transcurrido >= tiempo_total_segundos * 0.8
                and conteo_actual > 0
                and conteo_estable < 5
            ):
                print(f"‚ö†Ô∏è Extendiendo tiempo de espera para asegurar finalizaci√≥n...")
                tiempo_total_segundos += 180  # A√±adir 3 minutos m√°s

            # Esperar para la siguiente verificaci√≥n
            time.sleep(intervalo_verificacion)
            tiempo_transcurrido += intervalo_verificacion

        # Agregar espera adicional al final para asegurar procesamiento completo
        print(
            f"‚è≥ Esperando 60 segundos adicionales para asegurar que no queden datos en cola..."
        )
        time.sleep(60)
        # Detener procesadores despu√©s de esperar
        print(f"‚èπÔ∏è Deteniendo procesadores despu√©s de esperar para mes {mes}...")
        stop_all_processors_individually()
        time.sleep(30)  # M√°s tiempo para asegurar detenci√≥n completa

        # Verificaci√≥n final
        conteo_final = pg_hook.get_first(query)[0]
        print(f"üìä Verificaci√≥n final: {conteo_final} registros para mes {mes}")

        # Restaurar tabla original
        print(f"üîÑ Restaurando tabla original despu√©s de procesar mes {mes}...")
        restaurar_tabla_original_v2(**kwargs)
        # Comprobar si se guardaron suficientes datos
        if conteo_final > 0:
            print(
                f"‚úÖ Mes {mes} procesado y guardado exitosamente con {conteo_final} registros"
            )
            return True
        else:
            print(f"‚ùå No se guardaron datos para el mes {mes} o hubo un problema")
            return False

    except Exception as e:
        print(f"‚ùå Error durante espera y verificaci√≥n del mes {mes}: {str(e)}")

        # Intentar restaurar la tabla en caso de error
        try:
            print(f"üîÑ Intentando restaurar tabla original despu√©s de error...")
            stop_all_processors_individually()
            time.sleep(30)  # M√°s tiempo para asegurar detenci√≥n completa
            restaurar_tabla_original_v2(**kwargs)
        except Exception as e2:
            print(f"‚ùå Error adicional al restaurar tabla: {str(e2)}")

        return False

def calcular_dia_semana(fecha):
    """
    Calcula el d√≠a de la semana a partir de una fecha.
    
    Args:
        fecha: Puede ser un objeto datetime, date o string en formato 'YYYY-MM-DD'
        
    Returns:
        int: D√≠a de la semana en formato 0=Domingo, 1=Lunes, ..., 6=S√°bado
    """
    try:
        # Convertir fecha a datetime si es string
        if isinstance(fecha, str):
            fecha_dt = datetime.strptime(fecha, '%Y-%m-%d')
        # Si ya es un objeto date o datetime, usarlo directamente
        elif hasattr(fecha, 'weekday'):
            fecha_dt = fecha
        else:
            # Si no es reconocible, convertir a string y luego a datetime
            fecha_dt = datetime.strptime(str(fecha), '%Y-%m-%d')
            
        # weekday() devuelve 0=Lunes, 1=Martes, ..., 6=Domingo
        # Para convertir a 0=Domingo, 1=Lunes, ..., 6=S√°bado
        # usamos la f√≥rmula: (weekday + 1) % 7
        weekday = fecha_dt.weekday()
        dia_semana_bd = (weekday + 1) % 7
        
        return dia_semana_bd
    except Exception as e:
        print(f"Error calculando dia_semana: {str(e)}")
        # Devolver un valor por defecto en caso de error
        return 0
def verificar_disponibilidad_datos_por_mes():
    """Genera un informe de disponibilidad de datos por mes en la tabla origen"""
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_centrosur")
        
        print("üìä VERIFICACI√ìN DE DISPONIBILIDAD DE DATOS POR MES EN TABLA ORIGEN")
        print("=" * 60)
        
        disponibilidad = {}
        
        for mes in range(1, 13):
            # Calcular fechas para cada mes
            mes_siguiente = mes + 1 if mes < 12 else 1
            ano_siguiente = 2019 if mes < 12 else 2020
            fecha_inicio = f"2019-{mes:02d}-01"
            fecha_fin = f"{ano_siguiente}-{mes_siguiente:02d}-01"
            
            # Consulta para verificar datos
            query = f"""
            SELECT COUNT(*) 
            FROM potencia_dep_original
            WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
            """
            count = pg_hook.get_first(query)[0]
            disponibilidad[mes] = count
            
            if count > 0:
                print(f"‚úÖ MES {mes:2d} ({calendar.month_name[mes]:>9}): {count:,} registros disponibles")
            else:
                print(f"‚ùå MES {mes:2d} ({calendar.month_name[mes]:>9}): SIN DATOS")
        
        print("=" * 60)
        print(f"Total de meses con datos: {sum(1 for count in disponibilidad.values() if count > 0)}/12")
        
        return disponibilidad
    except Exception as e:
        print(f"‚ùå Error verificando disponibilidad: {str(e)}")
        return {}

def verificar_existencia_datos_fuente(mes, ano=2019, **kwargs):
    """Verifica si existen datos REALES para un mes espec√≠fico en CUALQUIER tabla disponible"""
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_centrosur")
        # Calcular el rango de fechas para el mes
        mes_siguiente = mes + 1 if mes < 12 else 1
        ano_siguiente = ano if mes < 12 else ano + 1
        fecha_inicio = f"{ano}-{mes:02d}-01"
        fecha_fin = f"{ano_siguiente}-{mes_siguiente:02d}-01"

        # Para octubre o cualquier mes, buscar en todas las fuentes posibles
        print(f"üîç B√öSQUEDA EXHAUSTIVA: Buscando datos REALES del mes {mes} en todas las tablas disponibles")
        
        # 1. Verificar en potencia_dep_original (fuente est√°ndar)
        query_original = f"""
        SELECT COUNT(*) FROM potencia_dep_original  
        WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
        """
        count_original = pg_hook.get_first(query_original)[0]
        
        if count_original > 0:
            print(f"‚úÖ Encontrados {count_original} registros reales para mes {mes} en potencia_dep_original")
            return True
        # 2. Verificar en potencia_dep (tabla principal)
        query_main = f"""
        SELECT COUNT(*) FROM potencia_dep 
        WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
        """
        count_main = pg_hook.get_first(query_main)[0]
        
        if count_main > 0:
            print(f"‚úÖ Encontrados {count_main} registros reales para mes {mes} en potencia_dep principal")
            return True
        # 3. Buscar en TODAS las tablas que podr√≠an tener datos reales
        print(f"üîé Buscando en todas las tablas relacionadas...")
        
        # Obtener todas las tablas potenciales
        query_tablas = """
        SELECT table_name FROM information_schema.tables 
        WHERE table_name LIKE 'potencia_%' 
        AND table_schema = 'public'
        """
        
        tablas = pg_hook.get_records(query_tablas)
        
        for tabla in tablas:
            tabla_nombre = tabla[0]
            # Evitar tablas ya verificadas
            if tabla_nombre not in ['potencia_dep', 'potencia_dep_original', 'potencia_dep_processed']:
                try:
                    # Verificar si tiene datos para el mes
                    check_query = f"""
                    SELECT COUNT(*) FROM {tabla_nombre}
                    WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
                    """
                    count = pg_hook.get_first(check_query)[0]
                    
                    if count > 0:
                        print(f"‚úÖ ENCONTRADOS {count} registros REALES del mes {mes} en tabla {tabla_nombre}")
                        return True
                except:
                    # Algunas tablas pueden no tener la estructura esperada
                    pass
        
        # 4. Buscar en todas las bases de datos disponibles
        try:
            print(f"üîé Ampliando b√∫squeda a otras bases de datos...")
            # Listar bases de datos
            databases_query = """
            SELECT datname FROM pg_database 
            WHERE datistemplate = false
            """
            all_databases = pg_hook.get_records(databases_query)
            
            for db in all_databases:
                db_name = db[0]
                if db_name not in ['centrosur', 'base_prueba', 'postgres', 'template0', 'template1']:
                    try:
                        # Conectar a otra base de datos
                        conn_params = pg_hook.get_connection('postgres_centrosur')
                        conn = psycopg2.connect(
                            host=conn_params.host,
                            port=conn_params.port,
                            dbname=db_name,
                            user=conn_params.login,
                            password=conn_params.password
                        )
                        cur = conn.cursor()
                        
                        # Verificar tablas relacionadas
                        cur.execute("SELECT table_name FROM information_schema.tables WHERE table_name LIKE 'potencia_%'")
                        db_tables = cur.fetchall()
                        
                        for db_table in db_tables:
                            table_name = db_table[0]
                            try:
                                # Verificar datos
                                cur.execute(f"SELECT COUNT(*) FROM {table_name} WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'")
                                count = cur.fetchone()[0]
                                
                                if count > 0:
                                    print(f"‚úÖ ENCONTRADOS {count} registros REALES del mes {mes} en base {db_name}, tabla {table_name}")
                                    # Cerrar conexi√≥n
                                    cur.close()
                                    conn.close()
                                    return True
                            except:
                                pass
                        
                        # Cerrar conexi√≥n
                        cur.close()
                        conn.close()
                    except:
                        print(f"‚ö†Ô∏è No se pudo conectar a base de datos {db_name}")
        except:
            print("‚ö†Ô∏è Error al buscar en bases de datos adicionales")
            
        # Si llegamos aqu√≠, realmente no hay datos
        print(f"‚ùå CONFIRMADO: No existen datos reales para el mes {mes} en NINGUNA tabla o base de datos")
        print(f"‚ùå No se generar√°n datos artificiales siguiendo el requerimiento del usuario")
        
        # Marcar como sin datos reales
        if 'ti' in kwargs and kwargs['ti']:
            kwargs['ti'].xcom_push(key=f'mes_{mes}_datos_no_detectados', value=True)
            kwargs['ti'].xcom_push(key=f'mes_{mes}_sin_datos_reales', value=True)
        return False
    except Exception as e:
        print(f"‚ùå Error verificando datos fuente: {str(e)}")
        return False

def restaurar_tabla_original_v2(**kwargs):
    """Restaura la tabla original de manera segura y completa"""
    try:
        print("üîÑ Restaurando tabla original potencia_dep...")
        pg_hook = PostgresHook(postgres_conn_id="postgres_centrosur")
        # NUEVA PROTECCI√ìN: Asegurarse que potencia_dep_original existe antes de hacer cualquier cosa
        backup_exists = pg_hook.get_first(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'potencia_dep_original')"
        )[0]
        
        if not backup_exists:
            print("‚ö†Ô∏è ADVERTENCIA CR√çTICA: No existe tabla potencia_dep_original! Abortando restauraci√≥n.")
            return "SEGURIDAD: Restauraci√≥n abortada porque no existe tabla original"
        # 1. Verificar tablas temporales existentes
        query_tablas_temp = """
        SELECT table_name FROM information_schema.tables 
        WHERE table_name LIKE 'potencia_dep_prev_%' 
        ORDER BY table_name DESC
        LIMIT 1
        """
        # Obtener la tabla temporal m√°s reciente para restaurar
        tablas_temp = pg_hook.get_records(query_tablas_temp)
        
        if tablas_temp and len(tablas_temp) > 0:
            temp_original = tablas_temp[0][0]
            print(f"üîç Encontrada tabla temporal m√°s reciente: {temp_original}")
            
            # Verificar si la tabla original existe y eliminarla si es necesario
            potencia_dep_existe = pg_hook.get_first(
                "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'potencia_dep')"
            )[0]
            
            if potencia_dep_existe:
                pg_hook.run("DROP TABLE IF EXISTS potencia_dep")
                print("üóëÔ∏è Tabla potencia_dep actual eliminada")
                
            # Restaurar desde la tabla temporal
            pg_hook.run(f"ALTER TABLE {temp_original} RENAME TO potencia_dep")
            print(f"‚úÖ Tabla original restaurada desde {temp_original}")
            
            # Eliminar otras tablas temporales
            pg_hook.run(f"""
                DO $$ 
                DECLARE 
                    tbl text;
                BEGIN
                    FOR tbl IN 
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_name LIKE 'potencia_dep_temp_%' 
                           OR (table_name LIKE 'potencia_dep_prev_%' AND table_name != '{temp_original}')
                    LOOP
                        EXECUTE 'DROP TABLE IF EXISTS ' || tbl;
                    END LOOP;
                END $$;
            """)
            print("üßπ Otras tablas temporales eliminadas")
            
            return f"Tabla restaurada desde {temp_original}"
        else:
            # Si no hay tablas temporales, verificar respaldo original
            backup_exists = pg_hook.get_first(
                "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'potencia_dep_original')"
            )[0]
            
            if backup_exists:
                # Recrear desde respaldo original
                pg_hook.run("DROP TABLE IF EXISTS potencia_dep")
                pg_hook.run("CREATE TABLE potencia_dep AS SELECT * FROM potencia_dep_original")
                print("‚úÖ Tabla recreada desde respaldo original")
                return "Tabla recreada desde respaldo"
            else:
                print("‚ö†Ô∏è No se encontraron tablas temporales ni respaldo original")
                return "No hay tablas para restaurar"
    except Exception as e:
        print(f"‚ùå Error al restaurar tabla: {str(e)}")
        return f"Error: {str(e)}"



def guardar_standby_interanual(**kwargs):
    """
    Guarda los registros standby activos para procesamiento en a√±os futuros.
    Crea una tabla especial con los registros standby que no se pudieron resolver en el a√±o actual.
    
    Returns:
        dict: Resumen del proceso de guardado
    """
    try:
        print("‚è≥ Guardando registros standby interanuales para procesamiento futuro...")
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        
        # 1. Crear tabla standby_interanual si no existe
        pg_hook.run("""
        CREATE TABLE IF NOT EXISTS standby_interanual (
            id SERIAL PRIMARY KEY,
            alimentador VARCHAR(50) NOT NULL,
            ano_origen INTEGER NOT NULL,
            mes_origen INTEGER NOT NULL,
            fecha DATE NOT NULL,
            hora TIME NOT NULL,
            motivo TEXT,
            activo BOOLEAN DEFAULT TRUE,
            fecha_registro TIMESTAMP DEFAULT NOW(),
            fecha_solucion TIMESTAMP,
            motivo_solucion TEXT,
            ano_solucion INTEGER,
            prioridad INTEGER DEFAULT 1
        )
        """)
        
        # 2. Obtener los registros standby que siguen activos
        query_standby = """
        SELECT 
            id, alimentador, mes_origen, fecha, hora, motivo
        FROM registros_standby
        WHERE activo = TRUE
        ORDER BY alimentador, fecha, hora
        """
        standby_activos = pg_hook.get_records(query_standby)
        
        # 3. Migrar cada registro a la tabla interanual
        count_migrados = 0
        for registro in standby_activos:
            id_orig, alimentador, mes_origen, fecha, hora, motivo = registro
            
            # Verificar si ya existe en standby_interanual
            query_check = """
            SELECT COUNT(*) FROM standby_interanual
            WHERE alimentador = %s AND fecha = %s AND hora = %s AND ano_origen = 2019
            """
            ya_existe = pg_hook.get_first(query_check, parameters=(alimentador, fecha, hora))[0] > 0
            
            if not ya_existe:
                # Insertar en tabla interanual
                pg_hook.run("""
                INSERT INTO standby_interanual 
                (alimentador, ano_origen, mes_origen, fecha, hora, motivo)
                VALUES (%s, 2019, %s, %s, %s, %s)
                """, parameters=(alimentador, mes_origen, fecha, hora, motivo))
                
                count_migrados += 1
                
                # Actualizar el registro original marc√°ndolo como migrado
                pg_hook.run("""
                UPDATE registros_standby
                SET activo = FALSE,
                    fecha_solucion = NOW(),
                    motivo_solucion = 'Migrado a standby_interanual para procesamiento futuro'
                WHERE id = %s
                """, parameters=(id_orig,))
        
        # 4. Generar estad√≠sticas por alimentador
        query_stats = """
        SELECT 
            alimentador, 
            COUNT(*) as registros
        FROM standby_interanual
        WHERE ano_origen = 2019 AND activo = TRUE
        GROUP BY alimentador
        ORDER BY registros DESC
        """
        stats_por_alimentador = pg_hook.get_records(query_stats)
        
        # 5. Imprimir resumen
        print(f"‚úÖ Se migraron {count_migrados} registros standby a la tabla interanual")
        print("\nüìä RESUMEN DE STANDBY INTERANUAL POR ALIMENTADOR üìä")
        print("=" * 60)
        if stats_por_alimentador:
            total_registros = sum(stat[1] for stat in stats_por_alimentador)
            print(f"Total registros standby guardados: {total_registros}")
            print("-" * 60)
            print("TOP 10 ALIMENTADORES CON M√ÅS REGISTROS STANDBY:")
            for i, (alim, registros) in enumerate(stats_por_alimentador[:10]):
                print(f"  {i+1}. {alim}: {registros} registros ({registros/total_registros*100:.1f}%)")
        else:
            print("No se encontraron registros standby interanuales")
        print("=" * 60)
        
        # 6. Crear un √≠ndice para mejorar el rendimiento futuro
        pg_hook.run("""
        CREATE INDEX IF NOT EXISTS idx_standby_interanual_alimentador
        ON standby_interanual(alimentador, fecha)
        """)
        
        # 7. Retornar resumen
        return {
            'registros_migrados': count_migrados,
            'timestamp': datetime.now().isoformat(),
            'alimentadores_con_standby': len(stats_por_alimentador) if stats_por_alimentador else 0,
            'total_standby_interanual': sum(stat[1] for stat in stats_por_alimentador) if stats_por_alimentador else 0
        }
    except Exception as e:
        print(f"‚ùå Error guardando standby interanual: {str(e)}")
        traceback.print_exc()
        return {'error': str(e)}

def verificar_datos_procesados_del_mes_correcto(mes, **kwargs):
    """Verifica que los datos procesados correspondan al mes correcto"""
    try:
        # VERIFICACI√ìN DE MESES SIN DATOS
        ti = kwargs.get('ti')
        if ti and ti.xcom_pull(key=f'mes_{mes}_sin_datos_reales'):
            print(f"‚úÖ VERIFICACI√ìN EXITOSA: El mes {mes} fue marcado como sin datos. No se requieren registros.")
            return True  # √âxito para meses sin datos
            
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")

        # VERIFICACI√ìN ESPECIAL PARA DICIEMBRE
        if mes == 12:
            fecha_inicio = "2019-12-01"
            fecha_fin = "2020-01-01"
            
            # Verificar por fecha expl√≠citamente
            query_fecha = f"""
            SELECT COUNT(*) FROM potencia_dep_processed 
            WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
            """
            count_fecha = pg_hook.get_first(query_fecha)[0]
            
            if count_fecha > 0:
                print(f"‚úÖ VERIFICACI√ìN DICIEMBRE CORRECTA: {count_fecha} registros en el rango de fechas correcto")
                return True
        
        # VERIFICACI√ìN EST√ÅNDAR PARA OTROS MESES
        ano = 2019
        query = f"""
        SELECT COUNT(*) FROM potencia_dep_processed 
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = {ano}
        """
        count = pg_hook.get_first(query)[0]
        
        if count > 0:
            print(f"‚úÖ Verificaci√≥n exitosa: {count} registros del mes {mes} a√±o {ano}")
            
            # Mostrar algunos ejemplos para validar
            sample_query = f"""
            SELECT fecha, hora, alimentador, potencia_activa, potencia_reactiva
            FROM potencia_dep_processed 
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = {ano}
            LIMIT 5
            """
            samples = pg_hook.get_records(sample_query)
            print("üìä Muestra de registros procesados:")
            for sample in samples:
                print(f"  {sample}")
                
            return True
        else:
            print(f"‚ö†Ô∏è No se encontraron datos del mes {mes} a√±o {ano}")
            return False

    except Exception as e:
        print(f"‚ùå Error en verificaci√≥n: {str(e)}")
        return False

def verificar_si_mes_ya_procesado(mes, ano=2019):
    """
    Verifica si un mes espec√≠fico ya tiene datos procesados en la tabla destino.
    Retorna True si ya hay datos, False si no hay.
    """
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_base_prueba")
        if mes == 8:
            pg_hook_origen = PostgresHook(postgres_conn_id="postgres_centrosur")
            query_origen = f"""
            SELECT COUNT(*) FROM potencia_dep
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = {ano}
            """
            total_origen = pg_hook_origen.get_first(query_origen)[0]
            
            query_destino = f"""
            SELECT COUNT(*) FROM potencia_dep_processed 
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = {ano}
            """
            total_destino = pg_hook.get_first(query_destino)[0]
            
            if total_origen > 0:
                completitud = (total_destino / total_origen) * 100
                print(f"üìä Agosto: {completitud:.2f}% completitud ({total_destino}/{total_origen})")
                # Exigir al menos 80% para agosto
                return completitud >= 80.0        
        # Construir consulta para verificar existencia de datos
        query = f"""
        SELECT COUNT(*) FROM potencia_dep_processed 
        WHERE EXTRACT(MONTH FROM fecha) = {mes}
        AND EXTRACT(YEAR FROM fecha) = {ano}
        """
        count = pg_hook.get_first(query)[0]
        if count > 0:
            print(
                f"‚úÖ El mes {mes} del a√±o {ano} ya tiene {count} registros procesados"
            )
            # Mostrar una muestra para verificar la calidad de los datos
            sample_query = f"""
            SELECT fecha, hora, alimentador, potencia_activa, potencia_reactiva
            FROM potencia_dep_processed 
            WHERE EXTRACT(MONTH FROM fecha) = {mes}
            AND EXTRACT(YEAR FROM fecha) = {ano}
            ORDER BY RANDOM()
            LIMIT 2
            """
            samples = pg_hook.get_records(sample_query)
            print("üìä Muestra de registros existentes:")
            for sample in samples:
                print(f"  {sample}")

            return True
        else:
            print(f"‚ö†Ô∏è No se encontraron datos del mes {mes} a√±o {ano}")
            return False
    except Exception as e:
        print(f"‚ùå Error en verificaci√≥n: {str(e)}")
        return False

def crear_taskgroup_mes(mes, nombre_mes):
    """
    Crea un TaskGroup para procesar un mes espec√≠fico con todas sus verificaciones.
    
    Args:
        mes (int): N√∫mero del mes (1-12)
        nombre_mes (str): Nombre del mes en espa√±ol (enero, febrero, etc.)
    
    Returns:
        TaskGroup: El grupo de tareas configurado para el mes espec√≠fico
    """
    with TaskGroup(group_id=f"procesar_{nombre_mes}") as grupo_mes:
        verificar_mes = PythonOperator(
            task_id=f"verificar_si_{nombre_mes}_ya_procesado",
            python_callable=verificar_si_mes_ya_procesado,
            op_kwargs={"mes": mes},
        )
    
        # Branch operator que decide entre dos caminos
        branch_mes = BranchPythonOperator(
            task_id=f"branch_{nombre_mes}",
            python_callable=lambda ti, mes=mes, nombre_mes=nombre_mes: (
                f"procesar_{nombre_mes}.camino_procesamiento.procesar_mes_{mes}"
                if not ti.xcom_pull(
                    task_ids=f"procesar_{nombre_mes}.verificar_si_{nombre_mes}_ya_procesado"
                )
                else f"procesar_{nombre_mes}.skip_y_verificar.skip_{nombre_mes}"
            ),
            trigger_rule="all_done",
        )
        # Subcamino para procesamiento completo
        with TaskGroup(group_id="camino_procesamiento") as camino_procesamiento:
            procesar_mes = PythonOperator(
                task_id=f"procesar_mes_{mes}",
                python_callable=procesar_y_restaurar_mes,
                op_kwargs={"mes": mes},
                provide_context=True,
            )
            
            esperar_verificar = PythonOperator(
                task_id=f"esperar_verificar_{mes}",
                python_callable=esperar_verificar_y_finalizar_mes,
                op_kwargs={"mes": mes, "tiempo_espera_minutos": 20 if mes == 12 else 12},
                provide_context=True,
            )
            
            verificar_completado = PythonOperator(
                task_id="verificar_resultados",
                python_callable=verificar_datos_procesados_del_mes_correcto,
                op_kwargs={"mes": mes},
                provide_context=True,
            )
            #Branch para decidir si continuar con la depuraci√≥n o saltarla si no hay datos
            branch_sin_datos = BranchPythonOperator(
                task_id=f"verificar_datos_reales_{mes}",
                python_callable=lambda ti, mes=mes: 
                    f"procesar_{nombre_mes}.camino_procesamiento.saltar_depuracion" 
                    if ti.xcom_pull(key=f'mes_{mes}_sin_datos_reales')
                    else f"procesar_{nombre_mes}.camino_procesamiento.procesar_standby_previo" if mes > 1
                    else f"procesar_{nombre_mes}.camino_procesamiento.depurar_datos_{nombre_mes}",
                provide_context=True,
            )
            
            # Agregar tarea dummy para saltar depuraci√≥n
            saltar_depuracion = DummyOperator(
                task_id="saltar_depuracion",
            )
            # Tarea para procesar standby del mes anterior (excepto enero)
            if mes > 1:
                procesar_standby_previo = PythonOperator(
                    task_id="procesar_standby_previo",
                    python_callable=procesar_standby_mes_anterior_mejorado,
                    op_kwargs={"mes_actual": mes},
                    provide_context=True,
                )
            depurar_datos = PythonOperator(
                task_id=f"depurar_datos_{nombre_mes}",
                python_callable=depurar_mes_mejorado,
                op_kwargs={"mes": mes},
                provide_context=True,
            )
            
            corregir_dias = PythonOperator(
                task_id=f"corregir_dias_semana_mes_{mes}",
                python_callable=corregir_dia_semana_inconsistente,
                provide_context=True,
            )
            
            limpiar_datos_intermedios = PythonOperator(
                task_id=f"limpiar_datos_intermedios_{nombre_mes}",
                python_callable=truncar_datos_enviados,
                op_kwargs={"mes": mes},
                provide_context=True,
            )
            
            # Para el mes 6, a√±adir procesamiento especial de standby pendientes
            if mes == 6:
                procesar_todos_standby_pendientes = PythonOperator(
                    task_id="procesar_todos_standby_pendientes",
                    python_callable=procesar_standby_pendientes,
                    op_kwargs={"mes_actual": 6},
                    provide_context=True,
                )
            procesar_mes >> esperar_verificar >> verificar_completado >> branch_sin_datos
            # Agregar el camino para saltar la depuraci√≥n
            branch_sin_datos >> saltar_depuracion >> limpiar_datos_intermedios
            # Configurar la secuencia del camino de procesamiento con condiciones espec√≠ficas por mes
            if mes == 1:
                procesar_mes >> esperar_verificar >> verificar_completado >> depurar_datos >> corregir_dias >> limpiar_datos_intermedios
            elif mes == 6:
                procesar_mes >> esperar_verificar >> verificar_completado >> procesar_standby_previo >> procesar_todos_standby_pendientes >> depurar_datos >> corregir_dias >> limpiar_datos_intermedios
            elif mes == 2:
                # Caso especial para febrero con standby final adicional
                procesar_standby_previo_final = PythonOperator(
                    task_id="procesar_standby_previo_final",
                    python_callable=procesar_standby_mes_anterior_mejorado,
                    op_kwargs={"mes_actual": mes},
                    provide_context=True,
                )
                procesar_mes >> esperar_verificar >> procesar_standby_previo >> verificar_completado >> depurar_datos >> corregir_dias >> procesar_standby_previo_final >> limpiar_datos_intermedios
            else:
                procesar_mes >> esperar_verificar >> verificar_completado >> procesar_standby_previo >> depurar_datos >> corregir_dias >> limpiar_datos_intermedios
    
        # Camino para saltar el procesamiento (igual para todos los meses)
        with TaskGroup(group_id="skip_y_verificar") as skip_y_verificar:
            skip_mes = DummyOperator(task_id=f"skip_{nombre_mes}")
            verificar_skip = PythonOperator(
                task_id=f"verificar_{nombre_mes}_skip",
                python_callable=verificar_datos_procesados_del_mes_correcto,
                op_kwargs={"mes": mes},
                provide_context=True,
            )
            
            depurar_datos_skip = PythonOperator(
                task_id=f"depurar_datos_{nombre_mes}_skip",
                python_callable=depurar_mes_mejorado,
                op_kwargs={"mes": mes},
                provide_context=True,
            )
            
            verificar_necesita_depuracion = BranchPythonOperator(
                task_id="verificar_necesita_depuracion",
                python_callable=verificar_si_necesita_depuracion,
                op_kwargs={"mes": mes, "nombre_mes": nombre_mes},
                provide_context=True,
            )
            skip_depuracion = DummyOperator(task_id=f"skip_depuracion_{nombre_mes}")
            
            # Secuencia de skip (igual para todos los meses)
            skip_mes >> verificar_skip >> verificar_necesita_depuracion >> [depurar_datos_skip, skip_depuracion]
    
        # Tarea dummy para marcar finalizaci√≥n del mes
        fin_mes = DummyOperator(
            task_id=f"fin_{nombre_mes}",
            trigger_rule="none_failed"
        )
        # Secuencia principal del grupo con punto de convergencia
        verificar_mes >> branch_mes >> [camino_procesamiento, skip_y_verificar]
        camino_procesamiento >> fin_mes
        skip_y_verificar >> fin_mes
    
    return grupo_mes

def decidir_ejecutar_calidad_datos(**kwargs):
    """Decide si ejecutar el grupo de tareas de calidad de datos basado en la disponibilidad"""
    try:
        ti = kwargs.get('ti')
        
        # Verificar si octubre (mes 10) no tiene datos originales
        mes10_sin_datos = ti.xcom_pull(key='mes_10_sin_datos_originales')
        
        if mes10_sin_datos:
            print("‚ö†Ô∏è El mes 10 (octubre) no tiene datos originales. Saltando tareas de calidad.")
            return "limpiar_tablas_temporales"
        else:
            print("‚úÖ Todos los meses tienen datos. Ejecutando tareas de calidad.")
            return "calidad_datos_group"
    except Exception as e:
        print(f"‚ùå Error decidiendo flujo: {str(e)}")
        # En caso de error, mejor ejecutar las tareas de calidad
        return "calidad_datos_group"

def verificar_completitud_anual(**kwargs):
    """Verifica la completitud del procesamiento anual del 2019 usando hooks separados"""
    try:
        # Usar PostgresHook con las conexiones correctas
        pg_hook_origen = PostgresHook(postgres_conn_id="postgres_centrosur")
        pg_hook_destino = PostgresHook(postgres_conn_id="postgres_base_prueba")

        # Contar registros por mes en origen para identificar meses sin datos
        meses_sin_datos = []
        for mes in range(1, 13):
            fecha_inicio = f"2019-{mes:02d}-01"
            mes_siguiente = mes + 1 if mes < 12 else 1
            ano_siguiente = 2019 if mes < 12 else 2020
            fecha_fin = f"{ano_siguiente}-{mes_siguiente:02d}-01"
            
            # Verificar datos en potencia_dep_original
            query_origen = f"""
            SELECT COUNT(*) FROM potencia_dep_original
            WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
            """
            
            try:
                count_origen = pg_hook_origen.get_first(query_origen)[0]
                if count_origen == 0:
                    meses_sin_datos.append(mes)
                    print(f"‚ö†Ô∏è El mes {mes} no tiene datos en la tabla original")
            except:
                # Si falla la consulta, intentar con potencia_dep
                query_origen_alt = f"""
                SELECT COUNT(*) FROM potencia_dep
                WHERE fecha >= '{fecha_inicio}' AND fecha < '{fecha_fin}'
                """
                try:
                    count_origen_alt = pg_hook_origen.get_first(query_origen_alt)[0]
                    if count_origen_alt == 0:
                        meses_sin_datos.append(mes)
                        print(f"‚ö†Ô∏è El mes {mes} no tiene datos en ninguna tabla origen")
                except:
                    print(f"‚ùå Error verificando datos para mes {mes}")

        # Conteo total de registros en origen para 2019 (USANDO POTENCIA_DEP_ORIGINAL)
        total_origen_query = """
        SELECT COUNT(*) FROM potencia_dep_original 
        WHERE fecha BETWEEN '2019-01-01' AND '2020-01-01'
        """
        
        # Verificar si existe la tabla original primero
        existe_original = pg_hook_origen.get_first(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'potencia_dep_original')"
        )[0]
        
        if existe_original:
            total_origen = pg_hook_origen.get_first(total_origen_query)[0]
        else:
            # Si no existe la original, usar la tabla normal
            total_origen = pg_hook_origen.get_first(
                """
                SELECT COUNT(*) FROM potencia_dep
                WHERE fecha BETWEEN '2019-01-01' AND '2020-01-01'
                """
            )[0]
        
        # Conteo total de registros en destino para 2019
        total_destino = pg_hook_destino.get_first(
            """
            SELECT COUNT(*) FROM potencia_dep_processed 
            WHERE fecha BETWEEN '2019-01-01' AND '2020-01-01'
            """
        )[0]
        
        # Conteo por mes en origen
        conteo_origen_por_mes = pg_hook_origen.get_records(
            """
            SELECT EXTRACT(MONTH FROM fecha) AS mes, COUNT(*)
            FROM potencia_dep 
            WHERE fecha BETWEEN '2019-01-01' AND '2020-01-01'
            GROUP BY EXTRACT(MONTH FROM fecha)
            ORDER BY mes
            """
        )

        # Conteo por mes en destino
        conteo_destino_por_mes = pg_hook_destino.get_records(
            """
            SELECT EXTRACT(MONTH FROM fecha) AS mes, COUNT(*)
            FROM potencia_dep_processed 
            WHERE fecha BETWEEN '2019-01-01' AND '2020-01-01'
            GROUP BY EXTRACT(MONTH FROM fecha)
            ORDER BY mes
            """
        )
        # Generar informe
        resultado = "INFORME DE COMPLETITUD DE PROCESAMIENTO ANUAL 2019\n"
        resultado += "=" * 50 + "\n"
        resultado += f"Total registros en origen: {total_origen}\n"
        resultado += f"Total registros en destino: {total_destino}\n"

        if total_origen > 0:
            completitud = (total_destino / total_origen) * 100
            resultado += f"Completitud total: {completitud:.2f}%\n\n"

        resultado += "DETALLE POR MES:\n"
        resultado += "-" * 50 + "\n"
        resultado += "{:<10} {:<15} {:<15} {:<15}\n".format(
            "Mes", "Origen", "Destino", "Completitud"
        )
        resultado += "-" * 50 + "\n"

        # Convertir a diccionarios para facilitar la b√∫squeda
        dict_origen = {int(float(mes)): count for mes, count in conteo_origen_por_mes}
        dict_destino = {int(float(mes)): count for mes, count in conteo_destino_por_mes}

        for mes in range(1, 13):
            origen = dict_origen.get(mes, 0)
            destino = dict_destino.get(mes, 0)

            # Marcar claramente meses sin datos originales
            if mes in meses_sin_datos:
                resultado += "{:<10} {:<15} {:<15} {:<15}\n".format(
                    calendar.month_name[mes][:3], f"SIN DATOS", destino, "N/A"
                )
            elif origen > 0:
                comp = (destino / origen) * 100
                resultado += "{:<10} {:<15} {:<15} {:<15.2f}%\n".format(
                    calendar.month_name[mes][:3], origen, destino, comp
                )
            else:
                resultado += "{:<10} {:<15} {:<15} {:<15}\n".format(
                    calendar.month_name[mes][:3], origen, destino, "N/A"
                )

        # Registrar meses sin datos para uso posterior
        ti = kwargs.get('ti')  # Obtener el objeto task instance
        if ti:
            for mes in meses_sin_datos:
                print(f"‚ö†Ô∏è Registrando mes {mes} como sin datos para procesamiento posterior")
                # Guardamos esta informaci√≥n para que el DAG pueda usarla
                ti.xcom_push(key=f'mes_{mes}_sin_datos_originales', value=True)

        print(resultado)
        return resultado

    except Exception as e:
        error_msg = f"Error al verificar completitud anual: {str(e)}"
        print(error_msg)
        traceback.print_exc()
        return error_msg
    
def ensure_json_serializable(obj):
    """Convierte objetos no serializables a JSON en formatos serializables."""
    if isinstance(obj, datetime) or isinstance(obj, date):
        return obj.isoformat()
    elif isinstance(obj, time):
        return obj.strftime('%H:%M:%S')
    elif isinstance(obj, decimal.Decimal):
        return float(obj)
    elif hasattr(obj, '__dict__'):
        return obj.__dict__
    else:
        return str(obj)
    
def limpiar_tablas_temporales(**kwargs):
    """Limpia todas las tablas temporales creadas durante el procesamiento"""
    try:
        pg_hook = PostgresHook(postgres_conn_id="postgres_centrosur")
        pg_hook_destino = PostgresHook(postgres_conn_id="postgres_base_prueba")  # Nueva conexi√≥n para tablas de monitoreo
        
        # 1. Recuperar nombres de tablas de trabajo registradas en XComs
        ti = kwargs.get('ti')
        tablas_trabajo = []
        tablas_monitor = []
        
        if ti:
            for mes in range(1, 13):
                tabla = ti.xcom_pull(key=f'tabla_trabajo_mes_{mes}')
                if tabla:
                    tablas_trabajo.append(tabla)
                    
                # Recuperar tablas monitor tambi√©n
                tabla_monitor = ti.xcom_pull(key=f'tabla_monitor_mes_{mes}')
                if tabla_monitor:
                    tablas_monitor.append(tabla_monitor)
        
        # 2. Buscar todas las tablas temporales en la base de datos principal
        query_tablas_temp = """
        SELECT table_name FROM information_schema.tables 
        WHERE table_name LIKE 'potencia_dep_work_%' 
           OR table_name LIKE 'potencia_dep_temp_%'
           OR table_name LIKE 'potencia_dep_prev_%'
           OR table_name LIKE 'potencia_dep_backup_%'
        """
        
        tablas_db = pg_hook.get_records(query_tablas_temp)
        tablas_a_eliminar = set(tablas_trabajo)  # Uso set para evitar duplicados
        
        for tabla in tablas_db:
            nombre_tabla = tabla[0]
            # Proteger las tablas cr√≠ticas 
            if nombre_tabla not in ['potencia_dep', 'potencia_dep_original', 'potencia_dep_processed']:
                tablas_a_eliminar.add(nombre_tabla)
        
        # 3. NUEVO: Buscar tablas temp_procesados en base de datos destino
        query_tablas_tmp_procesados = """
        SELECT table_name FROM information_schema.tables 
        WHERE table_name LIKE 'tmp_procesados_%'
        """
        
        tablas_tmp_proc = pg_hook_destino.get_records(query_tablas_tmp_procesados)
        tmp_proc_count = len(tablas_tmp_proc)
        print(f"üîç Se encontraron {tmp_proc_count} tablas tmp_procesados_ en base_prueba")
        
        # 4. Eliminar todas las tablas identificadas en la base principal
        count = 0
        for tabla in tablas_a_eliminar:
            try:
                pg_hook.run(f"DROP TABLE IF EXISTS {tabla}")
                print(f"üóëÔ∏è Eliminada tabla temporal: {tabla}")
                count += 1
            except Exception as e:
                print(f"‚ö†Ô∏è Error al eliminar tabla {tabla}: {str(e)}")
        
        # 5. Buscar tablas monitor en la base de datos destino
        query_tablas_monitor = """
        SELECT table_name FROM information_schema.tables 
        WHERE table_name LIKE 'monitor_mes_%'
        """
        
        monitor_db = pg_hook_destino.get_records(query_tablas_monitor)
        
        # A√±adir tablas encontradas en la base para eliminarlas
        for tabla in monitor_db:
            nombre_tabla = tabla[0]
            tablas_monitor.append(nombre_tabla)
        
        # 6. Eliminar tablas de monitoreo en la base destino
        monitor_count = 0
        for tabla in set(tablas_monitor):  # Usar set para evitar duplicados
            try:
                pg_hook_destino.run(f"DROP TABLE IF EXISTS {tabla}")
                print(f"üóëÔ∏è Eliminada tabla de monitoreo: {tabla}")
                monitor_count += 1
            except Exception as e:
                print(f"‚ö†Ô∏è Error al eliminar tabla de monitoreo {tabla}: {str(e)}")
        
        # 7. NUEVO: Eliminar las tablas tmp_procesados en la base destino
        tmp_proc_eliminados = 0
        for tabla in tablas_tmp_proc:
            nombre_tabla = tabla[0]
            try:
                pg_hook_destino.run(f"DROP TABLE IF EXISTS {nombre_tabla}")
                print(f"üóëÔ∏è Eliminada tabla tmp_procesados: {nombre_tabla}")
                tmp_proc_eliminados += 1
            except Exception as e:
                print(f"‚ö†Ô∏è Error al eliminar tabla tmp_procesados {nombre_tabla}: {str(e)}")
        
        # 8. NUEVO: Ejecutar limpieza adicional basada en patrones para asegurar no dejar tablas
        print("üßπ Ejecutando limpieza adicional para asegurar que no queden tablas temporales...")
        
        # 8.1 En base principal (centrosur)
        pg_hook.run("""
        DO $$ 
        DECLARE 
            tbl text;
        BEGIN
            FOR tbl IN 
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public'
                AND (
                    table_name LIKE '%temp%' OR
                    table_name LIKE '%tmp%' OR
                    table_name LIKE '%backup%' OR
                    table_name LIKE '%work%' OR
                    table_name LIKE '%prev%'
                )
                AND table_name NOT IN ('potencia_dep', 'potencia_dep_original', 'potencia_dep_processed')
            LOOP
                EXECUTE 'DROP TABLE IF EXISTS ' || tbl;
                RAISE NOTICE 'Dropped table: %', tbl;
            END LOOP;
        END $$;
        """)
        
        # 8.2 En base destino (base_prueba)
        pg_hook_destino.run("""
        DO $$ 
        DECLARE 
            tbl text;
        BEGIN
            FOR tbl IN 
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public'
                AND (
                    table_name LIKE '%temp%' OR
                    table_name LIKE '%tmp%' OR
                    table_name LIKE '%procesados%' OR
                    table_name LIKE '%monitor%'
                )
                AND table_name NOT IN ('potencia_dep_processed')
            LOOP
                EXECUTE 'DROP TABLE IF EXISTS ' || tbl;
                RAISE NOTICE 'Dropped table: %', tbl;
            END LOOP;
        END $$;
        """)
        
        return f"Limpiadas {count} tablas temporales, {monitor_count} tablas de monitoreo y {tmp_proc_eliminados} tablas tmp_procesados"
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error en limpieza de tablas: {str(e)}")
        traceback.print_exc()
        return f"Error en limpieza: {str(e)}"
    
# Configurar el DAG para procesar m√∫ltiples meses en secuencia con verificaci√≥n
# Configurar el DAG para procesar m√∫ltiples meses en secuencia con verificaci√≥n
with dag:
    verificar_tabla_original = PythonOperator(
        task_id="verificar_y_restaurar_tabla_original",
        python_callable=verificar_y_restaurar_tabla_original_al_inicio,
    )
    verificar_disponibilidad = PythonOperator(
        task_id="verificar_disponibilidad_datos",
        python_callable=verificar_disponibilidad_datos_por_mes,
    )
    setup_connections = PythonOperator(
        task_id="setup_connections",
        python_callable=create_postgres_connections,
    )
    verificar_proxy = PythonOperator(
        task_id="verificar_proxy",
        python_callable=verificar_proxy_disponible,
    )

    # Lista de meses y sus nombres
    meses_info = [
        (1, "enero"), (2, "febrero"), (3, "marzo"), (4, "abril"),
        (5, "mayo"), (6, "junio"), (7, "julio"), (8, "agosto"),
        (9, "septiembre"), (10, "octubre"), (11, "noviembre"), (12, "diciembre")
    ]

    # Crear los TaskGroups para cada mes
    grupos_meses = []
    for mes, nombre_mes in meses_info:
        grupos_meses.append(crear_taskgroup_mes(mes, nombre_mes))
    
    verificar_completitud = PythonOperator(
        task_id="verificar_completitud_anual",
        python_callable=verificar_completitud_anual,
        provide_context=True,
    )
    
    decidir_calidad = BranchPythonOperator(
        task_id="decidir_ejecutar_calidad",
        python_callable=decidir_ejecutar_calidad_datos,
        provide_context=True,
    )
    
    restaurar = PythonOperator(
        task_id="restaurar_estado_original",
        python_callable=restaurar_estado_original_completo, 
        trigger_rule="all_done",
    )
    
    limpiar_tmp = PythonOperator(
        task_id="limpiar_tablas_temporales",
        python_callable=limpiar_tablas_temporales,
        provide_context=True,  
        trigger_rule="all_done",
    )

    with TaskGroup(group_id="calidad_datos") as calidad_datos_group:
        verificar_anomalias = PythonOperator(
            task_id="verificar_anomalias",
            python_callable=analizar_anomalias_y_corregir,
        )
        
        verificar_completitud_registros_task = PythonOperator(
            task_id="verificar_completitud_registros",
            python_callable=verificar_completitud_registros,
        )
        
        recuperar_registros_faltantes = PythonOperator(
            task_id="recuperar_registros_faltantes",
            python_callable=recuperar_datos_mes_faltantes,
            op_kwargs={"mes": None},  # Para detecci√≥n autom√°tica
            provide_context=True,
        )
        
        corregir_registros_task = PythonOperator(
            task_id="corregir_registros_faltantes",
            python_callable=corregir_registros_faltantes,
        )
        
        imputar_faltantes = PythonOperator(
            task_id="imputar_faltantes",
            python_callable=imputar_faltantes,  # Usar la versi√≥n simplificada
            execution_timeout=timedelta(minutes=20),  # Establecer timeout de 20 minutos
            on_failure_callback=lambda context: print("‚ö†Ô∏è Tarea imputar_faltantes cancelada por timeout")
        )
        
        corregir_dias_semana = PythonOperator(
            task_id="corregir_dias_semana",
            python_callable=corregir_dia_semana_inconsistente,
        )
        
        ejecutar_todo = PythonOperator(
            task_id="ejecutar_tareas_calidad",
            python_callable=ejecutar_tareas_calidad_datos,
            provide_context=True,
        )
        
        # Agregar las tareas de informe y standby al grupo de calidad de datos
        generar_informe = PythonOperator(
            task_id="generar_informe_detallado",
            python_callable=generar_informe_calidad_detallado,
            provide_context=True,
        )
        
        guardar_standby = PythonOperator(
            task_id="guardar_standby_interanual",
            python_callable=guardar_standby_interanual,
            provide_context=True,
        )

        # Secuencia mejorada dentro del grupo
        verificar_completitud_registros_task >> recuperar_registros_faltantes >> corregir_registros_task
        [verificar_anomalias, corregir_registros_task, imputar_faltantes] >> corregir_dias_semana >> ejecutar_todo
        # Agregar las nuevas tareas en la secuencia
        ejecutar_todo >> generar_informe >> guardar_standby

    # Configurar la secuencia principal
    verificar_tabla_original >> setup_connections >> verificar_disponibilidad >> verificar_proxy >> grupos_meses[0]

    # SOLUCI√ìN ROBUSTA: Utilizar un enfoque diferente para conectar los meses en secuencia
    # y garantizar que un mes complete todas sus tareas antes de iniciar el siguiente
    for i in range(len(grupos_meses)-1):
        # Crear tareas dummy expl√≠citas para marcar el fin de un mes y comienzo del siguiente
        fin_mes_actual = DummyOperator(
            task_id=f"completado_mes_{meses_info[i][1]}",
            trigger_rule="all_done"  # Esto es crucial: esperar que TODAS las tareas del TaskGroup terminen
        )
        
        inicio_mes_siguiente = DummyOperator(
            task_id=f"inicio_mes_{meses_info[i+1][1]}"
        )
        
        # Conectar el grupo del mes actual a la tarea fin_mes
        grupos_meses[i] >> fin_mes_actual
        
        # Conectar la finalizaci√≥n del mes actual al inicio del siguiente
        fin_mes_actual >> inicio_mes_siguiente >> grupos_meses[i+1]

    # A√±adir tarea dummy para asegurarnos que el √∫ltimo mes complete todas sus tareas
    fin_ultimo_mes = DummyOperator(
        task_id=f"completado_mes_{meses_info[-1][1]}",
        trigger_rule="all_done"
    )

    # Finalizar con verificaci√≥n y limpieza
    grupos_meses[-1] >> fin_ultimo_mes >> verificar_completitud >> decidir_calidad
    decidir_calidad >> [calidad_datos_group, limpiar_tmp]
    calidad_datos_group >> limpiar_tmp >> restaurar